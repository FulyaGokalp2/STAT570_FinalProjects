[
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html",
    "title": "Police Department Stop Data",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data set.\nAuthor:\nBuse Bakış - buse.bakis@metu.edu.tr\nThe San Francisco Police Department (SFPD) Stop Data was designed to capture information to comply with the Racial and Identity Profiling Act (RIPA), or California Assembly Bill (AB)953. SFPD officers collect specific information on each stop, including elements of the stop, circumstances and the perceived identity characteristics of the individual(s) stopped. The information obtained by officers is reported to the California Department of Justice. This data set includes data on stops starting on July 1st, 2018, which is when the data collection program went into effect. Also, the data set includes information about police stops that occurred, including some details about the person(s) stopped, and what happened during the stop. Each row is a person stopped with a record identifier for the stop and a unique identifier for the person. A single stop may involve multiple people and may produce more than one associated unique identifier for the same record identifier.\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\n\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(hrbrthemes)\nlibrary(viridis)\nlibrary(MASS) \nlibrary(reshape2) \nlibrary(reshape)\nlibrary(ggmap)\nlibrary(leaflet)\nlibrary(stringr)\nlibrary(wordcloud)\n\nstop &lt;- read.csv(\"stopdata.csv\", sep = \",\")\n\nstop$date &lt;- ymd_hms(stop$stop_datetime)\n\nstop$doj_record_id &lt;- as.factor(stop$doj_record_id)\n\nstop &lt;- stop |&gt;\n  group_by(unique_identifier) |&gt;\n  mutate(count = n())"
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#introduction",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#introduction",
    "title": "Police Department Stop Data",
    "section": "Introduction",
    "text": "Introduction\nIn this project to investigate police stop data and traffic accident that caused a person injury in San Francisco data is used. One of the data sets is between in 2018-2023 (Police Stop Data), and the other one is between in 2005-2023 (Accident data). However, to be able to work in the same period, it was selected as 2018-2023.\nThere is a 244934 observation and 87 variable in Police Stop data, and 57456 observation and 57 variable in Accident data.\nFirstly, the structure of the Police Stop data was investigated. Only numeric variables are numeric and others even if date variable was also character variable, date variable changed as date with the function from lubridate package, and the monthly trends of number of stops is check with the interactive plotly line plot.\nThere was a huge dramatic decrease in February 2020, probably pandemic situation is affected the rate of the stop people to be safe from Covid with less interaction. May be police officer just give the penalty tickets without stopping.\n\nstop_monthly &lt;- stop |&gt;\n    group_by(date = lubridate::floor_date(date, 'month')) |&gt;\n    summarize(count = sum(count)) \n\nfig &lt;- plot_ly(stop_monthly, x = ~date, y = ~count, type = 'scatter', mode = 'lines') |&gt;\n    layout(title = 'Trend of number of people stoped by police 2018-2023', plot_bgcolor = \"#e5ecf6\", xaxis = list(title = 'Date'), \n         yaxis = list(title = 'Count of stop'))\nfig\n\n\n\n\n\nThis data set comprises numerous records of police stops, yet some of them have undergone status changes, specifically being marked as “deleted.” For this project, only data with the status “Completed - Successful Submission” has been included.\n\nstop_age &lt;- stop |&gt;\n    group_by(gender = as.factor(perceived_gender), age = perceived_age) |&gt;\n    summarize(count = sum(count))\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\nggplot(stop_age, aes(x = age  , y = count, colour = gender )) +\n  geom_line()\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\nstop &lt;- stop |&gt; \n  filter(perceived_age &lt; 80 && perceived_age &gt; 16) |&gt; \n  filter(stop_data_record_status == \"Completed - Successful Submission\")\n\nfactor_stop &lt;- as.data.frame(unclass(stop), stringsAsFactors = TRUE)\n\nstop_age2 &lt;- stop |&gt;\n    group_by(gender = as.factor(perceived_gender), age = as.factor(perceived_age_group)) |&gt;\n    summarize(count =n()) |&gt;\n    filter(gender %in% c(\"Male\",\"Female\"))\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\nggplot(stop_age2, aes(fill=gender, y=count, x=age)) + \n    geom_bar(position=\"dodge\", stat=\"identity\") +\n    scale_fill_viridis(discrete = T, option = \"E\") +\n    ggtitle(\"Gender vs Age Groups\") +\n    facet_wrap(~gender) +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +\n    xlab(\"\")"
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#demographic-information",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#demographic-information",
    "title": "Police Department Stop Data",
    "section": "Demographic Information",
    "text": "Demographic Information\nCertain demographic information has been examined, starting with the analysis of gender and age through a line plot. The most of them are male and their ages is in between 25 to 50. The most prevalent gender, among others, is female within the same age group. A minority group of transgender individuals is present in the data set. As evident from the plot, outliers in age are apparent. In California, the minimum age for obtaining a driver’s license is 16.5, yet the data starts from 1. Additionally, individuals above the age of 80 should not be eligible to drive, but there are values close to 125. To address this, the data has been filtered to include ages between 16 and 80.\n\nsummary(factor_stop$perceived_race_ethnicity)\n\n                        Asian        Black/African American \n                        27041                         59846 \n           Hispanic/Latino(a) Middle Eastern or South Asian \n                        47937                         16152 \n                 Multi-racial               Native American \n                         5989                           356 \n             Pacific Islander                         White \n                         2959                         84654 \n\nsummary(factor_stop$perceived_age_group)\n\n   18 - 29    30 - 39    40 - 49    50 - 59 60 or over   Under 18 \n     64295      78355      51320      33524      16673        767 \n\nstop_eth &lt;- stop |&gt;\n    group_by(age = perceived_age_group, \n             ethnic = perceived_race_ethnicity) |&gt;\n    summarize(count = sum(count))\n\n`summarise()` has grouped output by 'age'. You can override using the `.groups`\nargument.\n\nlevels(as.factor(stop_eth$ethnic))\n\n[1] \"Asian\"                         \"Black/African American\"       \n[3] \"Hispanic/Latino(a)\"            \"Middle Eastern or South Asian\"\n[5] \"Multi-racial\"                  \"Native American\"              \n[7] \"Pacific Islander\"              \"White\"                        \n\nstop_eth &lt;- stop_eth |&gt; \n  mutate(count = case_when(ethnic == \"Asian\" ~ count*0.17,\n                           ethnic == \"Middle Eastern or South Asian\"~ count*0.17,\n                           ethnic == \"Hispanic/Latino(a)\"~ count*0.15,\n                           ethnic == \"White\"~ count*0.43, \n                           ethnic == \"Black/African American\"~ count*0.052,\n                           ethnic == \"Multi-racial\"~ count*0.08, \n                           ethnic == \"Native American\"~ count*0.005,\n                           ethnic == \"Pacific Islander\"~ count*0.003)\n         )\n\nggplot(data=stop_eth, aes(x=age, y=count, fill=ethnic)) + \n  geom_bar(stat=\"identity\") +\n  scale_fill_brewer(palette=\"Blues\")\n\n\n\n\nAfter checking age and gender values, ethnicity of person is investigated. Across all age groups, individuals identified as white are most frequently stopped by police officers, which aligns with the fact that almost half of the population consists of white individuals. Interestingly, despite Hispanics making up 15% of the population, they exhibit a higher rate of police stops compared to other ethnicities."
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#accident-data-vs-police-stop-data-trend",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#accident-data-vs-police-stop-data-trend",
    "title": "Police Department Stop Data",
    "section": "Accident Data vs Police Stop Data Trend",
    "text": "Accident Data vs Police Stop Data Trend\nTo check if there is any connection between the stop rate and crash rate, we used Traffic Accident data collected from the San Francisco government open data library, and we adjusted the date values similar to the corrections made in the Police Stop data. Even if crash rate is less than the police stop rate, it has similar trend with the stop rate. At the beginning of pandemic, the crash rate sharply decreased but after a while it started increase again. Perhaps this trend is influenced by seasonality, as there is a noticeable decrease at the beginning of each year.\n\ncrash &lt;- read.csv(\"crashes.csv\", sep = \",\")\nhead(crash)\n\n  unique_id cnn_intrsctn_fkey cnn_sgmt_fkey case_id_pkey tb_latitude\n1      4734          21703000       9150000      2008161    37.73103\n2     44875          27577000       3578000      2238762    37.78552\n3     39944          27694000      12281101      2116991    37.75097\n4     10509          25668000       6921000      2183936    37.75558\n5     18041          25876000       4008000      2382577    37.76849\n6     44932          27567000       3937000      2409306    37.78735\n  tb_longitude  geocode_source geocode_location     collision_datetime\n1    -122.4290 SFPD-CROSSROADS      CITY STREET 04/09/2005 02:03:00 AM\n2    -122.4604 SFPD-CROSSROADS      CITY STREET 09/06/2005 10:30:00 AM\n3    -122.4950 SFPD-CROSSROADS      CITY STREET 06/23/2005 02:30:00 PM\n4    -122.4296 SFPD-CROSSROADS      CITY STREET 08/10/2005 03:26:00 PM\n5    -122.4290 SFPD-CROSSROADS      CITY STREET 11/03/2005 02:21:00 PM\n6    -122.4570 SFPD-CROSSROADS      CITY STREET 12/26/2005 08:03:00 AM\n     collision_date collision_time accident_year     month day_of_week\n1     2005 April 09       02:03:00          2005     April    Saturday\n2 2005 September 06       10:30:00          2005 September     Tuesday\n3      2005 June 23       14:30:00          2005      June    Thursday\n4    2005 August 10       15:26:00          2005    August   Wednesday\n5  2005 November 03       14:21:00          2005  November    Thursday\n6  2005 December 26       08:03:00          2005  December      Monday\n             time_cat juris officer_id reporting_district beat_number\n1  2:01 am to 6:00 am  3801       4216                            004\n2 10:01 am to 2:00 pm  3801        625                  G        4B4G\n3  2:01 pm to 6:00 pm  3801       1629                            3I3\n4  2:01 pm to 6:00 pm  3801     002122                  D        4B6D\n5  2:01 pm to 6:00 pm  3801       1105                           3E61\n6 6:01 am to 10:00 am  3801        334              RICHM         3G2\n     primary_rd  secondary_rd distance direction weather_1  weather_2\n1    MISSION ST   TRUMBULL ST      132     North    Cloudy    Raining\n2 CALIFORNIA ST      02ND AVE       37      West    Cloudy Not Stated\n3   SUNSET BLVD     ORTEGA ST      211     South    Cloudy Not Stated\n4       HILL ST    SANCHEZ ST      133      East     Clear Not Stated\n5     CHURCH ST  RESERVOIR ST       66     South     Clear Not Stated\n6    JORDAN AVE CALIFORNIA ST      500     North     Clear Not Stated\n          collision_severity type_of_collision                 mviw\n1 Injury (Complaint of Pain)        Hit Object         Fixed Object\n2            Injury (Severe)             Other              Bicycle\n3 Injury (Complaint of Pain)        Hit Object         Fixed Object\n4     Injury (Other Visible)         Broadside Parked Motor Vehicle\n5     Injury (Other Visible)        Overturned         Fixed Object\n6 Injury (Complaint of Pain)          Rear End Parked Motor Vehicle\n                   ped_action road_surface          road_cond_1 road_cond_2\n1      No Pedestrian Involved          Wet No Unusual Condition  Not Stated\n2      No Pedestrian Involved          Dry No Unusual Condition  Not Stated\n3      No Pedestrian Involved          Dry No Unusual Condition  Not Stated\n4 In Road, Including Shoulder          Dry No Unusual Condition  Not Stated\n5      No Pedestrian Involved          Dry     Holes, Deep Ruts  Not Stated\n6      No Pedestrian Involved          Wet No Unusual Condition  Not Stated\n              lighting control_device    intersection vz_pcf_code vz_pcf_group\n1 Dark - Street Lights           None Midblock &gt; 20ft     Unknown      Unknown\n2             Daylight           None Midblock &gt; 20ft       22517        22517\n3             Daylight    Functioning Midblock &gt; 20ft     Unknown      Unknown\n4             Daylight           None Midblock &gt; 20ft     Unknown      Unknown\n5             Daylight           None Midblock &gt; 20ft     Unknown      Unknown\n6             Daylight           None Midblock &gt; 20ft     Unknown      Unknown\n                        vz_pcf_description\n1                                  Unknown\n2 Opening door on traffic side when unsafe\n3                                  Unknown\n4                                  Unknown\n5                                  Unknown\n6                                  Unknown\n                                                                                      vz_pcf_link\n1                                                                                      Not Stated\n2 http://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=VEH&sectionNum=22517\n3                                                                                      Not Stated\n4                                                                                      Not Stated\n5                                                                                      Not Stated\n6                                                                                      Not Stated\n  number_killed number_injured\n1             0              4\n2             0              1\n3             0              1\n4             0              1\n5             0              1\n6             0              1\n                                                                      street_view\n1 https://maps.google.com/maps?q=&layer=c&cbll=37.7310277112579,-122.429020773456\n2   https://maps.google.com/maps?q=&layer=c&cbll=37.785522231882,-122.46043606478\n3  https://maps.google.com/maps?q=&layer=c&cbll=37.750971046041,-122.494976107694\n4 https://maps.google.com/maps?q=&layer=c&cbll=37.7555838090639,-122.429578947769\n5 https://maps.google.com/maps?q=&layer=c&cbll=37.7684931481079,-122.429037469419\n6 https://maps.google.com/maps?q=&layer=c&cbll=37.7873546399109,-122.456963340925\n  dph_col_grp  dph_col_grp_description party_at_fault party1_type\n1          AA Vehicle(s) Only Involved             NA      Driver\n2          CC          Vehicle-Bicycle              1      Driver\n3          AA Vehicle(s) Only Involved             NA      Driver\n4          BB       Vehicle-Pedestrian              1      Driver\n5          FF             Bicycle Only             NA   Bicyclist\n6          AA Vehicle(s) Only Involved              1      Driver\n  party1_dir_of_travel  party1_move_pre_acc    party2_type party2_dir_of_travel\n1           Not Stated  Proceeding Straight                                    \n2                 East               Parked      Bicyclist                 East\n3                North         Ran Off Road                                    \n4                 East Other Unsafe Turning Parked Vehicle           Not Stated\n5                South  Proceeding Straight                                    \n6                North  Proceeding Straight Parked Vehicle                North\n  party2_move_pre_acc                               point\n1                     POINT (-122.429020773 37.731027711)\n2 Proceeding Straight POINT (-122.460436065 37.785522232)\n3                     POINT (-122.494976108 37.750971046)\n4              Parked POINT (-122.429578948 37.755583809)\n5                     POINT (-122.429037469 37.768493148)\n6              Parked  POINT (-122.456963341 37.78735464)\n              data_as_of        data_updated_at         data_loaded_at\n1 04/09/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n2 09/06/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n3 06/23/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n4 08/10/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n5 11/03/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n6 12/26/2005 12:00:00 AM 04/26/2023 12:00:00 AM 12/12/2023 12:18:08 PM\n\ncrash$collision_datetime &lt;- sub(\"[[:space:]].*\", \"\", crash$collision_datetime)\n\ncrash$collision_datetime &lt;- as.Date(crash$collision_datetime, \"%m/%d/%Y\")\n\ncrash$date &lt;- as.POSIXct(crash$collision_datetime, format=\"%Y-%m-%d\",tz=\"UTC\")\n\ncrash &lt;- crash |&gt;\n  group_by(unique_id) |&gt;\n  mutate(count = n())\n\ncrash_monthly &lt;- crash |&gt;\n    group_by(date = lubridate::floor_date(date, 'month')) |&gt;\n    summarize(count = sum(count)) \n\ncrash_monthly &lt;- subset(crash_monthly, date &gt;= \"2018-07-01\" & date &lt;= \"2023-06-01\")\ncrash_monthly &lt;- rbind(crash_monthly, c(\"2023-06-01\", 211))\n\nmonthly_data &lt;- cbind(crash_monthly, stop_monthly)\nmonthly_data &lt;- monthly_data[,-3]\ncolnames(monthly_data) &lt;- c(\"date\", \"crash_count\", \"stop_count\")\n\n\n\nmelt_data &lt;- melt(monthly_data, id = c(\"date\")) \n\nWarning in melt.data.frame(monthly_data, id = c(\"date\")): partial argument\nmatch of 'id' to 'id.vars'\n\n#melt_data\n\nggplot(crash_monthly, aes(x = date, y = count)) + \n  geom_line() + \n  scale_color_manual(values = \"darkred\")\n\n\n\nggplot(melt_data, aes(x = date, y = value)) + \n  geom_line(aes(color = variable, linetype = variable)) + \n  scale_color_manual(values = c(\"darkred\", \"steelblue\"))"
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#duration-time-according-to-demographic-information",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#duration-time-according-to-demographic-information",
    "title": "Police Department Stop Data",
    "section": "Duration Time according to Demographic Information",
    "text": "Duration Time according to Demographic Information\nWhen the police officer is stopped the person, is the ethnicity or the age affected the stop duration time? To see it, some interactive box-plots are created:\n\nno_na_df&lt;- stop[!is.na(stop$latitude), ]\ndf &lt;- no_na_df |&gt;\n  group_by(longitude,latitude) |&gt;\n  summarise(cnt = n())\n\n`summarise()` has grouped output by 'longitude'. You can override using the\n`.groups` argument.\n\nout &lt;- boxplot.stats(no_na_df$duration_of_stop)$out\nout_ind &lt;- which(no_na_df$duration_of_stop %in% c(out))\n#out_ind\ndf2 &lt;- no_na_df[-out_ind, ]\n\n\nplot_ly(\n  data = df2,\n  x = ~perceived_race_ethnicity,\n  y = ~duration_of_stop,\n  type = \"box\",\n  color = ~perceived_race_ethnicity,\n  showlegend = FALSE) %&gt;%\n  layout(xaxis = list(title = 'Ethnicity'), \n  yaxis = list(title = 'Duration time (minute)' ) \n  )\n\n\n\n\n\nAt the plot above, it can be seen that African American people have the highest duration of stop rate, but in Asians (normal and middle/south eastern) there is an interesting situation. Although their average duration is shorter from many other ethnicity but they includes many outliers in higher minutes.\n\nplot_ly(\n  data = df2,\n  x = ~perceived_age_group,\n  y = ~duration_of_stop,\n  type = \"box\",\n  color = ~perceived_age_group,\n  showlegend = FALSE) %&gt;%\n  layout(xaxis = list(title = 'Age Group'), \n  yaxis = list(title = 'Duration time (minute)' ) \n  )\n\n\n\n\n\nAge groups are checked according to duration time. As it is expected, 60 or over people duration time is shorter than the other age groups, but under 18 age group have the highest average duration rate and the longest ranges. However, other age groups duration minute rate are almost exactly the same."
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#stop-points-vs-crash-points",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#stop-points-vs-crash-points",
    "title": "Police Department Stop Data",
    "section": "Stop Points vs Crash Points",
    "text": "Stop Points vs Crash Points\nPolice stop points are generally in the same spots. In the Police Stop data set there was some geospatial values for stopping spot as longitude and latitude information. To see this spot in the map longitude and latitude values are grouped and counted as how many people stopped there. The most of the stop points had just 1 stopped. However to see the most popular stopping places having more than 100 stop places are selected. In the one place, the police officer stopped 2569 different people. To see the crashes in the map firstly longitude and latitude values created from point variable because it collected weird way as POINT (…) (What?). To create the geospatial information from this variable with the StringR packages, some string functions deleted unnecessary characters, then separated variable into two different variables as longitude and latitude.\n\nsummary(df$cnt)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    3.00   13.62   10.00 2542.00 \n\nmybins &lt;- seq(100, 2569, by=400)\nmypalette &lt;- colorBin( palette=\"YlOrBr\", domain=quakes$mag, na.color=\"transparent\", bins=mybins)\n\nm &lt;- leaflet(df) %&gt;% \n  addTiles()  %&gt;% \n  setView( lat=37.773972, lng=-122.431297, zoom=11) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\") %&gt;%\n  addCircleMarkers(~longitude, ~latitude, \n    fillColor = ~mypalette(cnt), fillOpacity = 0.7, color=\"white\", radius=8, stroke=FALSE,\n    labelOptions = labelOptions( style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), textsize = \"13px\", direction = \"auto\")\n  ) %&gt;%\n  addLegend(pal=mypalette, values=~cnt, opacity=0.5, title = \"count\", position = \"bottomright\" )\n\nWarning in mypalette(cnt): Some values were outside the color scale and will be\ntreated as NA\n\n\n\nhead(crash$point)\n\n[1] \"POINT (-122.429020773 37.731027711)\" \"POINT (-122.460436065 37.785522232)\"\n[3] \"POINT (-122.494976108 37.750971046)\" \"POINT (-122.429578948 37.755583809)\"\n[5] \"POINT (-122.429037469 37.768493148)\" \"POINT (-122.456963341 37.78735464)\" \n\ncrash$point&lt;-gsub(\"POINT \",\"\",as.character(crash$point))\ncrash$point&lt;-gsub(\")\",\"\",as.character(crash$point))\ncrash$point&lt;-gsub(\"^.{0,1}\",\"\",as.character(crash$point))\n\ndf1 &lt;- crash\n\ndf1[c('lon', 'lat')] &lt;- str_split_fixed(crash$point, ' ', 2)\ndf1$lon &lt;- as.numeric(df1$lon)\ndf1$lat &lt;- as.numeric(df1$lat)\n\ndf1 &lt;- df1 |&gt;\n  group_by(lon,lat) |&gt;\n  summarise(cnt = n())\n\n`summarise()` has grouped output by 'lon'. You can override using the `.groups`\nargument.\n\nsummary(df1)\n\n      lon              lat             cnt         \n Min.   :-122.5   Min.   :37.71   Min.   :  1.000  \n 1st Qu.:-122.4   1st Qu.:37.75   1st Qu.:  1.000  \n Median :-122.4   Median :37.77   Median :  1.000  \n Mean   :-122.4   Mean   :37.76   Mean   :  1.837  \n 3rd Qu.:-122.4   3rd Qu.:37.78   3rd Qu.:  1.000  \n Max.   :-122.4   Max.   :37.83   Max.   :197.000  \n NA's   :1        NA's   :1                        \n\nmybins1 &lt;- seq(5, 197, by=30)\nmypalette1 &lt;- colorBin( palette=\"YlOrBr\", domain=df1$cnt, na.color=\"transparent\", bins=mybins1)\n\nm1 &lt;- leaflet(df1) %&gt;% \n  addTiles()  %&gt;% \n  setView(lat=37.773972, lng=-122.431297, zoom=11) %&gt;%\n  addProviderTiles(\"Esri.WorldImagery\") %&gt;%\n  addCircleMarkers(~lon, ~lat, \n    fillColor = ~mypalette1(cnt), fillOpacity = 0.7, color=\"white\", radius=8, stroke=FALSE,\n    labelOptions = labelOptions( style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), textsize = \"13px\", direction = \"auto\")\n  ) %&gt;%\n  addLegend(pal=mypalette1, values=~cnt, opacity=0.5, title = \"count\", position = \"bottomright\" )\n\nWarning in validateCoords(lng, lat, funcName): Data contains 1 rows with either\nmissing or invalid lat/lon values and will be ignored\n\n\nWarning in mypalette1(cnt): Some values were outside the color scale and will\nbe treated as NA\n\n\n\npar(mfrow=c(1,2))\nm\n\n\n\n\nm1\n\n\n\n\n\nAfter mapping the longitude and latitude variables, there were no surprising results; the upper east side of the city emerged as the most popular location for both crashes and police stops. This area witnessed the highest number of crashes, suggesting that police officers may also be concentrated there to prevent accidents."
  },
  {
    "objectID": "PoliceStopData-BuseBakis/project_busebakis.html#districts-and-actions",
    "href": "PoliceStopData-BuseBakis/project_busebakis.html#districts-and-actions",
    "title": "Police Department Stop Data",
    "section": "Districts and Actions",
    "text": "Districts and Actions\nIn San Francisco, there is many districts. However, in this data set, district levels are more than the number of districts. Because some of district wrote as lower case, others are upper case and some of them as N/A. Firstly, N/A’s are dropped from data and others are converted as upper case. After these steps, the district information grouped by and counted and visualized with the word cloud plot. So, the most stop occurred in Southern, Mission and Central of San Francisco. Also, action that have taken is investigated in this project, but also this variable is collected with the weird way, the police officer wrote different action that have taken in one individual with “|” as separator. For dividing the sentences, we used the seperate_delim_longer function. The separated sentences as different value but it leaves white spaces in the end and beginning of the sentences, and this white spaces deleted with str_trim function. Therefore, all the actions taken became a level, and visualized it also word cloud plot. Therefore, it can be seen as the most taken action is “Patrol car detention”.\n\narea &lt;- dplyr::select(stop, c(\"city\",\"district\",\"unique_identifier\", \"actions_taken\"))\narea$city &lt;- as.factor(area$city)\narea$district &lt;- as.factor(area$district)\n\nhead(area)\n\n# A tibble: 6 × 4\n# Groups:   unique_identifier [6]\n  city          district        unique_identifier      actions_taken            \n  &lt;fct&gt;         &lt;fct&gt;           &lt;chr&gt;                  &lt;chr&gt;                    \n1 SAN FRANCISCO TARAVAL         U380119326E22D6100AC_5 Search of person was con…\n2 SAN FRANCISCO OUT OF SF / UNK U3801223400D4887116B_1 Property was seized | Ha…\n3 SAN FRANCISCO TENDERLOIN      U380121054B6A404E1D6_1 Search of person was con…\n4 SAN FRANCISCO BAYVIEW         U380118193C7D32D472B_1 Search of person was con…\n5 SAN FRANCISCO BAYVIEW         U38012029453C383DF7A_1 Search of person was con…\n6 SAN FRANCISCO OUT OF SF / UNK U3801210484E3C91B19E_1 Search of property was c…\n\nsum(is.na(area$actions_taken))\n\n[1] 0\n\nlong &lt;- area %&gt;% separate_longer_delim(actions_taken, \"|\")\n\nlong$actions_taken &lt;- str_trim(long$actions_taken, side = c(\"both\")) #if there is a white space it will delete\nlong$actions_taken &lt;- as.factor(long$actions_taken)\n\n#levels(long$actions_taken)\n#levels(long$district)\nlong$district &lt;- toupper(long$district)\nlong$district &lt;- as.factor(long$district)\n\nlong1 &lt;- subset(long, district =! \"#N/A\" )\n\nWarning: In subset.data.frame(long, district = !\"#N/A\") :\n extra argument 'district' will be disregarded\n\n#levels(long1$district)\n\ndf3 &lt;- long |&gt;\n  group_by(district, actions_taken) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count))\n\n`summarise()` has grouped output by 'district'. You can override using the\n`.groups` argument.\n\ndf4 &lt;- subset(df3, actions_taken != \"None\")\n\ndf3 &lt;- df3 |&gt;\n  group_by(district) |&gt;\n  summarise(count=sum(count))\n\ndf4 &lt;- df4 |&gt;\n  group_by(actions_taken) |&gt;\n  summarise(count=sum(count))\n\ndf3 %&gt;% with(wordcloud(district, count, max.words = 30, random.order = FALSE, rot.per = 0.35, \n    colors = brewer.pal(8, \"Dark2\")))\n\n\n\ndf4 %&gt;% with(wordcloud(actions_taken, count, max.words = 200, random.order = FALSE, rot.per = 0.15, \n    colors = brewer.pal(8, \"Dark2\")))\n\nWarning in wordcloud(actions_taken, count, max.words = 200, random.order =\nFALSE, : Curbside detention could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud(actions_taken, count, max.words = 200, random.order =\nFALSE, : Search of person was conducted could not be fit on page. It will not\nbe plotted.\n\n\nWarning in wordcloud(actions_taken, count, max.words = 200, random.order =\nFALSE, : Handcuffed or flex cuffed could not be fit on page. It will not be\nplotted.\n\n\nWarning in wordcloud(actions_taken, count, max.words = 200, random.order =\nFALSE, : Search of property was conducted could not be fit on page. It will not\nbe plotted.\n\n\n\n\n\n\nReferences\nSan Francisco, California Population 2024. Worldpopulationreview. https://worldpopulationreview.com/us-cities/san-francisco-ca-population\nPolice Department Stop Data. DataSF. https://data.sfgov.org/Public-Safety/Police-Department-Stop-Data/ubqf-aqzw/about_data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "How to ask questions about the projects?\n\nEach project page features introductory details, and questions can be directly posed to the provided emails on that page.\n\n\nAll Topics and Groups\n\nTopics and Groups\n\n\n\n\n\n\nTopics\nGroup Members\n\n\n\n\nAn Archery Game Analysis\nAltanŞener - altansener98@gmail.com\nAbdullah Mert Çelikkol - mertcelikkol10@gmail.com\nAlican Aktağ - alcnaktag@gmail.com\n\n\nTrend Analysis of Road Traffic Accidents of Turkey\nLevent Sari - levent.sari@metu.edu.tr\nHuseyin Tan - huseyin.tan@metu.edu.tr\n\n\nPolice Department Stop Data\nBuse Bakış - buse.bakis@metu.edu.tr\n\n\nWomen’s Leadership and Carbon Disclosure by The Global Energy Companies\nEkinsu Çiçek - ekinsu.cicek@metu.edu.tr\nKübra Nur Akdemir - kubra.akdemir@metu.edu.tr\nMehmet Ali Erkan - erkan.ali@metu.edu.tr\nOğuzhan Aydın - aydin.oguzhan_01@metu.edu.tr\n\n\nData Scientist Job Posting Data Analysis\nMiray Çınar - miray.cinar@metu.edu.tr\nEcenaz Tunç - ecenaz.tunc@metu.edu.tr\n\n\nInvestigation of Terrestrial Water Storage and Precipitation Observations\nÇağatay Çakan - cakan.cagatay@metu.edu.tr\n\n\nEvaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004\nMehmet AKSOY - aksoy.mehmet@metu.edu.tr\nSercan AKIL - sercan.akil@metu.edu.tr\n\n\n\nAlso visit the DSLAB for different projects."
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the dataset.\nAuthor:\nEkinsu Çiçek - ekinsu.cicek@metu.edu.tr\nKübra Nur Akdemir - kubra.akdemir@metu.edu.tr\nMehmet Ali Erkan - erkan.ali@metu.edu.tr\nOğuzhan Aydın - aydin.oguzhan_01@metu.edu.tr"
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#data-descriptions",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#data-descriptions",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "1.Data Descriptions",
    "text": "1.Data Descriptions\n\n\n\n\n\n\n\n\nFile Name\nName of the data in article\nFile description\n\n\n\n\nCID Score (Table A).xlsx\nTable A\nBinary and total CID scores for each company\n\n\nWDs Engagement (Table B).xlsx\nTable B\nPercentage of WDs’ engagement based on their classifications and percentage on the board\n\n\nEWDs Aggregated Score (Table C).xlsx\nTable C\nWDs engagement scores marked by aggregated score among the four dimensions of EWDs.\n\n\n\nThe data set is consist of three different files. The first one is CD Score which is called the Table A basically shows the CID scores, It includes ratings based on some criteria and gives a total CID score for each year. Table B shows the women directors engagement based on some classifications. All data in the Table B saved as percentages. Lastly, Table C show us the EWDs engagement scores and also gives the director names for each company. In the later steps, the new column will be added in the Table C. Additionally, each file includes three different sheets named 2018, 2019, and 2020, each containing recorded data for the corresponding year. See the data schema visual below:\n\nSince the data was published in October 2023, no detailed study has been done on the data yet. According to the author, this data set can be used to create a more sustainable future, the data is helpful for researchers examining women’s participation in net-zero emissions, gender equality, climate resilience, renewable energy, and energy transition in corporate boardrooms (Majid et all., 2023). For this purpose, some analyzes were applied to better understand the data and draw meaningful conclusions from the data."
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#reading-and-cleaning-the-data",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#reading-and-cleaning-the-data",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "2.Reading and Cleaning the Data",
    "text": "2.Reading and Cleaning the Data\nThe following libraries were used in this study to make the necessary arrangements.\n\n2.1 Necessary Libraries\n\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(jmv)\nlibrary(leaflet)\nlibrary(plotrix)\nlibrary(httr)\nlibrary(rvest)\nlibrary(sqldf)\n\nreadxl() is used for reading Excel files into R [4]. Part of the tidyverse, dplyr() is a powerful package for data manipulation [5]. ggplot2 is a popular package for creating static, interactive, and dynamic visualizations in R [6]. plotly() enables the creation of interactive plots and dashboards [7]. jmv() is used for statistical analysis and modelling [8]. leaflet() is a library that creating a interactive maps [9]. plotrix() is used for creating specialized plots and adding features to existing plots [10]. httr() is a package for working http requests, and also making it easier to interact with APIs [11]. rvest() is a web scrapping package in R, used for extracting data from HTML web pages [12]. Lastly, sqldf() allows to perform SQL - like operations on R data frames [13].\n\n\n2.2 Downloading and Reading the Data\nFirstly, local file name is set for the zip file, and download it. As you can see from the link, the data stored in Amazon Web Services. Here you can find the data [14]. Contents are extracted, and defined the names of specific excel files. These files likely contain different data sets related to the project. File names with excel extensions were created for these three different files as CID Scores, WDs Engagement and EWDs Aggregated Score.\n\n#defining the link for the data\nzipF &lt;- \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/d2s9yz65mm-4.zip\"\n#set out the local file name for the zip file\nlocal_zip_filename &lt;- \"Dataset of Women Directors Engagement.zip\"\n#downloading the data\ndownload.file(zipF, local_zip_filename, mode = \"wb\", method = \"auto\")\n#unzip the file\nunzip(local_zip_filename)\n#defining the name of the excel files\nexcel_files &lt;- c(\"CID Scores (Table A).xlsx\", \"WDs Engagement (Table B).xlsx\", \"EWDs Aggregated Score (Table C).xlsx\")\n\n\nCID Scores (Table A)\nThis excel file includes information about binary and total CID scores of the companies between the years of 2018 and 2020. The nine categories and 90 indicators of carbon disclosure procedures are included in Table 1 as a stand-in for CD practices for the sample firms. By assigning a score of 1 for “disclose” and a score of 0 for “not disclose” to each indication, the scoring approach evaluated the degree of CD among the energy leaders. This resulted in scores for each firm as well as an assessment using the content analysis technique. The highest score that may be obtained is 90. At the conclusion of the scoring methodology procedure, the amount of points for exposing any indications in the CD index will be divided.\n\n\n\nView of data in excel\n\n\n\n#defining working directory as the file name\nsetwd(\"Dataset of Women Directors’ Engagement and Carbon Information Disclosures of Global Energy Companies\")\n#reading each sheet in the excel file\nCID_2018 &lt;- read_excel(\"CID Scores (Table A).xlsx\", sheet = \"FYE 2018\")\nCID_2019 &lt;- read_excel(\"CID Scores (Table A).xlsx\", sheet = \"FYE 2019\")\nCID_2020 &lt;- read_excel(\"CID Scores (Table A).xlsx\", sheet = \"FYE 2020\")\n\n\n\n\nView of the data in R\n\n\nAfter the reading sheets, R reads titles as a rows and these column names were deleted. Moreover, there are no column names in the file so that we assigned column names for the tables. For that, instead of the giving column names for each sheets separately, a function that assign all common columns for each sheets was created. So, about the merging phase of these three different sheets, we try to first merge them as a column, but the number of columns has increased a lot, so that new columns was created for each sheets -year column- and combined data(all sheets) by rows with the rbind() in a file, instead of combining columns based. Lastly, the total CID scores of the companies are out of 90 and a different results were obtained for each year so that we combined the newly created year variable with existing total CID scores. In the end, except the company names and year variables, we changed all variables into numeric.\n\n#deleting unnecessary rows\nCID_2018 &lt;- CID_2018[-c(1,2),]\nCID_2019 &lt;- CID_2019[-c(1,2),]\nCID_2020 &lt;- CID_2020[-c(1,2),]\n\n#setting common column names\nset_column_names &lt;- function(data, year) {\n  colnames(data) &lt;- c(\"company_name\", \"strategy_policy\", \"climate_change_opportunities\",\n                      \"corporate_ghg_emissions_targets\", \"company_wide_carbon_footprint\",\n                      \"ghg_emissions_change_over_time\", \"energy_related_reporting\",\n                      \"emission_reduction_initiatives_implementation\",\n                      \"carbon_emission_accountability\", \"quality_of_disclosure\",\n                      paste0(\"quality_of_DisclosureTotal_cid_scores\"),\"year\")\n  \n  data$year &lt;- year\n  \n  return(data)\n}\n\nCID_2018 &lt;- set_column_names(CID_2018,\"2018\")\nCID_2019 &lt;- set_column_names(CID_2019,\"2019\")\nCID_2020 &lt;- set_column_names(CID_2020,\"2020\")\n\n#combining all the tables into one table\nCID_scores &lt;- rbind(CID_2018, CID_2019,CID_2020)\n#converting variables to numeric\nCID_scores &lt;- CID_scores %&gt;%\n  mutate_at(vars(-company_name, -year), as.numeric)\n\n\n\n\nView of the data in R after cleaning operations\n\n\n\n\nWDs Engagement (Table B)\nThis file contains information about women board members classiﬁcation between 2018 and 2020. There are four different predictors in the data set.\n(1) The percentage of women board members\n(2) The percentage of women board members who are industry experts\n(3) The percentage of women board members who act as advisors (ADV\n(4) The percentage of women board member who are community leaders(CL) among the board members for each company.\n\n\n\nView of the data in excel\n\n\nAll sheets are read. Like a first data set, titles seems as rows and column names does not seems well.\n\n#defining working directory as the file name\nsetwd(\"Dataset of Women Directors’ Engagement and Carbon Information Disclosures of Global Energy Companies\")\n#reading each sheet one by one\nWDs_2018 &lt;- read_xlsx(\"WDs Engagement (Table B).xlsx\", sheet = \"FYE 2018\")\nWDs_2019 &lt;- read_xlsx(\"WDs Engagement (Table B).xlsx\", sheet = \"FYE 2019\")\nWDs_2020 &lt;- read_xlsx(\"WDs Engagement (Table B).xlsx\", sheet = \"FYE 2020\")\n\n\n\n\nView of the data in R\n\n\nFirst rows of the data set was deleted, and applied a function to change all column names. Moreover, for combining all sheets, new year variables were created and added to data set. All sheets were combined by rows. As you can seen from the R view, as a mention before the data in this file consists of a percentage, when read them, the decimal numbers were too much. To change this, a function was applied. Firstly, with the mutate_at() functions, company_name and year variables were removed since these variables are categorical variables in a way. Thus, percentage sign was removed with sub() function, and all these values convert into numeric with the as.numeric functions and then round function applied to get two decimals.\n\n#deleting uncesessary rows\nWDs_2018 &lt;- WDs_2018[-1,]\nWDs_2019 &lt;- WDs_2019[-1,]\nWDs_2020 &lt;- WDs_2020[-1,]\n\n#setting common column names\nset_column_names_2 &lt;- function(data, year) {\n  colnames(data) &lt;- c(\"company_name\", \"number_of_wd\", \"per_of_wd_on_board\", \n                      \"per_of_wd_industry_expert\", \n                      \"per_of_wd_advisors\",\"per_of_wd_community_leader\")\n  \n  data$year &lt;- year\n  \n  return(data)\n}\n\nWDs_2018 &lt;- set_column_names_2(WDs_2018,\"2018\")\nWDs_2019 &lt;- set_column_names_2(WDs_2019,\"2019\")\nWDs_2020 &lt;- set_column_names_2(WDs_2020,\"2020\")\n\n#combining all the tables in one table\nWDs_engagement &lt;- rbind(WDs_2018, WDs_2019, WDs_2020)\n\n#converting columns to numeric (excluding company_name,year)\nWDs_engagement &lt;- WDs_engagement |&gt; \n  mutate_at(vars(-company_name,-year), function(x) round(as.numeric(sub(\"%\", \"\", x)), 2))\n\n\n\n\nView of the data in R after cleaning operations\n\n\n\n\nEWDs Aggregated Scores (Table C)\nThe file has the information about EWD’s engagement scores are indicated by the total score across the four different EWD parameters. Compare to the other files, the file also have women director names. There are different binary variables about the director positions that whether the director is expert, adviser or leaders. Moreover, there is a variable about the director years experience as well.\n\n\n\nView of the data in excel\n\n\nAll sheets are read one by one.\n\n#defining working directory as the file name\nsetwd(\"Dataset of Women Directors’ Engagement and Carbon Information Disclosures of Global Energy Companies\")\n#reading each sheet in the excel file\nEWDs_2018 &lt;- read_xlsx(\"EWDs Aggregated Score (Table C).xlsx\", sheet = \"FYE 2018\")\nEWDs_2019 &lt;- read_xlsx(\"EWDs Aggregated Score (Table C).xlsx\", sheet = \"FYE 2019\")\nEWDs_2020 &lt;- read_xlsx(\"EWDs Aggregated Score (Table C).xlsx\", sheet = \"FYE 2020\")\n\n# You can also read these files in a more efficient way:\n# years &lt;- 2018:2020\n# \n# read_ewds_list &lt;- lapply(years, function(year) {\n#   read_xlsx(\"EWDs Aggregated Score (Table C).xlsx\", sheet = paste(\"FYE\", year))\n# })\n# \n# ewds_all &lt;- do.call(rbind, read_ewds_list)\n\n\n\n\nView of the data in R\n\n\nIn this data set, name of the columns are similar to name that we assigned. For the easy interpretation, different column names are set and year column are added to the file.\nAs you can see there is a different issue in the data set compare to previous data. Some company names are empty so we assume the empty places are belong to the company above. When we read the file in R, it’s obvious that R seems these value as NA. Thus, we should replace these empty places with the company names. Initially, we specified how many maximum number of space in a sheet and then it’s known that sheet 2018 includes maximum four empty for a same company name, sheets 2019 includes 5, and sheets 2020 includes 6 maximum space. To fill and the organize these columns, for loops were applied to each different sheets. If a value in the company name column is NA, it replaces the NA value with the previous value in the same column by using lag() function. If value is not NA, it keeps the original value, otherwise, company name. After these procedure, our list file is also ready for the analysis.\n\n#setting common column names\nset.column.names &lt;- function(data,year) {\n  colnames(data) &lt;- c(\"company_name\", \"code\", \"WDsName\", \"industry_expert\", \n                      \"advisor\", \"community_leaders\", \"energy_experiments\", \"log_of_energy_experiment\")\n  data$year &lt;- year\n  \n  return(data)\n}\nEWDs_2018 &lt;- set.column.names(EWDs_2018,\"2018\")\nEWDs_2019 &lt;- set.column.names(EWDs_2019,\"2019\")\nEWDs_2020 &lt;- set.column.names(EWDs_2020,\"2020\")\n\n#replacing name in empty space\nfor (i in 1:4) {EWDs_2018 &lt;- EWDs_2018 |&gt; \n  mutate('company_name' = ifelse(is.na(company_name), lag(company_name), company_name))}\nfor (i in 1:5) {EWDs_2019 &lt;- EWDs_2019 |&gt; \n  mutate('company_name' = ifelse(is.na(company_name), lag(company_name), company_name))}\nfor (i in 1:6) {EWDs_2020 &lt;- EWDs_2020 |&gt; \n  mutate('company_name' = ifelse(is.na(company_name), lag(company_name), company_name))}\n\n#combining all the tables into one table\nEWDs_scores &lt;- rbind(EWDs_2018,EWDs_2019,EWDs_2020)\n\nAfter that, a categorical variable was added to the data set by generating new variables from data manipulation techniques. It would be a nice way to add titles to managers’ years of energy experience. As a result, we divided the years that managers worked into some classes. We classified managers who had no experience or did not work in the energy sector as “no experience” because they did not have any experience in this field. We evaluated directors with working years up to 15 years as assistant director, those with 15 to 30 years of experience as director, and those with more than 30 years as senior director. We combined this classification under the wd_director column.\n\n#converting energy_experiments variable into numeric\nEWDs_scores$energy_experiments &lt;- as.numeric(EWDs_scores$energy_experiments)\n#adding new column as wd_title accoring to the energy_experiments\nEWDs_scores &lt;- EWDs_scores |&gt; \n   mutate(wd_title = cut(energy_experiments, breaks = c(-Inf, 0 ,15, 30, 49),                         labels = c(\"No Experience\",\"Assistant Director\", \"Director\",\"Senior Director\"), include.lowest = TRUE))\n\n\n\n\nView of the data after cleaning in R"
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#descriptive-statistics",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#descriptive-statistics",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "3. Descriptive Statistics",
    "text": "3. Descriptive Statistics\n\nCID Scores Table\n\nsm_CID_scores &lt;- CID_scores |&gt; select(-company_name,-year)\nsummary(sm_CID_scores)\n\n strategy_policy climate_change_opportunities corporate_ghg_emissions_targets\n Min.   :0.000   Min.   :0.000                Min.   :0.00                   \n 1st Qu.:6.000   1st Qu.:5.000                1st Qu.:2.00                   \n Median :7.000   Median :5.000                Median :3.00                   \n Mean   :6.485   Mean   :4.845                Mean   :2.77                   \n 3rd Qu.:7.000   3rd Qu.:5.000                3rd Qu.:4.00                   \n Max.   :7.000   Max.   :5.000                Max.   :5.00                   \n company_wide_carbon_footprint ghg_emissions_change_over_time\n Min.   : 0.000                Min.   :0.000                 \n 1st Qu.: 8.000                1st Qu.:2.000                 \n Median :10.000                Median :3.000                 \n Mean   : 9.364                Mean   :2.718                 \n 3rd Qu.:12.000                3rd Qu.:3.000                 \n Max.   :13.000                Max.   :6.000                 \n energy_related_reporting emission_reduction_initiatives_implementation\n Min.   : 0.000           Min.   : 0.00                                \n 1st Qu.: 7.000           1st Qu.:17.00                                \n Median : 9.000           Median :18.00                                \n Mean   : 8.351           Mean   :17.32                                \n 3rd Qu.:11.000           3rd Qu.:19.00                                \n Max.   :11.000           Max.   :20.00                                \n carbon_emission_accountability quality_of_disclosure\n Min.   :0.000                  Min.   : 0.00        \n 1st Qu.:6.000                  1st Qu.: 8.00        \n Median :6.000                  Median :11.00        \n Mean   :5.763                  Mean   :10.48        \n 3rd Qu.:6.000                  3rd Qu.:14.00        \n Max.   :6.000                  Max.   :17.00        \n quality_of_DisclosureTotal_cid_scores\n Min.   : 0.00                        \n 1st Qu.:64.00                        \n Median :71.00                        \n Mean   :68.19                        \n 3rd Qu.:77.00                        \n Max.   :87.00                        \n\n\nThe collection offers insightful information about how businesses are adapting to climate change, especially when it comes to sustainability efforts and greenhouse gas (GHG) emissions. Interestingly, the average carbon footprint of corporations is 6.485, which highlights the necessity for coordinated efforts in emission mitigation. These footprints vary but are often large. The various GHG emissions objectives, which average 4.845 and show a good trend toward accepting and resolving environmental obligations, demonstrate a commitment to reduction. Moreover, positive trends in the execution of emission reduction programs are evident, as businesses regularly disclose their energy-related operations (average score of 8.351). Strong disclosure quality (average of 10.48) and high accountability scores (average of 5.763) highlight businesses’ accountability for monitoring and openly disclosing carbon emissions. With varied but typically excellent achievements in lowering carbon footprints and adopting transparent reporting procedures, the data set indicates an overall positive trajectory, demonstrating business commitment to sustainability and climate action.\n\n\nWDs Engagement Table\n\nsm_WDs_engagement &lt;- WDs_engagement |&gt; select(-company_name,-year)\nsummary(sm_WDs_engagement)\n\n  number_of_wd   per_of_wd_on_board per_of_wd_industry_expert\n Min.   :0.000   Min.   :0.0000     Min.   :0.0000           \n 1st Qu.:1.000   1st Qu.:0.1200     1st Qu.:0.0800           \n Median :3.000   Median :0.2500     Median :0.2000           \n Mean   :2.763   Mean   :0.2377     Mean   :0.2054           \n 3rd Qu.:4.000   3rd Qu.:0.3300     3rd Qu.:0.3300           \n Max.   :8.000   Max.   :0.6000     Max.   :0.5600           \n per_of_wd_advisors per_of_wd_community_leader\n Min.   :0.000      Min.   :0.0000            \n 1st Qu.:0.080      1st Qu.:0.0000            \n Median :0.200      Median :0.1000            \n Mean   :0.203      Mean   :0.1319            \n 3rd Qu.:0.330      3rd Qu.:0.2150            \n Max.   :0.600      Max.   :0.5000            \n\n\nThe data set provides information on the distribution of women directors (WD) on corporate boards. It shows that there are, on average, 2.76 WD per board, ranging from 0 to 8. The average proportion of female directors is 23.77%, indicating a range in gender representations across quartiles. The range of female directors is 0% to 60%. Furthermore, around 20.54% of female directors contribute their professional experience to the boards, highlighting the significance of fusing specialized knowledge with gender diversity. In addition, data shows that roughly 20.3% of female directors work as advisers, demonstrating their ability to impact strategic choices. With an average percentage of 13.19%, the data set further emphasizes the importance of female directors as community leaders. All things considered, these results paint a complex picture of gender diversity on boards, including differences in the quantity, level of experience, advising capacities, and leadership positions in the community held by women directors.\n\n\nEWDs Scores Table\n\nsm_EWDs_scores &lt;-EWDs_scores |&gt; select(energy_experiments,log_of_energy_experiment,wd_title)\nsummary(sm_EWDs_scores)\n\n energy_experiments log_of_energy_experiment               wd_title  \n Min.   : 0.000     Min.   :0.0000           No Experience     : 73  \n 1st Qu.: 2.000     1st Qu.:0.7945           Assistant Director:560  \n Median : 5.000     Median :1.7918           Director          :112  \n Mean   : 9.435     Mean   :1.7824           Senior Director   : 62  \n 3rd Qu.:13.000     3rd Qu.:2.7081           NA's              : 25  \n Max.   :49.000     Max.   :3.8918                                   \n NA's   :25         NA's   :98                                       \n\n\nThe data set, which has a mean of 9.435 and ranges from 0 to 49, shows a variety of energy experimentation. In order to provide a compact representation, log-transformed values offer a normalized view (0.0000 to 3.8918, mean log of 1.7824). The most common titles are “Assistant Director,” “Director,” and “Senior Director,” with 73 of them having the designation “No Experience.” Nonetheless, the data set has 25 missing values in the energy experiments and 98 missing log-transformed values. The depth of the information resides in its ability to capture a range of responsibilities and expertise levels; hence, more analysis is necessary to identify any potential relationships between professional titles and energy experiments."
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#analysis-and-research-questions",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#analysis-and-research-questions",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "4. Analysis and Research Questions",
    "text": "4. Analysis and Research Questions\n\n4.1 Research Questions:\n\nWhat are the companies with the highest average quality of disclosure total of CID scores of three years 2018, 2019, 2020?\n\n\n#finding the mean cid_scores of three years for each company and slicing the highest ten\nmean_cid &lt;- CID_scores %&gt;%\n    filter(year %in% c(\"2018\", \"2019\", \"2020\")) %&gt;%\n    group_by(company_name) %&gt;%\n    summarize(avg_cid_score = mean(quality_of_DisclosureTotal_cid_scores, na.rm = TRUE)) |&gt; \n  slice_max(order_by=avg_cid_score,n=10)\nmean_cid\n\n# A tibble: 10 × 2\n   company_name                      avg_cid_score\n   &lt;chr&gt;                                     &lt;dbl&gt;\n 1 Hera                                       83.3\n 2 Fairmount Santrol                          83  \n 3 Encana                                     80.7\n 4 Formosa Petrochemical Corporation          80  \n 5 Hellenic Petroleum                         79.7\n 6 Acea SpA                                   79.3\n 7 DCC                                        78.7\n 8 Hess Corporation                           78.7\n 9 PKN ORLEN                                  78.7\n10 Suncor Energy                              78.7\n\n\n\nplot_ly(data = mean_cid, x = ~company_name, y = ~avg_cid_score, type = 'bar', color = ~avg_cid_score) %&gt;%\n  layout(title = \"Top 10 Companies by Average Score\",\n         xaxis = list(title = \"Company Name\", categoryorder = 'total descending'),\n         yaxis = list(title = \"Average Score\", range = c(75, 85)))\n\n\n\n\n\nTop 10 companies by average CID scores for three years are Suncor Energy, Acea SpA, Hera, Fairmount Santrol, Encana, Formosa Petrochemical Corporation, Hellenic Petroleum, PKN ORLEN, Hess Corporation, and DCC. Compared to the other companies, company of Hera has the highest average CID score with 83.33. Then, it’s followed by the company of Fairmount Santrol with 83 average CID score. In the top 10 company, the lowest average CID score is 78.67, and there are four company that has the same score. According to the article of Dataset of exceptional women directors and carbon information disclosures of global energy companies (Majid et al., 2023), all of ten companies are at the high CID ranking\n\nHow does the percentage of experience levels of women directors change between the years 2018 and 2020?\n\n\nlibrary(ggplot2)\nggplot(EWDs_scores, aes(x = wd_title, fill = factor(year))) +\n  geom_bar(position = position_dodge(width = 0.8), stat = \"count\", show.legend = TRUE, width = 0.7) +\n  labs(title = \"Change of WD Titles Over Three Years\",\n       x = \"WD Title\",\n       y = \"Count\") +\n  scale_fill_brewer(palette = \"Set3\") +  # Change the color palette\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels\n  geom_text(stat = \"count\", aes(label = ..count..), position = position_dodge(width = 0.8), vjust = -0.5)\n\n\n\n\nDirectors of companies have four titles according to their experienced year. These are director with no experience, assistant director, director, and senior director. According to the plot, companies have assistant director at most in three years by a wide margin. Percentage of these directors, increased in 2020 from 66% to 74% compared to 2019. As percentage of assistant director increased, percentage of women directors with no experience decreased over three years. This is the biggest decrease, from 12% to 3% compared to change of other titles\n\nDoes the CID score decrease on the average as the number of female directors increases?\n\n\nCID_by_year &lt;- CID_scores |&gt; \n  group_by(year) |&gt; \n  summarize(\n    avg_company_wide_carbon_footprint = mean(company_wide_carbon_footprint),\n    avg_ghg_emissions_change_over_time = mean(ghg_emissions_change_over_time),\n    avg_emission_reduction_initiatives_implementation = mean(emission_reduction_initiatives_implementation),\n    avg_carbon_emission_accountability = mean(carbon_emission_accountability),\n    avg_quality_of_disclosure = mean(quality_of_disclosure),\n    avg_quality_of_DisclosureTotal_cid_scores = mean(quality_of_DisclosureTotal_cid_scores))\n\nThis code group the CID_scores data frame by year and calculate the average of each column for each year. The resulting data frame will have one row for each year and columns for each of the calculated averages.\n\navg_numberwd &lt;- WDs_engagement |&gt; \n  group_by(year) |&gt; \n  summarize(\n    avg_number_of_wd = mean(number_of_wd))\n\nfig &lt;- plot_ly(data = CID_by_year,x = ~year,\n                      y = ~log(avg_quality_of_DisclosureTotal_cid_scores), name = \"Quality of Disclosure Total CID Scores\",\n                      type = \"scatter\",mode = \"lines\") |&gt; \n  add_trace(y = ~avg_numberwd$avg_number_of_wd, name = \"Average number of Women Directors\")\n\nfig\n\n\n\n\n\nGrouping the WDs_engagement data frame by year and calculate the average number of women directors for each year. The resulting data frame will have one row for each year and a column for the calculated average. In order to have a better visualization, I take the log of CID scores. Since the range between these two columns are high, we could not clearly see the increases or decreases in the graph. In the plot, it looks like as the number of WD increases, quality of disclosure Total CID scores tend to decrease. However, since we do not have a enough observations in year basis, and we did not apply a statistical tests, this result is not trustable.\n\nIn the article, authors created to some hypothesis about the data set. Here is the some hypothesis:\n\n1-Women board members who are industry experts will positively enhance carbon disclosure.\n2-Women board members who act as advisers will positively enhance carbon disclosure.\n3-Women board members who serve as community leaders will positively enhance carbon disclosure.\nAuthors specified that some assumptions may not be logical in sense, but we try to find whether these assumptions are true or not. For the test for all assumptions, the code first calculates the average number of women directors for each year using the WDs_engagement data frame. Then, it creates a plot using plot_ly function from the plotly package. The plot has the year on the x-axis and the logarithm of the average quality of disclosure total CID scores on the y-axis. The plot also has three lines representing the average number of women industry experts, community leaders, and advisers. In short, the plot shows the relationship between the Quality of Disclosure Total CID Scores and the Average number of Women Industry Experts, Average number of Women Community Leader, and Average number of Women Advisers.\n\navg_numberwd &lt;- WDs_engagement |&gt; \n  group_by(year) |&gt; \n  summarize(\n    avg_number_of_wd = mean(number_of_wd),\n    avg_industry_expert = mean(per_of_wd_industry_expert),\n    avg_com_leader = mean(per_of_wd_community_leader),\n    avg_wd_advisors = mean(per_of_wd_advisors))\n\nfig2 &lt;- plotly::plot_ly(data = CID_by_year,x = ~year,\n                        y = ~log(avg_quality_of_DisclosureTotal_cid_scores), name = \"Quality of Disclosure Total CID Scores\",\n                        type = \"scatter\",mode = \"lines\") |&gt; \n  add_trace(y = ~avg_numberwd$avg_industry_expert*10, name = \"Average number of Women Industry Experts\") |&gt; \n  add_trace(y = ~avg_numberwd$avg_com_leader*10, name = \"Average number of Women Community Leader\") |&gt; \n  add_trace(y = ~avg_numberwd$avg_wd_advisors*10, name = \"Average number of Women Advisors\")\nfig2\n\n\n\n\n\nSince the range of the values we are interested in in the WD data set is between 0 and 1 and the range of the CID score is between 0 and 80, it would be difficult to see the breaks in the plot when we print the plot. For this reason, we took the log value of the CID score and multiplied the odds in the WD data set by 10 for better plot awareness. It does not look like there exists a meaningful relationship between the total CID scores and the other variables.\n\nHow are energy companies distributed on the map by country and continent?\n\nIt is investigated the distribution of companies around the world both in continental basis and country basis. To do so, firstly, data should be pulled by applying web scraping. Previously, we had company data with longitude and latitude information for the continents.\n\nurl_ulke &lt;- \"https://developers.google.com/public-data/docs/canonical/countries_csv?hl=en\"\n\nIn the website of google designed for developers, there are available data sets. In one of them, we pulled a data set which includes the country name, longitude and latitude of the countries.\n\nres &lt;- GET(url_ulke)\nhtml_con &lt;- content(res, \"text\")\nhtml_ulke &lt;- read_html(html_con)\n\nFirst, we define the url of the data set. Then, by read_html function, we read the url in R. However, data is still not readable in r. It is in xml_document xml_node class.\n\ntables &lt;- html_ulke |&gt; \n  html_nodes(\"table\") |&gt; \n  html_table() # Extract all tables from the webpage\n\nBy html_nodes() function, it is selected all the HTML table elements in the document. Then, html_table() converts the selected HTML tables into data frames.\n\nlong_lat_country &lt;- tables[[1]]\nlong_lat_country &lt;- long_lat_country[,-1]\ncolnames(long_lat_country)[3] &lt;- \"Country\"\n\n\n\n\nnew data set after scraping\n\n\nThen, we assigned the first table extracted from the HTML document to the variable long_lat_country. The table includes information about the latitude and longitude of different countries, code and name of the countries. We dropped the country code variable, and edit the column names of the country name.\nMoreover, we take the company names, continents and country where the company is located from appendix part in the article. [15]\n\ncompany_2 &lt;- read.csv('company_2.csv', sep = \";\",header = T)\n\n\n\n\ndata from the article\n\n\nLastly, we combined two data by same column called “Country”. One of the them is web scrapping data, another is the data was taken in the article (company). Now, in addition to its company names, it also includes country, continent and geographical location information. In order to have all the information in company data set, we used all.x=TRUE option in merge function.\n\ncombined_data &lt;- merge(x = company_2, y = long_lat_country, by = \"Country\", all.x = TRUE) \n\n\ndf &lt;- sqldf::sqldf(\"SELECT Country, continent, latitude, longitude, COUNT(*) AS Freq \n             FROM combined_data\n             GROUP BY Country\n                   ORDER BY Freq DESC\")\ndf_last &lt;- (sqldf::sqldf(\"SELECT Freq,A.* FROM df B LEFT JOIN combined_data A ON A.latitude=B.latitude\"))\n\nTo investigate the distribution of companies all around the world, we group them by country. By doing so, we can easily see how many companies there are for each continent. For the better understanding of the data, we order the data by its frequency.\n\ncenter_lon &lt;- median(df$longitude, na.rm = TRUE)\ncenter_lat &lt;- median(df$lattitude, na.rm = TRUE)\nlibrary(leaflet)\n\ngetColor &lt;- function(df) {\n  sapply(df$Freq, function(mag) {\n    if(mag &lt;= 2) {\n      \"green\"\n    } else if(mag &lt;= 4) {\n      \"orange\"\n    } else {\n      \"red\"\n    } })\n}\nicons &lt;- awesomeIcons(\n  icon = 'ios-close',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = getColor(df)\n)\n\nThe median values of longitude and latitude are calculated in order to arrange the zoom of the map. However, we did not use this because the distances between the continents were too far. In the map, we defined this function to visually see how the number of companies varies from country to country. The cut off values are determined by its quarters.\n\nstr(df)\n\n'data.frame':   31 obs. of  5 variables:\n $ Country  : chr  \"United States\" \"United Kingdom\" \"Italy\" \"India\" ...\n $ Continent: chr  \"North America\" \"Europe\" \"Europe\" \"Asia\" ...\n $ latitude : num  37.1 55.4 41.9 20.6 46.2 ...\n $ longitude: num  -95.71 -3.44 12.57 78.96 2.21 ...\n $ Freq     : int  19 8 7 7 5 5 4 4 3 3 ...\n\nleaflet() |&gt; \n  addProviderTiles(providers$Esri,\n                   options = providerTileOptions(noWrap = TRUE)) |&gt; \n  addAwesomeMarkers(\n    data = df_last,\n    lng = ~longitude,\n    lat = ~latitude,\n    label = ~Country,\n    icon = icons,\n    popup = ~paste(\"&lt;br&gt;Number of Company:\", Freq,\n                   \"&lt;br&gt;Company Names:\", company_name),\n    clusterOptions = markerClusterOptions()\n) \n\n\n\n\n\nleaflet() functions initializes a new Leaflet map. addProviderTiles() adds a tile layer from a known map provider. addAwesomeMarkers() adds markers to the map. The markers are created using data from the df_son. Lng and lat specify the longitude and latitude columns in the data frame. Label specifies the label column in the data frame. Icon specifies the icon for the markers. Popup specifies the HTML content for the popups that appear when a marker is clicked. clusterOptions specifies the options for clustering the markers."
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#conclusion",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#conclusion",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "5.Conclusion:",
    "text": "5.Conclusion:\nThis project examines the relationship between women’s leadership and carbon disclosure in the top 100 global energy companies from 2018 to 2020. The study focuses on the energy sector’s role as a major polluter and explores the intersection of gender equality, women’s issues, and climate change, aligned with Sustainable Development Goals. The data set, collected over three years from content analysis of reports from 97 companies, aims to evaluate the influence of women’s leadership on carbon disclosure in these energy companies. R programming is used for analysis, and the results are presented using Quarto. The data set comprises three files: CD Scores (carbon disclosure), WDs Engagement (women directors’ classification), and EWDs Aggregated Scores. The article outlines the process of reading and cleaning each file, addressing challenges like empty company names. It highlights the significance of the data for potential research on gender equality, climate mitigation, and the role of women in the energy sector, emphasizing the use of various R packages for data manipulation and analysis. Overall, the project provides a comprehensive overview of the research’s methodology and the importance of the data set in addressing crucial issues in the energy industry. We generate some research questions. In order to answer these questions, we pull data set from the website. Also, we made some operations within the data and create new variables and sub-data sets. To answer these questions visually, we used plotly, leaflet and ggplot. Moreover, we try to give answer some hypothesis by author and can’t find any meaningful results for the hypothesis. The projects is one of the leading project for this data set and it is open to any feedback, comments and developments."
  },
  {
    "objectID": "MehmetAli_STAT570/EnergyWomen570Final.html#references",
    "href": "MehmetAli_STAT570/EnergyWomen570Final.html#references",
    "title": "Women’s Leadership and Carbon Disclosure by The Global Energy Companies",
    "section": "6. References:",
    "text": "6. References:\n[1] Abd Majid, Nurshahirah, and Amar Hisham Jaaffar. “The Effect of Women’s Leadership on Carbon Disclosure by the Top 100 Global Energy Leaders.” MDPI, Multidisciplinary Digital Publishing Institute, 23 May 2023, www.mdpi.com/2071-1050/15/11/8491. \n[2] Nurshahirah Abd Majid a, et al. “Dataset of Exceptional Women Directors and Carbon Information Disclosures of Global Energy Companies.” Data in Brief, Elsevier, 5 Oct. 2023, www.sciencedirect.com/science/article/pii/S2352340923007357. \n[3] Quarto, quarto.org/.\n[4] Wickham H, Bryan J (2023). readxl: Read Excel Files. https://readxl.tidyverse.org, https://github.com/tidyverse/readxl.\n[5] Wickham H, François R, Henry L, Müller K, Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://github.com/tidyverse/dplyr, https://dplyr.tidyverse.org.\n[6] H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\n[7] “Plotly.” Plotly r Graphing Library in R, plotly.com/r/.\n[8] “The ‘jamovi’ Analyses [R Package Jmv Version 2.4.11].” The Comprehensive R Archive Network, Comprehensive R Archive Network (CRAN), 12 Oct. 2023, cran.r-project.org/web/packages/jmv/index.html. \n[9] “Create Interactive Web Maps with the JavaScript ‘leaflet’ Library [R Package Leaflet Version 2.2.1].” The Comprehensive R Archive Network, Comprehensive R Archive Network (CRAN), 13 Nov. 2023, cran.r-project.org/web/packages/leaflet/index.html. \n[10] “Package Plotrix.” CRAN, Comprehensive R Archive Network (CRAN), cran.r-project.org/web/packages/plotrix/index.html.\n[11] Wickham H (2023). httr: Tools for Working with URLs and HTTP. https://httr.r-lib.org/, https://github.com/r-lib/httr.\n[12] Wickham H (2023). rvest: Easily Harvest (Scrape) Web Pages. https://rvest.tidyverse.org/, https://github.com/tidyverse/rvest.\n[13] Grothendieck G (2017). sqldf: Manipulate R Data Frames Using SQL. R package version 0.4-11, https://CRAN.R-project.org/package=sqldf.\n[14] abd majid, nurshahirah (2023), “Dataset of Women Directors’ Engagement and Carbon Information Disclosures of Global Energy Companies”, Mendeley Data, V4, doi: 10.17632/d2s9yz65mm.4\n[15] TY - JOUR AU - Abd Majid, Nurshahirah AU - Jaafar, Amar PY - 2023/05/23 SP - 8491 T1 - The Effect of Women’s Leadership on Carbon Disclosure by the Top 100 Global Energy Leaders VL - 15 DO - 10.3390/su15118491 JO - Sustainability ER -"
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data set.\n\n\nLevent Sarı - levent.sari@metu.edu.tr\nHüseyin Tan - huseyin.tan@metu.edu.tr\n\n\nIn today’s world, traffic accidents emerge as a serious public health issue globally. Every year, thousands of lives are lost, and tens of thousands of individuals are injured. This situation adversely affects not only individuals but also the overall well-being of society. The increasing frequency of traffic accidents once again emphasizes the importance of a safe transportation environment.\nTraffic accidents continue to be a pressing issue worldwide, posing significant threats to public safety, economic stability, and overall well-being. In Turkey, a country with a dynamic transportation landscape marked by rapid urbanization and increased vehicular traffic, understanding and addressing the factors contributing to traffic accidents is of paramount importance.\nWith the rapidly growing use of transportation vehicles and roads, the causes and effects of accidents have become more complex. Factors such as driver errors, infrastructure deficiencies, weather conditions, and traffic congestion increase the likelihood of accidents. This underscores the need for more efforts in developing safe transportation systems and addressing existing issues effectively.\n\n\n\nAI Generated Image (generated by ChatGPT)\n\n\nIn this study, three different data sets were used and our goal is to examine the number of deaths and injuries in traffic accidents in Turkey between 2002 and 2022 according to age ranges. Also, We will share with you estimated number of road traffic death rate around the world and the current position of Turkey.\nOur goal is :\n\nConvert untidy data sets to tidy data sets.\nCreating a different data set from the edited data sets.\nExplaining data with graphics and tables."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#traffic-accident-death-and-injury-data",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#traffic-accident-death-and-injury-data",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data set.\n\n\nLevent Sarı - levent.sari@metu.edu.tr\nHüseyin Tan - huseyin.tan@metu.edu.tr\n\n\nIn today’s world, traffic accidents emerge as a serious public health issue globally. Every year, thousands of lives are lost, and tens of thousands of individuals are injured. This situation adversely affects not only individuals but also the overall well-being of society. The increasing frequency of traffic accidents once again emphasizes the importance of a safe transportation environment.\nTraffic accidents continue to be a pressing issue worldwide, posing significant threats to public safety, economic stability, and overall well-being. In Turkey, a country with a dynamic transportation landscape marked by rapid urbanization and increased vehicular traffic, understanding and addressing the factors contributing to traffic accidents is of paramount importance.\nWith the rapidly growing use of transportation vehicles and roads, the causes and effects of accidents have become more complex. Factors such as driver errors, infrastructure deficiencies, weather conditions, and traffic congestion increase the likelihood of accidents. This underscores the need for more efforts in developing safe transportation systems and addressing existing issues effectively.\n\n\n\nAI Generated Image (generated by ChatGPT)\n\n\nIn this study, three different data sets were used and our goal is to examine the number of deaths and injuries in traffic accidents in Turkey between 2002 and 2022 according to age ranges. Also, We will share with you estimated number of road traffic death rate around the world and the current position of Turkey.\nOur goal is :\n\nConvert untidy data sets to tidy data sets.\nCreating a different data set from the edited data sets.\nExplaining data with graphics and tables."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#literature-review",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#literature-review",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "Literature Review",
    "text": "Literature Review\nBefore conducting our analysis, we decided to examine previous studies about Turkey’s road traffic accidents, injuries and deaths. First and foremost, a study by Kaygisiz Et Al., (2017) considers a road traffic accident (RTA) to be the ones that include people or vehicles, and that happen on road. Earlier studies (Naci & Baker, 2008) reported that Turkey needed to focus on collecting data in an organized manner to understand various reasons related to the accidents, also suggesting that it could be a key element in reducing losses from them (Esiyok Et Al., 2005). Recent research (Erenler & Gumus, 2019) mentions that RTAs are one of the ten primary reasons of mortality worldwide, and even higher in developing countries. The study also provides the information that for persons between the ages 15 and 29, RTAs are the highest ranked reason for death. Many elements contribute to the causes of accidents, one of them being economic growth. While the study by Puvanachandra Et Al. (2012) claims that regions with lower income have higher RTA fatalities, the study of Erenler & Gumus (2019) suggests that developing countries experience more severe fatalities from RTAs. When the causes are investigated on an accident basis, another study (Sungur Et Al., 2014) that brings an epidemiological lookout on the case reports that less than 1% of the accidents are caused by non-human factors, while 95% of the time, the responsible party is the driver. It is also said that DUIs and exhaustion are of the most common reasons. On the economic impacts of RTAs, Naci & Baker (2008) suggest that in 2000, the deaths occurring on RTAs have impacted Turkey’s economy with a negative $2.6 Billion only by hindering productivity. Also, in the recent study of Ozturk (2022), child fatalities are also considered to have a considerable contribution to Turkey’s health burden. Thus, our study aims to use three data sets obtained from TURKSTAT and World Health Organization (WHO) to observe trends and provide comparisons of Turkey to other countries."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-sets",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-sets",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "Data sets",
    "text": "Data sets\nThree different data sets were used in this project. Two of these data sets were taken from TURKSTAT and one from the WHO website.\nFirst TURKSTAT Data Set contains following columns:\n\nAccidents involving death and personal injury\nAccidents involving material loss only\nTotal number of accidents\nYear\n\nSecond TURKSTAT Data Set contains following columns:\n\nKilled Persons\nInjured Persons\nAge Groups\nYear\n\nWHO Data Set contains following columns:\n\nCountries\nEstimated number of road traffic death rate\nYear\n\n\nTURKSTAT raw Data Sets:\nYou can view screenshots of the data sets in their original format below.\nFirst Data Set\n\nSecond Data Set\n\nTURKSTAT Data set Problems:\n\nThe spreadsheets starts and ends with a some text.\nColumn names are written separately in both English and Turkish.\nSome columns are left blank for visual purposes.\nThe Age group data divided into two group in the same sheet.\nData is not in the long format.\n\nWHO data set is tidy and available for study."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-collection-and-pre-processing",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-collection-and-pre-processing",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "Data Collection and Pre processing",
    "text": "Data Collection and Pre processing\nFirst, we will load all the libraries and define functions that will be needed throughout the study.\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(rvest)    \nlibrary(gridExtra)\nlibrary(ggrepel)\nlibrary(directlabels)\nlibrary(DT)\n\nread_clean &lt;- function(..., sheet){\n  read_excel(..., sheet = sheet)\n}\n\noptions(scipen = 999999)\n\nreorderFactors &lt;- function(df, column = \"my_column_name\", \n                           desired_level_order = c(\"fac1\", \"fac2\", \"fac3\")) {\n  \n  x = df[[column]]\n  lvls_src = levels(x) \n  \n  idxs_target &lt;- vector(mode=\"numeric\", length=0)\n  for (target in desired_level_order) {\n    idxs_target &lt;- c(idxs_target, which(lvls_src == target))\n  }\n  \n  x_new &lt;- factor(x,levels(x)[idxs_target])\n  \n  df[[column]] &lt;- x_new\n  \n  return(df)\n}\n\nHere, the scipen = 999999 option removes scientific notation of numbers and lets us create graphs with better axis break labels. The user-defined reorderFactors function is useful in ordering factors to our desired axis layout while plotting, as base R functions such as order sometimes fail to work within the ggplot2 library. To do this, the function stores the factor levels and orders them based on the desired index input.\nAfter loading required libraries, we will download our first data from TURKSTAT website.\n\nurl = 'https://data.tuik.gov.tr/Bulten/DownloadIstatistikselTablo?p=8RC9RpGXOVWg1rPaE6MQ4FUE37S8S2vsiIJglnqCOrpJfrRCUPa5n3wsXEnCI0Xf'\n\nraw_data = tempfile(fileext = \".xls\")\ndownload.file(url, raw_data,\n              method = \"auto\",\n              mode = \"wb\")\n\nsheets &lt;- excel_sheets(raw_data)\n\nread_clean &lt;- function(..., sheet){\n  read_excel(..., sheet = sheet)\n}\n\nraw_data &lt;- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 2,\n              sheet = .)\n) |&gt;\n  bind_rows()\n\nhead(raw_data,10)\n\n# A tibble: 10 × 8\n   ...1  `Toplam kaza` `Maddi hasarlı`    `Ölümlü, yaralanmalı` `Ölü sayısı (1)`\n   &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;              &lt;chr&gt;                 &lt;chr&gt;           \n 1 &lt;NA&gt;  sayısı        kaza sayısı        kaza sayısı           Killed persons …\n 2 Yıl   Total number  Accidents involvi… Accidents involving … Toplam          \n 3 Year  of accidents  material loss only and personal injury   Total           \n 4 2002  439777        374029             65748                 4093            \n 5 2003  455637        388606             67031                 3946            \n 6 2004  537352        460344             77008                 4427            \n 7 2005  620789        533516             87273                 4505            \n 8 2006  728755        632627             96128                 4633            \n 9 2007  825561        718567             106994                5007            \n10 2008  950120        845908             104212                4236            \n# ℹ 3 more variables: ...6 &lt;chr&gt;, ...7 &lt;chr&gt;, Yaralı &lt;chr&gt;\n\n\nThe data from TURKSTAT traditionally comes including the data name and translation in its first two rows. Thus, we are skipping those rows using skip = 2 in the read_clean function. Afterwards, taking a glimpse into the data, we can see that the column names and some rows need further processing.\n\ndata_1 = raw_data %&gt;% select(1:4) %&gt;% slice(-1)\ndata_1 = rbind(data_1, paste(data_1[1,],data_1[2,]))\ndata_2 = data_1 %&gt;% slice(3:23,30)\ndata_2 = data_2 %&gt;% row_to_names(22,remove_rows_above = FALSE)\ncolnames(data_2)[1] = 'Year'\ndata_accidents = data_2; rm(data_1); rm(data_2); rm(raw_data)\ndata_accidents = data_accidents %&gt;% \n  mutate(across(where(is.character),as.numeric))\n\nSince the second data will have common information with the first data, we decided to keep only the unique four columns of it with select(1:4), also since the first row was corrupted, it is removed using the slice(-1) command. Then, we combined the first and second rows of the new data to create proper column names. Lastly, we manually removed the Turkish translation of Year column, selected the rows that had data in them and fixed the column classes of the data before moving on to the second TURKSTAT data set.\n\nurl = 'https://data.tuik.gov.tr/Bulten/DownloadIstatistikselTablo?p=2/Sym42hc5kOF437mqcxligj8l5uHDGvvOSKXfSdmBmVHwkyus9lGDyc36ojWVBg'\n\nraw_data = tempfile(fileext = \".xls\")\ndownload.file(url, raw_data,\n              method = \"auto\",\n              mode = \"wb\")\n\nsheets &lt;- excel_sheets(raw_data)\n\n\nraw_data &lt;- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 2,\n              sheet = .)\n) |&gt;\n  bind_rows()\n\nhead(raw_data,30)\n\n# A tibble: 30 × 12\n   ...1  Yaş grupları - Age gr…¹ ...3  ...4  ...5  ...6  ...7  ...8  ...9  ...10\n   &lt;chr&gt; &lt;chr&gt;                   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;\n 1 &lt;NA&gt;  0 - 9                   &lt;NA&gt;  NA    10 -… &lt;NA&gt;  NA    15 -… &lt;NA&gt;  NA   \n 2 Yıl   Ölü sayısı (1)          Yara… NA    Ölü … Yara… NA    Ölü … Yara… NA   \n 3 Year  Killed persons (1)      Inju… NA    Kill… Inju… NA    Kill… Inju… NA   \n 4 2002  322                     8788  NA    84    4524  NA    68    4572  NA   \n 5 2003  178                     7149  NA    68    3992  NA    67    4533  NA   \n 6 2004  225                     8148  NA    80    4642  NA    66    5152  NA   \n 7 2005  179                     9077  NA    108   5988  NA    58    6095  NA   \n 8 2006  178                     9237  NA    89    6133  NA    74    6673  NA   \n 9 2007  179                     10333 NA    89    6790  NA    95    7337  NA   \n10 2008  151                     9486  NA    80    6689  NA    65    6930  NA   \n# ℹ 20 more rows\n# ℹ abbreviated name: ¹​`Yaş grupları - Age groups`\n# ℹ 2 more variables: ...11 &lt;chr&gt;, ...12 &lt;chr&gt;\n\n\nIn the second TURKSTAT data, the data reading process is repeated. When the first 30 rows of the data is observed, it can be seen that the data is split into two and merged vertically. To solve this problem, We split the data into two considering the last year ‘2022’ to be the last row for each part. Also, we removed that the data contained all NA columns for styling purposes in Excel that were unnecessary.\n\ndata_1 = raw_data %&gt;% \n  slice(1:49) %&gt;% \n  filter(row_number() &lt;= min(which(...1 == 2022))) %&gt;% \n  select(where(~!all(is.na(.)))) %&gt;% \n  slice(-2)\n\nhead(data_1)\n\n# A tibble: 6 × 9\n  ...1  `Yaş grupları - Age groups` ...3     ...5  ...6  ...8  ...9  ...11 ...12\n  &lt;chr&gt; &lt;chr&gt;                       &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 &lt;NA&gt;  0 - 9                       &lt;NA&gt;     10 -… &lt;NA&gt;  15 -… &lt;NA&gt;  18 -… &lt;NA&gt; \n2 Year  Killed persons (1)          Injured… Kill… Inju… Kill… Inju… Kill… Inju…\n3 2002  322                         8788     84    4524  68    4572  129   7143 \n4 2003  178                         7149     68    3992  67    4533  120   6880 \n5 2004  225                         8148     80    4642  66    5152  122   7984 \n6 2005  179                         9077     108   5988  58    6095  140   8990 \n\n\nLooking at the head of the data, we saw that the age groups are only labeled on the first columns they show up in. To fix that issue before merging the first rows and naming columns according to them, in a similar manner to the first TURKSTAT data, we filled NA values with last non NA value before them in that row.\n\nfor (i in 2:ncol(data_1)) {\n  if (is.na(data_1[1, i])) {\n    if (!is.na(data_1[1, i - 1])) {\n      data_1[1, i] = data_1[1, i - 1]\n    }\n  }\n}\n\ndata_1 = rbind(paste(data_1[1,],data_1[2,]),data_1)\ndata_1 = data_1 %&gt;% \n  slice(-2,-3) %&gt;% \n  row_to_names(1,remove_rows_above = F)\ncolnames(data_1)[1] = 'Year'\n\nSince the data is in wide format, we used regex patterns and pivot_longer function to transform it into long format.\n\ndata_transformed_1 = data_1 %&gt;%\n  pivot_longer(cols = -Year, \n               names_to = c(\"Age_Group\", \"Killed_Or_Injured\"), \n               names_pattern = \"(\\\\d+\\\\s-\\\\s\\\\d+)\\\\s(\\\\w+)\\\\spersons\") %&gt;%\n  mutate(Age_Group = gsub(\"-\", \"_\", Age_Group)) %&gt;%\n  filter(!is.na(value))\n\nThe same process is repeated for the second part of the data. However, since age groups ‘65+’ and ‘Unknown’ do not fit the format of our selection regex, we decided to manually input the NA returns using loops for those values.\n\ndata_2 = raw_data %&gt;% \n  slice(1:49) %&gt;% \n  filter(row_number() &gt; min(which(...1 == 2022))) %&gt;% \n  select(where(~!all(is.na(.)))) %&gt;% \n  slice(-1,-3)\n\nfor (i in 2:ncol(data_2)) {\n  if (is.na(data_2[1, i])) {\n    if (!is.na(data_2[1, i - 1])) {\n      data_2[1, i] = data_2[1, i - 1]\n    }\n  }\n}\n\ndata_2 = rbind(paste(data_2[1,],data_2[2,]),data_2)\ndata_2 = data_2 %&gt;% \n  slice(-2,-3) %&gt;% \n  row_to_names(1,remove_rows_above = F)\ncolnames(data_2)[1] = 'Year'\n\ndata_transformed_2 = data_2 %&gt;%\n  pivot_longer(cols = -Year, \n               names_to = c(\"Age_Group\", \"Killed_Or_Injured\"), \n               names_pattern = \"(\\\\d+\\\\s-\\\\s\\\\d+)\\\\s(\\\\w+)\\\\spersons\") %&gt;%\n  mutate(Age_Group = gsub(\"-\", \"_\", Age_Group)) %&gt;%\n  filter(!is.na(value))\n\nfor(i in 1:nrow(data_transformed_2)){\n  if(is.na(data_transformed_2[i,\"Age_Group\"])){\n    if(i%%2==1){\n      data_transformed_2[i,\"Age_Group\"] = '65 +'\n      data_transformed_2[i,\"Killed_Or_Injured\"] = 'Killed'\n    }\n    if(i%%2==0){\n      data_transformed_2[i,\"Age_Group\"] = 'Unknown'\n      data_transformed_2[i,\"Killed_Or_Injured\"] = 'Injured'\n    }\n  }\n}\n\nThe two parts of the data are then merged.\n\ndata_fin = rbind(data_transformed_1,data_transformed_2)\n\ndata_fin = data_fin %&gt;% \n  mutate(Year=as.numeric(Year),\n         value = as.numeric(value))\n\ndata_accidents_full = data_fin %&gt;% \n  left_join(data_accidents,by = 'Year')\n\nrm(data_1,data_2,data_accidents,data_fin,data_transformed_1,data_transformed_2,raw_data)\n\nLastly, to observe Turkey’s position in accidents and death rates among European countries, the third data set from WHO website is downloaded and read into R.\n\n#The link for the following data: https://www.who.int/data/gho/data/indicators/indicator-details/GHO/estimated-road-traffic-death-rate-(per-100-000-population)\n\nwho_data = read.csv('data.csv')\nwho_data_filtered = who_data %&gt;% \n  select(Location,Period,Value) %&gt;% \n  separate(Value, into = c('Value'), sep = ' '); rm(who_data)\n\nwho_data_filtered = who_data_filtered %&gt;% \n  mutate(Period = as.factor(Period),\n         Value = as.numeric(Value))\n\nhead(who_data_filtered)\n\n                          Location Period Value\n1              Antigua and Barbuda   2019  0.00\n2 Micronesia (Federated States of)   2019  0.16\n3                         Maldives   2019  1.63\n4                         Kiribati   2019  1.92\n5                            Egypt   2019 10.10\n6                          Ukraine   2019 10.20\n\n\nAs seen in the code, no pre-processing apart from selecting useful columns and fixing their classes is needed for the WHO data as it is already tidy."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-visualization-and-conclusions",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#data-visualization-and-conclusions",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "Data Visualization and Conclusions",
    "text": "Data Visualization and Conclusions\nIn exploratory analysis, with the help of tidyverse and datatable packages, various pivot tables as well as plots can be created.\n\nYearly Number of Accidents in Turkey\n\nTo view the table of yearly accidents in Turkey, as well as the summary statistics, we will first create two data tables. For the first frequency table, simply selecting the desired columns and piping them into a datatable() function is satisfactory.\n\ndata_accidents_full %&gt;% \n  select(Year,`Total number of accidents`,`Accidents involving material loss only`,`Accidents involving death and personal injury`) %&gt;% \n  unique() %&gt;% \n  datatable(class = \"compact\",\n            caption = 'Yearly Number of Accidents\\nin Turkey',\n            options = list(pageLength = nrow(.)))\n\n\n\n\n\nFor the summary statistics table, we will first select the columns we want to work with, calculate their summary statistics, convert the output into a data frame and then place the data frame into the datatable() function.\n\ndata_accidents_full %&gt;% \n  select(`Total number of accidents`,`Accidents involving material loss only`,`Accidents involving death and personal injury`) %&gt;% \n  summary() %&gt;% \n  as.data.frame() %&gt;% \n  select(Var2, Freq) %&gt;% \n  datatable(class = 'compact',\n            options = list(pageLength = nrow(.)),\n            colnames = c('Type','Sum. Stat.'),\n            caption = 'Summary Statistics of Accidents in Turkey between 2002-2022')\n\n\n\n\n\nWe will also visualize the data to make it more accessible.\n\ndata_accidents_full %&gt;% \n  select(Year,`Total number of accidents`,`Accidents involving material loss only`,`Accidents involving death and personal injury`) %&gt;% \n  group_by(Year) %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;% \n  unique() %&gt;%\n  gather('Accident','Count',-Year) %&gt;% \n  filter(!Accident=='Total number of accidents') %&gt;% \n  ggplot(.,aes(x = Year,y = Count, fill = Accident, group = Accident)) +\n  geom_bar(stat = 'identity') + \n  geom_text(aes(label = format(after_stat(y), big.mark = \".\", scientific = FALSE), group = Accident), fontface = 'bold',\n            stat = 'summary', fun = sum, vjust = 0.5,hjust = 1.4, position = position_stack()) +\n  theme_minimal() + \n  labs(x = 'Year', y = 'Count', title = 'Accidents per Year in Turkey', subtitle = '2002-2022') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'right',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4')) + \n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  coord_flip() + \n  scale_fill_manual(values =c('#ea7286','#a9c484'))\n\n\n\n\nThe plot above shows the Yearly Deaths and Injuries and Material Loss only Number in Turkey. Between 2002 and 2012 total number of accident count increasing each year. In 2012, Accident involving material loss only reach the peak point. Also, in 2022 Accident involving death and personal injury reach the peak point. The frequencies of deaths and injuries for every age group can be seen in the table below on a yearly manner.\n\ndata_accidents_full %&gt;% \n  select(Year,Age_Group,Killed_Or_Injured,value) %&gt;% \n  pivot_wider(names_from = c(\"Killed_Or_Injured\", \"Age_Group\"),values_from = \"value\") %&gt;% \n  rename_with(~ gsub(\"Killed_\", \"Killed Age: \", .), starts_with(\"Killed_\")) %&gt;%\n  rename_with(~ gsub(\"Injured_\", \"Injured Age: \", .), starts_with(\"Injured_\")) %&gt;% \n  datatable(class = \"compact\",\n            caption = 'Yearly Killed or Injured Persons by Age Groups\\nin Turkey',\n            options = list(pageLength = nrow(.)))\n\n\n\n\n\nWe also decided to go into detail about the percentage of deaths and injuries in accidents involving death/personal injury. To do so, it is enough to use mutate function to create killed and injured percentages before gathering the data for ggplot.\n\ndata_accidents_full %&gt;%\n  group_by(Year, Killed_Or_Injured) %&gt;%\n  mutate(Year = as.factor(Year)) %&gt;% \n  summarise(total_count = sum(value,na.rm = T)) %&gt;%\n  spread(Killed_Or_Injured, total_count, fill = 0) %&gt;%\n  mutate(Total = Killed + Injured,\n         Killed_percentage = (Killed / Total) * 100,\n         Injured_percentage = (Injured / Total) * 100) %&gt;%\n  select(Year, Killed_percentage, Injured_percentage) %&gt;% \n  gather('Type','Percentage',-Year) %&gt;% \n  ggplot(.,aes(x = Year,y = `Percentage`, fill = Type)) +\n  geom_bar(stat = 'identity') + \n  geom_text(aes(label = paste0('% ', round(Percentage,2))),fontface = 'bold',size = 5,hjust = 0.3) + \n  theme_minimal() + \n  labs(x = 'Year', y = 'Percentage', title = 'Death/Injury Percentages per Year in Turkey', subtitle = '2002-2022') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'right',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4')) + \n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  coord_flip() + \n  scale_fill_manual(values = c('#eab281','#ea7286'))\n\n\n\n\nIn this chart, we showed the deaths and injuries as percentages by year. Each year, between one and three percent of people involved in traffic accidents result in death. To detail our findings further, we decided to create line-graphs that investigate trends on deaths/injuries per age group, as well as overall deaths/injuries yearly. Here, we filtered the 25-64 age group to create a separate plot from the others as the magnitude of their statistics vastly out-range other groups. Then, using the grid_arrange function from the gridExtra library, we merge the plots into a single output.\n\npa1 = data_accidents_full %&gt;% \n  select(Age_Group,Year,value) %&gt;% \n  unique() %&gt;%\n  filter(Year %in% c(2002:2020)) %&gt;%\n  filter(!Age_Group == '25 _ 64') %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;% \n  group_by(Age_Group,Year) %&gt;% \n  summarise(value = sum(value,na.rm = T)) %&gt;% \n  mutate(label = if_else(Year == 2020, as.character(Age_Group), NA_character_)) %&gt;%\n  ggplot(.,aes(x = Year, y = value, group = Age_Group, color = Age_Group)) + \n  geom_line(size = 1) + \n  geom_point(size = 2.2) +\n  theme_minimal() + \n  geom_label_repel(aes(label = label),\n                   nudge_x = 1,\n                   na.rm = TRUE,\n                   fontface = 'bold') +\n  scale_color_manual(values = c('#eab281','#e3e19f','#a9c484','#5d937b','#58525a','#a07ca7','#f4a4bf'))+\n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  labs(x = 'Year', y = 'Persons', title = 'Persons Killed/Injured in Accidents', subtitle = '2002-2019, All Age Groups\\nNot Including 25-64') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\n\n\npa2 = data_accidents_full %&gt;% \n  select(Age_Group,Year,value) %&gt;% \n  unique() %&gt;%\n  filter(Year %in% c(2002:2020)) %&gt;% \n  filter(Age_Group == '25 _ 64') %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;% \n  group_by(Age_Group,Year) %&gt;% \n  summarise(value = sum(value,na.rm = T)) %&gt;% \n  ggplot(.,aes(x = Year, y = value, group = 1, color = '#eab281')) + \n  geom_line(size = 1) + \n  geom_point(size = 2.2) +\n  theme_minimal() + \n  scale_color_manual(values = c('#ea7286')) +\n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  labs(x = 'Year', y = 'Persons', title = 'Persons Killed/Injured in Accidents', subtitle = '2002-2019, Ages 25-64') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\n\n\ngrid.arrange(pa1,pa2,ncol = 1)\n\n\n\n\nAs you can see, each age group have similar trend except 65+ age group. In the first graph, 21-24 age group has the highest number of death or injury and the lowest group 65+ age group.\nSimilarly, in two line plots, death and injuries in Turkey are drawn to visualize the trend for the country overall.\n\np1 = data_accidents_full %&gt;% \n  filter(Year %in% c(2002:2020)) %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;% \n  filter(Killed_Or_Injured == 'Killed') %&gt;% \n  select(Year,`Accidents involving death and personal injury`,Killed_Or_Injured,value) %&gt;% \n  group_by(Year,Killed_Or_Injured) %&gt;% \n  summarise(value = sum(value)) %&gt;% \n  ggplot(.,aes(x = Year, y = value, group = Killed_Or_Injured, color = Killed_Or_Injured)) + \n  geom_line(size = 1) + \n  geom_point(size = 2.2) +\n  geom_text(aes(label = format(after_stat(y), big.mark = \".\", scientific = FALSE), fontface = 'bold'),\n            stat = 'summary', fun = sum, vjust = 0.5,hjust = -0.4, color = 'black') + \n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  theme_minimal() + \n  labs(x = 'Year', y = 'Persons', title = 'Persons Killed in Accidents', subtitle = '2002-2019') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\n\n\np2 = data_accidents_full %&gt;% \n  filter(Year %in% c(2002:2020)) %&gt;% \n  mutate(Year = as.factor(Year)) %&gt;% \n  filter(Killed_Or_Injured == 'Injured') %&gt;% \n  select(Year,`Accidents involving death and personal injury`,Killed_Or_Injured,value) %&gt;% \n  group_by(Year,Killed_Or_Injured) %&gt;% \n  summarise(value = sum(value)) %&gt;% \n  ggplot(.,aes(x = Year, y = value, group = Killed_Or_Injured, color = Killed_Or_Injured)) + \n  geom_line(size = 1) + \n  geom_point(size = 2.2) +\n  geom_text(aes(label = format(after_stat(y), big.mark = \".\", scientific = FALSE), fontface = 'bold'),\n            stat = 'summary', fun = sum, vjust = 0.5,hjust = -0.4, color = 'black') + \n  theme_minimal() + \n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  labs(x = 'Year', y = 'Persons', title = 'Persons Injured in Accidents', subtitle = '2002-2019') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\ngrid.arrange(p1,p2,ncol = 1)\n\n\n\n\n\nYearly Death Rates of Countries Comparison\n\nIn this graph, we see separately the number of people death or injured in traffic accidents. We see no stable trend in both graphs. The number of deaths doubled in 2015 compared to the previous year. The reason for this is explained in the text parts of the data that we deleted.\nUntil year 2015, figures on persons killed include the deaths only at the accident area however since year 2015 figures on persons killed also include the deaths within 30 days after the traffic accidents due to related accident and its impacts for people injured and sent to health facilities. Lastly, using the data we obtained from the WHO, we will observe how Turkey’s death rate on accidents fares against selected countries. First, we will observe yearly death rates per 100.000 people in Turkey.\n\nwho_data_filtered %&gt;% \n  filter(Location=='Türkiye') %&gt;% \n  ggplot(.,aes(x = Period,y = `Value`, fill = '#ea7286')) +\n  geom_bar(stat = 'identity') + \n  geom_text(aes(label = Value),fontface = 'bold',size = 5,hjust = -0.3) + \n  theme_minimal() + \n  labs(x = 'Year', y = 'Death Rate per 100.000 People', title = 'Road Traffic Death Rates per 100.000 People in Turkey', subtitle = 'Estimated, 2000-2019') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4')) + \n  coord_flip() + \n  scale_fill_manual(values = c('#eab281'))\n\n\n\n\nIn the first plot, we can see that while the rates were mostly declining to between 6-7 since 2000, the death rate in Turkey has peaked in 2011, almost doubling the year before. Afterwards, it took a steady decline until 2019, where it seems to have returned to pre-2011 death rates.\nSecondly, we can see how Turkey fares against G7 countries in road traffic death rates\n\nwho_data_filtered %&gt;% \n  filter(Location=='Türkiye' | Location=='Canada' | Location=='France' | Location=='Germany' | Location =='Italy' | Location == 'Japan'\n         | Location == 'United Kingdom of Great Britain and Northern Ireland'\n         | Location == 'United States of America') %&gt;% \n  mutate(label = if_else(Period == '2019', as.character(Location), NA_character_)) %&gt;%\n  ggplot(.,aes(x = Period, y = Value, group = Location, color = Location)) + \n  geom_line(size = 1.2) + \n  geom_point(size = 2.2) +\n  theme_minimal() + \n  geom_label_repel(aes(label = label),\n                   nudge_x = 1,\n                   na.rm = TRUE,\n                   fontface = 'bold') + \n  scale_color_manual(values = c('#ea7286','#eab281','#e3e19f','#a9c484','#5d937b','#58525a','#a07ca7','#f4a4bf'))+\n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  labs(x = 'Year', y = 'Death Rate per 100.000 People', title = 'Death Rates per 100.000 People', subtitle = '2000-2019, Turkey & G7 Countries') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\n\n\n\n\nWhen we compare Turkey with G7 countries, we can see that while Turkey had similar death rates with European countries and Japan until 2011, the sharp spike in 2011 separated us from the remaining countries, almost reaching the level USA. It can also be seen that the USA always had higher death rates compared to European G7 countries, Turkey, and Japan. To see the trend in Turkey more clearly, we can also highlight Turkey from the rest of the countries.\n\nwho_data_filtered %&gt;% \n  filter(Location=='Türkiye' | Location=='Canada' | Location=='France' | Location=='Germany' | Location =='Italy' | Location == 'Japan'\n         | Location == 'United Kingdom of Great Britain and Northern Ireland'\n         | Location == 'United States of America') %&gt;% \n  mutate(label = if_else(Period == '2019', as.character(Location), NA_character_)) %&gt;%\n  ggplot(.,aes(x = Period, y = Value, group = Location, color = Location)) + \n  geom_line(size = 1.2) + \n  geom_point(size = 2.2) +\n  theme_minimal() + \n  geom_label_repel(aes(label = label),\n                   nudge_x = 1,\n                   na.rm = TRUE,\n                   fontface = 'bold') + \n  scale_color_manual(values = c('grey','grey','grey','grey','grey','darkred','grey','grey'))+\n  scale_y_continuous(labels=function(x) format(x, big.mark = \".\", scientific = FALSE)) +\n  labs(x = 'Year', y = 'Death Rate per 100.000 People', title = 'Death Rates per 100.000 People', subtitle = '2000-2019, Turkey & G7 Countries') +\n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4'))\n\n\n\n\nWe will also look for the 20-year averages for those countries, as well as the country with the highest and lowest average. First, we will filter the data and order the countries in a decreasing manner.\n\nwho_data_filtered_plot = who_data_filtered %&gt;% \n  group_by(Location) %&gt;% \n  summarise(Value = mean(Value)) %&gt;% \n  filter(Location=='Türkiye' | Location=='Canada' | Location=='France' | Location=='Germany' | Location =='Italy' | Location == 'Japan'\n         | Location == 'United Kingdom of Great Britain and Northern Ireland'\n         | Location == 'United States of America' | Value == min(Value) | Value == max(Value)) \nwho_data_filtered_plot$Location = as.factor(who_data_filtered_plot$Location)\nwho_data_filtered_plot[order(who_data_filtered_plot$Value,decreasing = T),]\n\n# A tibble: 10 × 2\n   Location                                             Value\n   &lt;fct&gt;                                                &lt;dbl&gt;\n 1 Thailand                                             36.8 \n 2 United States of America                             13.6 \n 3 Italy                                                 8.58\n 4 Türkiye                                               8.11\n 5 France                                                7.67\n 6 Canada                                                7.49\n 7 Japan                                                 6.74\n 8 Germany                                               5.90\n 9 United Kingdom of Great Britain and Northern Ireland  4.75\n10 Maldives                                              2.16\n\n\nFor extra safety, we will use the pre-defined reorderFactors function to order their levels, allowing us to use the data in ggplot as is.\n\nwho_data_filtered_plot = reorderFactors(who_data_filtered_plot,'Location',c('Maldives','United Kingdom of Great Britain and Northern Ireland','Germany',\n                                                                            'Japan','Canada','France','Türkiye','Italy','United States of America','Thailand'))\n\nThen, we can create the plot and highlight the countries we want using ggplot.\n\nwho_data_filtered_plot %&gt;% \n  ggplot(.,aes(x = Location,y = `Value`, fill = Location)) +\n  geom_bar(stat = 'identity') + \n  geom_text(aes(label = Value),fontface = 'bold',size = 5,hjust = -0.3) + \n  theme_minimal() + \n  labs(x = 'Year', y = 'Death Rate per 100.000 People', title = 'Death Rates per 100.000 People in Turkey', subtitle = '2000-2019') +\n  scale_y_continuous(limits = c(0,45)) + \n  theme(axis.text.x = element_text(size = 12, face = 'bold'),\n        axis.title.x = element_text(size = 13, face = 'bold'),\n        axis.text.y = element_text(size = 12, face = 'bold'),\n        axis.title.y = element_text(size = 13, face = 'bold'),\n        legend.position = 'none',\n        title = element_text(size = 14, face = 'bold'),\n        plot.subtitle = element_text(size = 13, face = 'italic'),\n        plot.background = element_rect(fill = '#F4F4F4'),\n        panel.background = element_rect(fill = '#F4F4F4'),\n        strip.background = element_rect(fill = '#F4F4F4')) + \n  coord_flip() + \n  scale_fill_manual(values = c('#a9c484','grey','grey','grey','grey','grey','#eab281','grey','grey','darkred'))\n\n\n\n\nBy looking at the last plot, we can see that on average, road accident death rates of Turkey for the 20 year span has been close to European countries. The country with the lowest death rate, Maldives, has a death rate of almost one fourth of Turkey on average while the country with the highest death rate, Thailand, has a death rate higher than four times of the average of Turkey."
  },
  {
    "objectID": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#references",
    "href": "LeventSari_HuseyinTan/STAT570_Levent_Huseyin.html#references",
    "title": "Trend Analysis of Road Traffic Accidents of Turkey",
    "section": "References",
    "text": "References\nErenler, A. K., & Gumus, B. (2019). Analysis of Road Traffic Accidents in Turkey between 2013 and 2017. Medicina, 55(10), 679. doi:10.3390/medicina55100679\nEsiyok, B., Korkusuz, I., Canturk, G., Alkan, H. A., Karaman, A. G., & Hamit Hanci, I. (2005). Road traffic accidents and disability: A cross-section study from Turkey. Disability and Rehabilitation, 27(21), 1333–1338. doi:10.1080/09638280500164867\nKaygisiz, O., Senbil, M., & Yildiz, A. (2017). Influence of urban built environment on traffic accidents: The case of Eskisehir (Turkey). Case Studies on Transport Policy, 5(2), 306–313. doi:10.1016/j.cstp.2017.02.002\nNaci, H., & Baker, T. D. (2008). Productivity losses from road traffic deaths in Turkey. International Journal of Injury Control and Safety Promotion, 15(1), 19–24. doi:10.1080/17457300701847648\nOzturk E. A. (2022). Burden of deaths from road traffic injuries in children aged 0-14 years in Turkey. Eastern Mediterranean health journal = La revue de sante de la Mediterranee orientale = al-Majallah al-sihhiyah li-sharq al-mutawassit, 28(4), 272–280. https://doi.org/10.26719/emhj.22.013\nPuvanachandra, P., Hoe, C., Ozkan, T., & Lajunen, T. (2012). Burden of Road Traffic Injuries in Turkey. Traffic Injury Prevention, 13(sup1), 64–75. doi:10.1080/15389588.2011.633135\nSungur, I., Akdur, R., & Piyal, B. (2014). Analysis of Traffic Accidents in Turkey. Ankara Medical Journal, 14(3). https://doi.org/10.17098/amj.65427"
  },
  {
    "objectID": "Water_Budget/TWS_PRCP_v02.html",
    "href": "Water_Budget/TWS_PRCP_v02.html",
    "title": "Investigation of Terrestrial Water Storage and Precipitation Observations",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the dataset.\nÇağatay Çakan cakan.cagatay@metu.edu.tr"
  },
  {
    "objectID": "Water_Budget/TWS_PRCP_v02.html#introduction",
    "href": "Water_Budget/TWS_PRCP_v02.html#introduction",
    "title": "Investigation of Terrestrial Water Storage and Precipitation Observations",
    "section": "1 Introduction",
    "text": "1 Introduction\nWater is one of the essential resources in the world. It is used for many purposes such as agricultural, industrial, and domestic activities. The excessive use of water could be a problem in the future. For this reason, it should be tracked. Investigation of terrestrial water storage (TWS) is one way of tracking the water. TWS is defined as the total amount of water stored on land. In other words, TWS can be described as the sum of snow, ice, surface water, soil moisture, and groundwater. I want to focus on the cause of terrestrial water storage variability. The change in the storage can be explained by the water balance equation. The formula of the change in the storage is following:\n\\[ dS/dt = P - ET - R \\]\nwhere, \\(dS/dt\\) is the change in the storage, \\(P\\) is the precipitation, \\(ET\\) is the evapotranspiration, and \\(R\\) is the runoff. Assuming the relationship between \\(P\\) and \\(ET+R\\) is constant for a region, thus the variability of the storage come from precipitation."
  },
  {
    "objectID": "Water_Budget/TWS_PRCP_v02.html#data",
    "href": "Water_Budget/TWS_PRCP_v02.html#data",
    "title": "Investigation of Terrestrial Water Storage and Precipitation Observations",
    "section": "2 Data",
    "text": "2 Data\n\n2.1 TWS Observations\nTWS observations are obtained from the Gravity Recovery and Climate Experiment (GRACE) and GRACE Follow-On (GRACE-FO). They are two missions and each mission has twin satellites. GRACE operated between April 2002 and July 2017, and GRACE-FO is launched May 2018 and the data is available up to date. The working principle of these satellite is related to gravity and surface mass. They measure temporal variations of Earth’s gravitational potential. After removing the atmospheric and oceanic effects, the remaining signal on monthly to inter-annual timescales is mostly related to variations of TWS. The data set have monthly temporal resolution and 1° spatial resolution. However, the preprocessed version of the data is available which is JPL GRACE and GRACE-FO Mascon Product. This data has spatial resolution of 0.5°.\n\n\n\nGRACE and GRACE-FO Working Principle. Source of the image: https://gracefo.jpl.nasa.gov/resources/50/how-grace-fo-measures-gravity/\n\n\n\n\n2.2 Precipitation Observations\nThe precipitation observations are obtained from the Global Precipitation Climatology Project (GPCP) Monthly Analysis Product. The precipitation data has monthly temporal resolution. However, its spatial resolution is 2.5°."
  },
  {
    "objectID": "Water_Budget/TWS_PRCP_v02.html#data-handling",
    "href": "Water_Budget/TWS_PRCP_v02.html#data-handling",
    "title": "Investigation of Terrestrial Water Storage and Precipitation Observations",
    "section": "3 Data Handling",
    "text": "3 Data Handling\nThe TWS and precipitation observation data will be handled in this part. ncdf4 package is used to handle netcdf file. lubridate package is used for date. ggplot2 package is used for figures. fields package is used for image.plot function. rvest package is used to handle html files. raster package is used for resample function.\n\nlibrary(ncdf4) \nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(fields)\nlibrary(rvest)\nlibrary(raster)\nsetwd(here::here())\nmain_folder = getwd()\n\n\n3.1 TWS\n\n3.1.1 Downloading TWS Data\nThe GRACE and GRACE-FO TWS data as the Mascon Product can be downloaded from here. After clicking the data, you should sign in and the data will be downloaded automatically. First, what the data contains should be checked.\n\n\n3.1.2 Handling TWS Data\n\ninname110 &lt;- paste0(main_folder, '/01-GRACE/GRCTellus.JPL.200204_202310.GLO.RL06.1M.MSCNv03CRI.nc')\n\nnc_file1 &lt;-  nc_open(inname110)\n\nThe downloaded data are in netcdf format. This type of format is used in hydro-meteorology a lot. The view of this format in R can be seen in the below figure.\n\n\n\nThe View of the GRACE and GRACE-FO Mascon Product\n\n\nThe lwe_thickness variable is used as TWS data for this dataset.\n\n\n\nThe View of the lwe_thickness Variable in the GRACE and GRACE-FO Mascon Product\n\n\n\n# The units of the variable\nnc_file1[[\"var\"]][[\"lwe_thickness\"]][[\"units\"]]\n\n[1] \"cm\"\n\n# The dimensions of the variable\nnc_file1[[\"var\"]][[\"lwe_thickness\"]][[\"varsize\"]]\n\n[1] 720 360 226\n\n\nThe units of the data is cm. The data has 0.5° spatial resolution, so longitude data contains 360/0.5 = 720 points, latitude data contains 180/0.5 = 360 points. Up to this point, everything is perfect. However, the last column shows 226 points. The observed period is between 2002 and 2023. So there are 22 years, in other words there are 264 months. Thus, there are some missing months, but which ones are missing? Probably, up to the April 2002, there are missing 3 months because the mission was not started at that period, and after the October 2023, there are missing 2 months because the data are not available for that period. What about others? How could you decide which months are missing? Check the time variable!\n\nnc_file1[[\"dim\"]][[\"time\"]]\n\n$name\n[1] \"time\"\n\n$len\n[1] 226\n\n$unlim\n[1] FALSE\n\n$group_index\n[1] 1\n\n$group_id\n[1] 65536\n\n$id\n[1] 2\n\n$dimvarid\n$id\n[1] 2\n\n$group_index\n[1] 1\n\n$group_id\n[1] 65536\n\n$list_index\n[1] -1\n\n$isdimvar\n[1] TRUE\n\nattr(,\"class\")\n[1] \"ncid4\"\n\n$units\n[1] \"days since 2002-01-01T00:00:00Z\"\n\n$calendar\n[1] \"gregorian\"\n\n$vals\n  [1]  106.5  129.5  227.5  258.0  288.5  319.0  349.5  380.5  410.0  439.5\n [11]  470.0  495.5  561.5  592.5  623.0  653.0  684.0  714.5  736.5  777.0\n [21]  805.5  836.0  866.5  897.0  927.5  958.5  989.0 1019.5 1050.0 1080.5\n [31] 1111.5 1141.0 1170.5 1201.0 1231.5 1262.0 1292.5 1323.5 1354.0 1384.5\n [41] 1415.0 1445.5 1476.5 1506.0 1535.5 1566.0 1596.5 1627.0 1657.5 1688.5\n [51] 1719.0 1749.5 1780.0 1810.5 1841.5 1871.0 1900.5 1931.0 1961.5 1992.0\n [61] 2022.5 2053.5 2084.0 2114.5 2145.0 2175.5 2206.5 2236.5 2266.5 2297.0\n [71] 2327.5 2358.0 2388.5 2419.5 2450.0 2480.5 2511.0 2541.5 2572.5 2602.0\n [81] 2631.5 2662.0 2692.5 2723.0 2753.5 2784.5 2815.0 2845.5 2876.0 2906.5\n [91] 2937.5 2967.0 2996.5 3027.0 3057.5 3088.0 3118.5 3149.5 3180.0 3210.5\n[101] 3241.0 3269.5 3335.5 3361.5 3392.0 3422.5 3485.5 3514.5 3545.0 3575.5\n[111] 3591.5 3652.0 3667.5 3697.5 3727.5 3746.5 3819.0 3849.5 3880.5 3908.0\n[121] 3974.5 4002.5 4033.5 4062.0 4128.0 4153.5 4184.0 4214.5 4306.5 4337.0\n[131] 4367.5 4391.5 4457.5 4488.0 4518.5 4546.0 4610.5 4641.0 4671.5 4703.0\n[141] 4769.5 4793.0 4822.5 4853.0 4864.0 4943.5 4975.5 5004.5 5104.5 5128.5\n[151] 5157.0 5188.5 5253.0 5280.0 5309.5 5346.5 5444.5 5471.5 5499.0 5568.5\n[161] 5592.5 5610.5 5640.0 6010.0 6034.0 6147.5 6163.0 6193.5 6224.5 6253.0\n[171] 6283.5 6314.0 6344.5 6375.0 6405.5 6436.5 6467.0 6497.5 6528.0 6558.5\n[181] 6589.5 6619.5 6649.5 6680.0 6710.5 6741.0 6771.5 6802.5 6833.0 6863.5\n[191] 6894.0 6924.5 6955.5 6985.0 7014.5 7045.0 7075.5 7106.0 7136.5 7167.5\n[201] 7198.0 7228.5 7259.0 7289.5 7320.5 7350.0 7379.5 7410.0 7440.5 7471.0\n[211] 7501.5 7532.5 7563.0 7593.5 7624.0 7654.5 7685.5 7715.0 7744.5 7775.0\n[221] 7805.5 7836.0 7866.5 7897.5 7928.0 7958.5\n\n$create_dimvar\n[1] TRUE\n\nattr(,\"class\")\n[1] \"ncdim4\"\n\n\nThe units of the time is day since 2002-01-01T00:00:00Z, and there are values. These values are represent the day since January 1st, 2002. Thus, the exact location of the data could be found, and relocate the data called GRF_TWS.\n\nstart_date &lt;-  as.Date('2002-01-01')\n\nnc_tws &lt;-  ncvar_get(nc_file1, 'lwe_thickness')\nnc_tim &lt;-  ncvar_get(nc_file1, 'time')\nTWS_lon &lt;-  ncvar_get(nc_file1, 'lon')\nTWS_lat &lt;-  ncvar_get(nc_file1, 'lat')\ntimlen &lt;-  length(nc_tim)\n\nGRF_TWS &lt;-  array(NA, dim = c(720,360,12,22))\n\nfor (j in 1:timlen) {\n  nc_date &lt;-  start_date + nc_tim[j]\n  nc_mon0 &lt;-  as.numeric(format(nc_date, format = \"%m\"))\n  nc_year &lt;-  as.numeric(format(nc_date, format = \"%Y\"))\n  year_loc &lt;-  nc_year - 2002 + 1 \n  GRF_TWS[,,nc_mon0,year_loc] = nc_tws[,,j]\n}\n\nCheck the time series of the global spatial mean of TWS.\n\nGRF_TWS_3D &lt;-  GRF_TWS\ndim(GRF_TWS_3D) &lt;-  c(dim(GRF_TWS)[1], dim(GRF_TWS)[2], dim(GRF_TWS)[4]*12)\nTWS_TS &lt;- colMeans(GRF_TWS_3D, na.rm = T, dims = 2)\n\nstart_date &lt;- ymd(\"2002-01-01\")\nend_date   &lt;- ymd(\"2023-12-31\")\nTWS_date &lt;- seq(start_date, end_date, by = \"months\")\n\nTWS_df &lt;- data.frame(TWS_date, TWS_TS)\n\nggplot(TWS_df, aes(x = TWS_date, y = TWS_TS)) +\n  geom_line() +\n  scale_x_date(limit = c(as.Date(\"2002-01-01\"), as.Date(\"2025-06-01\"))) +\n  labs(x = \"\", y = \"Spatial Mean of TWS (cm)\")\n\n\n\n\nTime Series\n\n\n\n\nThe missing data are shown in this figure. The reason for this is that there is a battery problem in the satellites especially after 2011. Also, there is a gap between May 2017 and July 2018 because there is no satellite for that period. Let’s see what the data looks like.\n\nGRF_TWS_2D &lt;-  rowMeans(GRF_TWS, na.rm = T, dims = 2)\nimage.plot(TWS_lon, TWS_lat, GRF_TWS_2D, xlab = 'longitude', \n           ylab = 'latitude', main = 'Temporal Mean of TWS (cm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Actual TWS Data\n\n\n\n\nThe shape of the figure is not clear, so some mask should be applied. There is a land_mask in the data. Let’s use this land_mask:\n\nnc_land_mask1 &lt;-  ncvar_get(nc_file1, 'land_mask')\nnc_land_mask2 &lt;- nc_land_mask1\nnc_land_mask2[nc_land_mask2 == 0] &lt;-  NA\nland_mask &lt;- array(nc_land_mask2, dim = dim(GRF_TWS))\nGRF_TWS_masked &lt;- GRF_TWS*land_mask\nGRF_TWS_masked_2D &lt;-  rowMeans(GRF_TWS_masked, na.rm = T, dims = 2)\nimage.plot(TWS_lon, TWS_lat, GRF_TWS_masked_2D, \n           xlab = 'longitude', ylab = 'latitude', \n           main = 'Temporal Mean of TWS (cm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Masked TWS Data\n\n\n\n\nThe representation of the globe is not suitable for the hydro-meteorological variables. Their longitude is going from -180 to 180.\n\ndimx &lt;- dim(GRF_TWS_masked)[1]\nGRF_TWS_masked2 &lt;-  GRF_TWS_masked\nfor (xx in 1:dimx) {\n  \n  if (xx &lt;= 360) {GRF_TWS_masked2[xx,,,] &lt;-  GRF_TWS_masked[xx+360,,,]}\n  if (xx &gt; 360) {GRF_TWS_masked2[xx,,,] &lt;-  GRF_TWS_masked[xx-360,,,]}\n\n}\n\nTWS_lat2 = TWS_lat\nTWS_lon2 = seq(-179.75, 179.75, 0.5)\n\nCheck the image again.\n\nGRF_TWS_masked_2_2D &lt;-  rowMeans(GRF_TWS_masked2, na.rm = T, dims = 2)\nimage.plot(TWS_lon2, TWS_lat2, GRF_TWS_masked_2_2D, \n           xlab = 'longitude', ylab = 'latitude', \n           main = 'Temporal Mean of TWS (cm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Masked TWS Data (Rearranged Longitude)\n\n\n\n\nThat’s perfect. The TWS data is ready for the analysis part. However, let’s look at the variability of temporal mean of TWS data.\n\nGRF_TWS_masked_2_2D &lt;-  rowMeans(GRF_TWS_masked2, na.rm = T, dims = 2)\nimage.plot(TWS_lon2, TWS_lat2, GRF_TWS_masked_2_2D, xlab = 'longitude', \n           ylab = 'latitude', main = 'Temporal Mean of TWS (cm)',\n           zlim = c(-30,30))\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\nWarning in temp$z: partial match of 'z' to 'zlim'\n\n\n\n\n\nTemporal Mean of Masked TWS Data (Rearranged Longitude and Limit of TWS (-30,30))\n\n\n\n\n\n\n\n3.2 Precipitation\n\n3.2.1 Downloading Precipitation Data\nThe GPCP precipitation observations could be downloaded from here. However, there is a problem to download the data. Each year has separate folders, and these folders include separate files. Moreover, there is no specific pattern of the files. For this reason, read_html function was used to extract the file names in each folder.\n\nyears &lt;-  2002:2023\n\nlen_year &lt;-  length(years)\n\nurl01 = 'https://www.ncei.noaa.gov/data/global-precipitation-climatology-project-gpcp-monthly/access/'\n\nfor (yyyy in 1:len_year) {\n  url02 &lt;-  paste0(url01, years[yyyy],'/')\n  \n  file_list02 &lt;-  read_html(url02) |&gt;\n    html_nodes(\"a\") |&gt;\n    html_text(trim = T)\n  \n  file_list_gpcp &lt;-  file_list02[grepl('gpcp_',file_list02)]\n  \n  len_file2 &lt;-  length(file_list_gpcp)\n  \n  for (ff02 in 1:len_file2) {\n    inname02 &lt;-  paste0(url02, file_list_gpcp[ff02])\n    outname02 &lt;-  paste0(main_folder,'/02-GPCP-Precipitation/',\n                         file_list_gpcp[ff02])\n    if (file.exists(outname02)) {next}\n    download.file(inname02, outname02, mode=\"wb\")\n  }\n}\n\n\n\n3.2.2 Handling of Precipitation Data\nThe view of this format in R can be seen in the figure.\n\n\n\nView of the netcdf file in R\n\n\nThe detail of the precipitation data is in the precip variable so it should be extracted. The view of this variable can be seen in the figure.\n\n\n\nThe View of the Precipitation Data in the precip Variable\n\n\nThe dimensions of the data for one month can be seen in this figure. The dimensions are 144, 72. Precipitation observations were downloaded for each month separately between 2002 and 2023. For easier use in later stages of the study, precipitation data was assigned to a single variable called GPCP_PRCP. In this step, longitude and latitude of the data were extracted.\n\ninfold1100 &lt;-  paste0(main_folder, '/02-GPCP-Precipitation/')\n\nfile_list1 &lt;-  list.files(infold1100)\nlen_file1  &lt;-  length(file_list1)\nfile_list2 &lt;-  file_list1[1:258]\nlen_file2  &lt;- length(file_list2)\n\ndim_year &lt;-  ceiling(len_file2/12)\n\nGPCP_PRCP &lt;-  array(NA, dim = c(144,72,12,dim_year))\n\ninname0000 &lt;-  paste0(infold1100, file_list2[1])\nnc_file2 &lt;-  nc_open(inname0000)\nnc_close(nc_file2)\n\nfor (ff01 in 1:len_file2) {\n  inname0000 &lt;-  paste0(infold1100, file_list2[ff01])\n  \n  year &lt;-  as.numeric(substr(file_list2[ff01],22,25))\n  month &lt;-  as.numeric(substr(file_list2[ff01],26,27))\n  year_loc &lt;-  year-2002+1\n  \n  nc_file3 &lt;-  nc_open(inname0000)\n  GPCP_PRCP0 &lt;-  ncvar_get(nc_file3, 'precip')\n  PRCP_lon &lt;-  ncvar_get(nc_file3, 'longitude')\n  PRCP_lat &lt;-  ncvar_get(nc_file3, 'latitude')\n  nc_close(nc_file3)\n  \n  GPCP_PRCP[,,month,year_loc] &lt;-  GPCP_PRCP0\n}\n\nhead(PRCP_lon)\n\n[1]  1.25  3.75  6.25  8.75 11.25 13.75\n\ntail(PRCP_lon)\n\n[1] 346.25 348.75 351.25 353.75 356.25 358.75\n\nhead(PRCP_lat)\n\n[1] -88.75 -86.25 -83.75 -81.25 -78.75 -76.25\n\ntail(PRCP_lat)\n\n[1] 76.25 78.75 81.25 83.75 86.25 88.75\n\n\nThe longitude of the data starts from 1.25° and going up to 358.75°, while the latitude of the data starts from -88.75° and going up to 88.75°.\nSince TWS observations have 1° spatial resolution, the precipitation data was down scaled from 2.5° to 1° using bilinear interpolation using raster package. The unit of the precip variable in the netcdf file is mm/day. However, the unit should be mm/month. Thus, each data is multiple by the days in the corresponding month.\n\ndimx &lt;-  dim(GPCP_PRCP)[1]\ndimy &lt;-  dim(GPCP_PRCP)[2]\ndim4 &lt;-  dim(GPCP_PRCP)[4]\n\nr &lt;-  raster(nrow = 720, ncol = 360)\n\nWarning in .local(...): partial argument match of 'nrow' to 'nrows'\n\n\nWarning in .local(...): partial argument match of 'ncol' to 'ncols'\n\nextent(r) &lt;-  extent(c(0,1,0,1))\nGPCP_PRCP_Downscaled &lt;-  array(NA, dim = c(720, 360, 12, dim4))\n\ndaymon &lt;-  c(31,28,31,30,31,30,31,31,30,31,30,31)\n\nfor (yyyy in 1:dim4) {\n  for (mm in 1:12) {\n    PRCP1 &lt;-  GPCP_PRCP[,,mm,yyyy]\n    PRCP2 &lt;-  raster(PRCP1)\n    PRCP3 &lt;-  resample(PRCP2, r, metod = 'bilinear')\n    PRCP4 &lt;-  as.matrix(PRCP3)*daymon[mm]\n    GPCP_PRCP_Downscaled[,,mm,yyyy] &lt;-  PRCP4\n    \n  }\n}\n\nCheck the time series of the spatial mean of precipitation.\n\nGPCP_PRCP_Downscaled_3D &lt;-  GPCP_PRCP_Downscaled\ndim(GPCP_PRCP_Downscaled_3D) &lt;-  \n  c(dim(GPCP_PRCP_Downscaled)[1], dim(GPCP_PRCP_Downscaled)[2], \n    dim(GPCP_PRCP_Downscaled)[4]*12)\nGPCP_PRCP_TS &lt;- colMeans(GPCP_PRCP_Downscaled_3D, na.rm = T, dims = 2)\n\nstart_date &lt;- ymd(\"2002-01-01\")\nend_date   &lt;- ymd(\"2023-12-31\")\nGPCP_PRCP_date &lt;- seq(start_date, end_date, by = \"months\")\n\nGPCP_PRCP_df &lt;- data.frame(GPCP_PRCP_date, GPCP_PRCP_TS)\n\nggplot(GPCP_PRCP_df, aes(x = GPCP_PRCP_date, y = GPCP_PRCP_TS)) +\n  geom_line() +\n  scale_x_date(limit = c(as.Date(\"2002-01-01\"), as.Date(\"2025-06-01\"))) +\n  labs(x = \"\", y = \"Spatial Mean of Precipitation (mm)\")\n\n\n\n\nSpatial Mean of Precipitation\n\n\n\n\nLet’s see what the data looks like.\n\nPRCP_lon &lt;-  seq(0.25, 359.75, 0.5)\nPRCP_lat &lt;-  seq(-89.75, 89.75, 0.5)\nGPCP_PRCP_Downscaled_2D &lt;-  rowMeans(GPCP_PRCP_Downscaled, na.rm = T, dims = 2)\nimage.plot(PRCP_lon, PRCP_lat, GPCP_PRCP_Downscaled_2D, \n           xlab = 'longitude', ylab = 'latitude', \n           main = 'Temporal Mean of Precipitation (mm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Actual Precipitation Data\n\n\n\n\nThe shape of the figure is not clear, so some mask should be applied. Same land_mask with TWS data can be applied.\n\nGPCP_PRCP_Downscaled_masked &lt;- GPCP_PRCP_Downscaled*land_mask\nGPCP_PRCP_Downscaled_masked_2D &lt;- rowMeans(GPCP_PRCP_Downscaled_masked, na.rm = T, dims = 2)\nimage.plot(PRCP_lon, PRCP_lat, GPCP_PRCP_Downscaled_masked_2D, \n           xlab = 'longitude', ylab = 'latitude', \n           main = 'Temporal Mean of Precipitation (mm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Masked Precipitation Data\n\n\n\n\nThe representation of the globe is not suitable for the hydro-meteorological variables. Their longitude is going from -180 to 180.\n\ndimx &lt;- dim(GPCP_PRCP_Downscaled_masked)[1]\nGPCP_PRCP_Downscaled_masked2 = GPCP_PRCP_Downscaled_masked\nfor (xx in 1:dimx) {\n  \n  if (xx &lt;= 360) {GPCP_PRCP_Downscaled_masked2[xx,,,] &lt;-  GPCP_PRCP_Downscaled_masked[xx+360,,,]}\n  if (xx &gt; 360) {GPCP_PRCP_Downscaled_masked2[xx,,,] &lt;-  GPCP_PRCP_Downscaled_masked[xx-360,,,]}\n\n}\n\nPRCP_lat2 = PRCP_lat\nPRCP_lon2 = seq(-179.75, 179.75, 0.5)\n\nCheck the image again.\n\nGPCP_PRCP_Downscaled_masked2_2D &lt;-  rowMeans(GPCP_PRCP_Downscaled_masked2, na.rm = T, dims = 2)\nimage.plot(PRCP_lon2, PRCP_lat2, GPCP_PRCP_Downscaled_masked2_2D, \n           xlab = 'longitude', ylab = 'latitude', \n           main = 'Temporal Mean of Precipitation (mm)')\n\nWarning in temp$x: partial match of 'x' to 'xlab'\n\n\nWarning in temp$y: partial match of 'y' to 'ylab'\n\n\n\n\n\nTemporal Mean of Masked Precipitation Data (Arranged Longitude)\n\n\n\n\nThat’s perfect. The precipitation data is ready for the analysis part."
  },
  {
    "objectID": "570_project_archery_game_analysis/arc_warden.html",
    "href": "570_project_archery_game_analysis/arc_warden.html",
    "title": "An Archery Game Analysis",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data set.\n\n\nAltan Şener altansener98@gmail.com\nAbdullah Mert Çelikkol mertcelikkol10@gmail.com\nAlican Aktağ alcnaktag@gmail.com\n\nINTRODUCTION\nThe data that has been used in this study in gathered from a mobile game that can be played on an Android operating system.\nThe data has a total of 24762 observations and 66 variables. In the game, there is a spinning ball in the middle and the player throws arrows to the spinning ball by touching the screen, if the arrows hit on another but not the spinning ball, the player has to repeat the level.\nThere are 200 levels and each level gets more difficult gradually. The value of the data set is derived from their thorough collection, ensuring a wealth of detail. Furthermore, the substantial sample size provides sufficient grounds for meaningful interpretation. Also the sample size is not too large to cause any memory problems, therefore no sub-sample has been taken. The original data set is in json format and has to be cleaned, the variable names have to be adjusted and the types of the columns should be corrected.\nThis is how a particular json data set looks like:\n\nAt the first glance, it is not easy to understand the data. Firstly, we change the format of this data to a csv.\n\nNow it looks better :) Isn’t it?\nThe game has been available to users for a while (2012). A lot of data was collected during this time. From which country the users are in to which phone they use. In the data set, we can also find how much time players spent on each level, their success rates in the levels, and the android version they used.\nBased on what we know, we wondered about some things and wanted to visualize them and share the results we found with you.\nA few things we will cover up :\nResearch Question 1:\n- How does the distribution of players vary across continents and countries in the game, and are there noticeable patterns in user engagement based on continent?\nResearch Question 2:\n- What is the distribution of the maximum level attained by users in the game, analyzed in conjunction with their respective continents ?\n- Are there significant variations in the maximum level achieved, and do these patterns differ across continents ?”\nResearch Question 3:\n- How does user engagement differ across mobile device categories in the game? Furthermore, are there variations in the maximum levels achieved by users within each category?\n- This analysis seeks to uncover patterns and disparities in user behavior based on the distinction between mobile and phone categories.”\nResearch Question 4:\n- What is the distribution of successful trial frequencies at different levels within a mobile game?\nThe answers and visualizations to these questions will give us a better understanding of the game.\n\nThis is a photo of the 43th level of the game. The screenshot is from one of ours phones (Altan, I guess), I got to the 43th level in around 45 minutes, so you can see that the game isn’t that hard.\n\n\n\nDATA CLEANING\nDisclaimer\nThis notebook is the first example of a Firebase Analytics data being flattened in R Programming Language, and solutions might not be the most optimal ones.\n\nPreparing JSONL(L stands for new line delimited) data to be in tabular, can sometimes be a fun journey. Especially when you are dealing with Firebase Analytics Data. What is Firebase ? It’s the most widely used data tracking software used for mobile applications. Below is how Firebase analytics data look like when it’s partially cleaned:\n\nEach row is an event, all events have a time stamp and several information such as country , version etc. However, more importantly, there is a key, and values, where values can be integer, float, or string. But not two different at the same time for a given single key. The above is a summary of a generic, mostly clean Firebase analytics data and the following picture is from Google Cloud Bigquery:\n\nTrick 1\nYou can use the here function from , guess the name here package in R to set working directory to your script’s location, so people who use your code don’t have to change any lines to test it out.\n\nsetwd(here::here()) \n\nYou can now check if the here::here() have worked\n\ngetwd()\n\nAs we previously have shown you, our data is in json format, lets load it and check the head of the data to see how it looks like:\n\nlibrary(jsonlite) ## no error messages here thanks to the \"#| warning: false\" option!\nlibrary(tidyverse)\njsonl_data &lt;- stream_in(file(\"data.json\"),verbose = FALSE)\nhead(jsonl_data,2)\n\n  event_date  event_timestamp  event_name\n1   20231221 1703146325195000  app_remove\n2   20231221 1703134161235001 screen_view\n                                                                                                                                                                                                                             event_params\n1                                                                                                                                               ga_session_id, ga_session_number, firebase_event_origin, 1702888810, 10, NA, NA, NA, auto\n2 entrances, ga_session_number, ga_session_id, firebase_screen_id, firebase_event_origin, engaged_session_event, firebase_screen_class, 1, 52, 1703134160, -8779596609919170096, NA, 1, NA, NA, NA, NA, NA, auto, NA, UnityPlayerActivity\n  event_bundle_sequence_id event_server_timestamp_offset\n1                       15                 1383899326014\n2                      100                           494\n                    user_pseudo_id privacy_info.analytics_storage\n1 eae0e04fa3fa69ef646baaeaa716ea6b                            Yes\n2 74b842a52de12e4cf74034f998ee98cf                            Yes\n  privacy_info.ads_storage privacy_info.uses_transient_token\n1                      Yes                                No\n2                      Yes                                No\n                                                                                                                         user_properties\n1 ga_session_id, ga_session_number, first_open_time, 1702888810, 10, 1701284400000, 1702888810463000, 1702888810463000, 1701284050034000\n2 ga_session_number, ga_session_id, first_open_time, 52, 1703134160, 1698397200000, 1703134160741000, 1703134160741000, 1698396193166000\n  user_first_touch_timestamp device.category device.mobile_brand_name\n1           1701284050034000          mobile                  Samsung\n2           1698396193166000          mobile                  Samsung\n  device.mobile_model_name device.mobile_marketing_name\n1                 SM-G7102               Galaxy Grand 2\n2                 SM-J110H                    Galaxy J1\n  device.mobile_os_hardware_model device.operating_system\n1                        SM-G7102                 Android\n2                        SM-J110H                 Android\n  device.operating_system_version                device.advertising_id\n1                   Android 4.4.2 f28a3de1-340f-4d84-a491-ae5bc5c66a8a\n2                   Android 4.4.4 6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\n  device.language device.is_limited_ad_tracking device.time_zone_offset_seconds\n1           ar-ae                           Yes                            7200\n2           ar-ae                           Yes                            7200\n        geo.city geo.country geo.continent                 geo.region\n1 Kafr el-Sheikh       Egypt        Africa Kafr El-Sheikh Governorate\n2                     Jordan          Asia          Amman Governorate\n  geo.sub_continent geo.metro        app_info.id app_info.version\n1   Northern Africa (not set) com.elakerem.focus           2.0.22\n2      Western Asia (not set) com.elakerem.focus           2.0.22\n                     app_info.firebase_app_id app_info.install_source\n1 1:2474473662:android:5047021a790dce42eb06ef     com.android.vending\n2 1:2474473662:android:5047021a790dce42eb06ef     com.android.vending\n  traffic_source.name traffic_source.medium traffic_source.source  stream_id\n1            (direct)                (none)              (direct) 2758285888\n2            (direct)                (none)              (direct) 2758285888\n  platform items is_active_user event_previous_timestamp\n1  ANDROID  NULL          FALSE                     &lt;NA&gt;\n2  ANDROID  NULL           TRUE         1703035993871001\n  collected_traffic_source.manual_source collected_traffic_source.manual_medium\n1                                   &lt;NA&gt;                                   &lt;NA&gt;\n2                                   &lt;NA&gt;                                   &lt;NA&gt;\n\n\nOverall, it is uninterpreted , because we have key value formats, data frames of data frames, lists of data frames, and all sorts of weird things. Let’s also without going into next levels through use of max.level=1 argument to check out the data again\n\nstr(jsonl_data,max.level = 1)\n\n'data.frame':   24762 obs. of  20 variables:\n $ event_date                   : chr  \"20231221\" \"20231221\" \"20231221\" \"20231221\" ...\n $ event_timestamp              : chr  \"1703146325195000\" \"1703134161235001\" \"1703134235541004\" \"1703134279049008\" ...\n $ event_name                   : chr  \"app_remove\" \"screen_view\" \"level_end\" \"user_engagement\" ...\n $ event_params                 :List of 24762\n $ event_bundle_sequence_id     : chr  \"15\" \"100\" \"100\" \"100\" ...\n $ event_server_timestamp_offset: chr  \"1383899326014\" \"494\" \"494\" \"494\" ...\n $ user_pseudo_id               : chr  \"eae0e04fa3fa69ef646baaeaa716ea6b\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" ...\n $ privacy_info                 :'data.frame':  24762 obs. of  3 variables:\n $ user_properties              :List of 24762\n $ user_first_touch_timestamp   : chr  \"1701284050034000\" \"1698396193166000\" \"1698396193166000\" \"1698396193166000\" ...\n $ device                       :'data.frame':  24762 obs. of  11 variables:\n $ geo                          :'data.frame':  24762 obs. of  6 variables:\n $ app_info                     :'data.frame':  24762 obs. of  4 variables:\n $ traffic_source               :'data.frame':  24762 obs. of  3 variables:\n $ stream_id                    : chr  \"2758285888\" \"2758285888\" \"2758285888\" \"2758285888\" ...\n $ platform                     : chr  \"ANDROID\" \"ANDROID\" \"ANDROID\" \"ANDROID\" ...\n $ items                        :List of 24762\n $ is_active_user               : logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n $ event_previous_timestamp     : chr  NA \"1703035993871001\" \"1703134211021004\" \"1703036243245008\" ...\n $ collected_traffic_source     :'data.frame':  24762 obs. of  2 variables:\n\n\nSo we have some lists, some data frames, some characters, and some logical ones, we got a beautiful soup of data types. Recall, the event_params column from the example picture\n\nevent_params has key and value nestings, and value has string, float, int double value nestings, and these are all in different formats, lets take a look at event_params with str function\n\nstr(jsonl_data$event_params,list.len=3)\n\nList of 24762\n $ :'data.frame':   3 obs. of  2 variables:\n  ..$ key  : chr [1:3] \"ga_session_id\" \"ga_session_number\" \"firebase_event_origin\"\n  ..$ value:'data.frame':   3 obs. of  2 variables:\n  .. ..$ int_value   : chr [1:3] \"1702888810\" \"10\" NA\n  .. ..$ string_value: chr [1:3] NA NA \"auto\"\n $ :'data.frame':   7 obs. of  2 variables:\n  ..$ key  : chr [1:7] \"entrances\" \"ga_session_number\" \"ga_session_id\" \"firebase_screen_id\" ...\n  ..$ value:'data.frame':   7 obs. of  2 variables:\n  .. ..$ int_value   : chr [1:7] \"1\" \"52\" \"1703134160\" \"-8779596609919170096\" ...\n  .. ..$ string_value: chr [1:7] NA NA NA NA ...\n $ :'data.frame':   9 obs. of  2 variables:\n  ..$ key  : chr [1:9] \"ga_session_number\" \"level\" \"firebase_screen_id\" \"firebase_event_origin\" ...\n  ..$ value:'data.frame':   9 obs. of  2 variables:\n  .. ..$ int_value   : chr [1:9] \"52\" \"1\" \"-8779596609919170096\" NA ...\n  .. ..$ string_value: chr [1:9] NA NA NA \"app\" ...\n  [list output truncated]\n\n\nWe have a complicated format, here is a demonstration of how to access event_params and its sub parts\n\nclass(jsonl_data) ## Whole data class \n\n[1] \"data.frame\"\n\nclass(jsonl_data$event_params) ## Event params class \n\n[1] \"list\"\n\njsonl_data$event_params[[1]] ## access first row's event_params\n\n                    key value.int_value value.string_value\n1         ga_session_id      1702888810               &lt;NA&gt;\n2     ga_session_number              10               &lt;NA&gt;\n3 firebase_event_origin            &lt;NA&gt;               auto\n\nclass(jsonl_data$event_params[[1]]) ## it's class \n\n[1] \"data.frame\"\n\njsonl_data$event_params[[1]][1] ## how to access event_params$key\n\n                    key\n1         ga_session_id\n2     ga_session_number\n3 firebase_event_origin\n\nclass(jsonl_data$event_params[[1]][1]) ## it's class\n\n[1] \"data.frame\"\n\njsonl_data$event_params[[1]][2] ## how to access event_params$value\n\n  value.int_value value.string_value\n1      1702888810               &lt;NA&gt;\n2              10               &lt;NA&gt;\n3            &lt;NA&gt;               auto\n\nclass(jsonl_data$event_params[[1]][2]) ## it's class\n\n[1] \"data.frame\"\n\n\nWe have a data frame jsonl_data, it has a list event_params, list is made of data frames, and in the data frame, we have key column, and a data frame named value, which has two columns, named int_value and string_value.\n\nHere is the issue, our data also has inconsistencies. At bellow, take a look at two different “dataframes” under the event_params:\n\njsonl_data$event_params[[4983]]  \n\n                    key string_value\n1   previous_os_version           11\n2 firebase_event_origin         auto\n\njsonl_data$event_params[[1]]\n\n                    key value.int_value value.string_value\n1         ga_session_id      1702888810               &lt;NA&gt;\n2     ga_session_number              10               &lt;NA&gt;\n3 firebase_event_origin            &lt;NA&gt;               auto\n\n\nLets begin cleaning, firstly there were some non data frame, lists objects inside the event_params, we will iterate over the event_params, and convert them.\n\nlist_of_dfs=list()\nfor(i in 1:nrow(jsonl_data)){\n  temp_df= as.data.frame(jsonl_data$event_params[[i]])\n  list_of_dfs[[i]] &lt;- temp_df\n}\nclass(list_of_dfs)\n\n[1] \"list\"\n\nlist_of_dfs[[1]]\n\n                    key value.int_value value.string_value\n1         ga_session_id      1702888810               &lt;NA&gt;\n2     ga_session_number              10               &lt;NA&gt;\n3 firebase_event_origin            &lt;NA&gt;               auto\n\n\nAt the next step, recall we had int value, and string value for each key, with one of these two always being null. Since one of them is always null, we can concatenate them by binding columns , but we got two type of objects inside the event_params(was three before the above loop), the case when there is event_params with key, and value with value having two more sub-columns and the case with even_params having no nested value column but instead a “string_value” column. We can iterate over it in a for loop, for these two specific cases fix, bind the columns and unnest it out of the value and have a single value column, and when there is only key with string_value, we can just rename the column. Just for fun, we will use try catch because why not to learn it while working in R. Here is the example usage of try catch:\nTry\nTo Do Something\nExcept ## D0f (What?) fail\nDo something else instead\n\nfor (i in seq_along(list_of_dfs)) {\n  tryCatch({\n    if (\"value\" %in% names(list_of_dfs[[i]])) {\n      if (is.list(list_of_dfs[[i]]$value)) {\n        list_of_dfs[[i]]$value &lt;- ifelse(\n          !is.na(list_of_dfs[[i]]$value$int_value),\n          as.character(list_of_dfs[[i]]$value$int_value),\n          as.character(list_of_dfs[[i]]$value$string_value)\n        )\n        list_of_dfs[[i]]$value &lt;- as.character(list_of_dfs[[i]]$value)\n        list_of_dfs[[i]] &lt;- list_of_dfs[[i]][, !(names(list_of_dfs[[i]]) %in% c(\"int_value\", \"string_value\"))]\n      }\n    }\n  }, error = function(e) {\n    # If an error occurs, rename the second column to \"value\"\n    if (length(names(list_of_dfs[[i]])) &gt;= 2) {\n      new_temp_df= cbind.data.frame(list_of_dfs[[i]][[1]],c(list_of_dfs[[i]][[2]]))\n      names(new_temp_df) &lt;- c(\"key\",\"value\")\n      list_of_dfs[[i]]&lt;&lt;- new_temp_df\n    }\n  })\n}\n\nlist_of_dfs[[1]]\n\n                    key      value\n1         ga_session_id 1702888810\n2     ga_session_number         10\n3 firebase_event_origin       auto\n\n\nNote that try catch and error handling is usually much slower than using if statements and should primarily be used for cases that can’t be predicted or handled with regular methods. But it can also be faster when used in place of an extremely complex if check.\nAt the next step, we flatten the event_params. Instead of having key and values, what if every unique key was a data set column, and values were under it, and if in that respective row, there is no element for a specific key we can just keep null, this would make it easier for anyone else working in this data later.\n\nLuckily, Tidyverse ensures we don’t have to write a complex loop here since we have dealt with the inconsistencies in our data but first lets get these thing out of list of data frames into a single DF, we will use bind_rows function again from tidyverse:\n\ncombined_df &lt;- bind_rows(list_of_dfs, .id = \"df_id\") \nhead(combined_df)  \n\n  df_id                   key      value\n1     1         ga_session_id 1702888810\n2     1     ga_session_number         10\n3     1 firebase_event_origin       auto\n4     2             entrances          1\n5     2     ga_session_number         52\n6     2         ga_session_id 1703134160\n\n\nNow all we got to do is change the binded element from long to wide format for each ID, pivot_wider will automatically fill it with nulls for cases when in that row a specific key is not used.\n\nflattened_df &lt;- combined_df %&gt;%\n  pivot_wider(names_from = key, values_from = value)\n\nflattened_df &lt;- flattened_df[, -1]\n\nhead(flattened_df )\n\n# A tibble: 6 × 24\n  ga_session_id ga_session_number firebase_event_origin entrances\n  &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt;                 &lt;chr&gt;    \n1 1702888810    10                auto                  &lt;NA&gt;     \n2 1703134160    52                auto                  1        \n3 1703134160    52                app                   &lt;NA&gt;     \n4 1703134160    52                auto                  &lt;NA&gt;     \n5 1703190214    53                auto                  1        \n6 1703190214    53                app                   &lt;NA&gt;     \n# ℹ 20 more variables: firebase_screen_id &lt;chr&gt;, engaged_session_event &lt;chr&gt;,\n#   firebase_screen_class &lt;chr&gt;, level &lt;chr&gt;, success &lt;chr&gt;, value &lt;chr&gt;,\n#   engagement_time_msec &lt;chr&gt;, session_engaged &lt;chr&gt;,\n#   firebase_previous_class &lt;chr&gt;, firebase_previous_id &lt;chr&gt;,\n#   update_with_analytics &lt;chr&gt;, system_app &lt;chr&gt;,\n#   previous_first_open_count &lt;chr&gt;, system_app_update &lt;chr&gt;,\n#   firebase_conversion &lt;chr&gt;, source &lt;chr&gt;, medium &lt;chr&gt;, …\n\n\nWe can now cbind this into our main dataframe, and also drop some other irrelevant columns and the column we have just flattened\n\nfinal_df= cbind.data.frame(jsonl_data,flattened_df)\nfinal_df2= final_df |&gt; select(-event_params,-user_properties,-items) \n\nLet’s take a look at our data again\n\nstr(final_df2)\n\n'data.frame':   24762 obs. of  41 variables:\n $ event_date                   : chr  \"20231221\" \"20231221\" \"20231221\" \"20231221\" ...\n $ event_timestamp              : chr  \"1703146325195000\" \"1703134161235001\" \"1703134235541004\" \"1703134279049008\" ...\n $ event_name                   : chr  \"app_remove\" \"screen_view\" \"level_end\" \"user_engagement\" ...\n $ event_bundle_sequence_id     : chr  \"15\" \"100\" \"100\" \"100\" ...\n $ event_server_timestamp_offset: chr  \"1383899326014\" \"494\" \"494\" \"494\" ...\n $ user_pseudo_id               : chr  \"eae0e04fa3fa69ef646baaeaa716ea6b\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" ...\n $ privacy_info                 :'data.frame':  24762 obs. of  3 variables:\n  ..$ analytics_storage   : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n  ..$ ads_storage         : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n  ..$ uses_transient_token: chr  \"No\" \"No\" \"No\" \"No\" ...\n $ user_first_touch_timestamp   : chr  \"1701284050034000\" \"1698396193166000\" \"1698396193166000\" \"1698396193166000\" ...\n $ device                       :'data.frame':  24762 obs. of  11 variables:\n  ..$ category                : chr  \"mobile\" \"mobile\" \"mobile\" \"mobile\" ...\n  ..$ mobile_brand_name       : chr  \"Samsung\" \"Samsung\" \"Samsung\" \"Samsung\" ...\n  ..$ mobile_model_name       : chr  \"SM-G7102\" \"SM-J110H\" \"SM-J110H\" \"SM-J110H\" ...\n  ..$ mobile_marketing_name   : chr  \"Galaxy Grand 2\" \"Galaxy J1\" \"Galaxy J1\" \"Galaxy J1\" ...\n  ..$ mobile_os_hardware_model: chr  \"SM-G7102\" \"SM-J110H\" \"SM-J110H\" \"SM-J110H\" ...\n  ..$ operating_system        : chr  \"Android\" \"Android\" \"Android\" \"Android\" ...\n  ..$ operating_system_version: chr  \"Android 4.4.2\" \"Android 4.4.4\" \"Android 4.4.4\" \"Android 4.4.4\" ...\n  ..$ advertising_id          : chr  \"f28a3de1-340f-4d84-a491-ae5bc5c66a8a\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" ...\n  ..$ language                : chr  \"ar-ae\" \"ar-ae\" \"ar-ae\" \"ar-ae\" ...\n  ..$ is_limited_ad_tracking  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n  ..$ time_zone_offset_seconds: chr  \"7200\" \"7200\" \"7200\" \"7200\" ...\n $ geo                          :'data.frame':  24762 obs. of  6 variables:\n  ..$ city         : chr  \"Kafr el-Sheikh\" \"\" \"\" \"\" ...\n  ..$ country      : chr  \"Egypt\" \"Jordan\" \"Jordan\" \"Jordan\" ...\n  ..$ continent    : chr  \"Africa\" \"Asia\" \"Asia\" \"Asia\" ...\n  ..$ region       : chr  \"Kafr El-Sheikh Governorate\" \"Amman Governorate\" \"Amman Governorate\" \"Amman Governorate\" ...\n  ..$ sub_continent: chr  \"Northern Africa\" \"Western Asia\" \"Western Asia\" \"Western Asia\" ...\n  ..$ metro        : chr  \"(not set)\" \"(not set)\" \"(not set)\" \"(not set)\" ...\n $ app_info                     :'data.frame':  24762 obs. of  4 variables:\n  ..$ id             : chr  \"com.elakerem.focus\" \"com.elakerem.focus\" \"com.elakerem.focus\" \"com.elakerem.focus\" ...\n  ..$ version        : chr  \"2.0.22\" \"2.0.22\" \"2.0.22\" \"2.0.22\" ...\n  ..$ firebase_app_id: chr  \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" ...\n  ..$ install_source : chr  \"com.android.vending\" \"com.android.vending\" \"com.android.vending\" \"com.android.vending\" ...\n $ traffic_source               :'data.frame':  24762 obs. of  3 variables:\n  ..$ name  : chr  \"(direct)\" \"(direct)\" \"(direct)\" \"(direct)\" ...\n  ..$ medium: chr  \"(none)\" \"(none)\" \"(none)\" \"(none)\" ...\n  ..$ source: chr  \"(direct)\" \"(direct)\" \"(direct)\" \"(direct)\" ...\n $ stream_id                    : chr  \"2758285888\" \"2758285888\" \"2758285888\" \"2758285888\" ...\n $ platform                     : chr  \"ANDROID\" \"ANDROID\" \"ANDROID\" \"ANDROID\" ...\n $ is_active_user               : logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n $ event_previous_timestamp     : chr  NA \"1703035993871001\" \"1703134211021004\" \"1703036243245008\" ...\n $ collected_traffic_source     :'data.frame':  24762 obs. of  2 variables:\n  ..$ manual_source: chr  NA NA NA NA ...\n  ..$ manual_medium: chr  NA NA NA NA ...\n $ ga_session_id                : chr  \"1702888810\" \"1703134160\" \"1703134160\" \"1703134160\" ...\n $ ga_session_number            : chr  \"10\" \"52\" \"52\" \"52\" ...\n $ firebase_event_origin        : chr  \"auto\" \"auto\" \"app\" \"auto\" ...\n $ entrances                    : chr  NA \"1\" NA NA ...\n $ firebase_screen_id           : chr  NA \"-8779596609919170096\" \"-8779596609919170096\" \"-8779596609919170096\" ...\n $ engaged_session_event        : chr  NA \"1\" \"1\" \"1\" ...\n $ firebase_screen_class        : chr  NA \"UnityPlayerActivity\" \"UnityPlayerActivity\" \"UnityPlayerActivity\" ...\n $ level                        : chr  NA NA \"1\" NA ...\n $ success                      : chr  NA NA \"0\" NA ...\n $ value                        : chr  NA NA \"0\" NA ...\n $ engagement_time_msec         : chr  NA NA NA \"114189\" ...\n $ session_engaged              : chr  NA NA NA NA ...\n $ firebase_previous_class      : chr  NA NA NA NA ...\n $ firebase_previous_id         : chr  NA NA NA NA ...\n $ update_with_analytics        : chr  NA NA NA NA ...\n $ system_app                   : chr  NA NA NA NA ...\n $ previous_first_open_count    : chr  NA NA NA NA ...\n $ system_app_update            : chr  NA NA NA NA ...\n $ firebase_conversion          : chr  NA NA NA NA ...\n $ source                       : chr  NA NA NA NA ...\n $ medium                       : chr  NA NA NA NA ...\n $ campaign_info_source         : chr  NA NA NA NA ...\n $ previous_os_version          : chr  NA NA NA NA ...\n $ appnava_churn_prob           : chr  NA NA NA NA ...\n\n\nOur data is now free of the chaos of event_params. We still have few more things to do as we got some columns nested, example, geo column has other columns under it like geo.country, geo.continent and so on, and this might make future work slower.\n\nstr(final_df2$geo)\n\n'data.frame':   24762 obs. of  6 variables:\n $ city         : chr  \"Kafr el-Sheikh\" \"\" \"\" \"\" ...\n $ country      : chr  \"Egypt\" \"Jordan\" \"Jordan\" \"Jordan\" ...\n $ continent    : chr  \"Africa\" \"Asia\" \"Asia\" \"Asia\" ...\n $ region       : chr  \"Kafr El-Sheikh Governorate\" \"Amman Governorate\" \"Amman Governorate\" \"Amman Governorate\" ...\n $ sub_continent: chr  \"Northern Africa\" \"Western Asia\" \"Western Asia\" \"Western Asia\" ...\n $ metro        : chr  \"(not set)\" \"(not set)\" \"(not set)\" \"(not set)\" ...\n\n\nTo ensure our data is friendly for anyone, we can write a small function that detect such “dataframes under our dataframe” and unnest them\n\nis_dataframe &lt;- function(column) {\n  is.data.frame(column)\n}\n### object to save columns which are dataframes\ndataframe_cols &lt;- c()\n\n# Loop through each column in final_df2\nfor (col in colnames(final_df2)) {\n  if (is_dataframe(final_df2[[col]])) {\n    dataframe_cols &lt;- c(dataframe_cols, col)\n  }\n}\n\n## loop through them, take the column under column to outside of it  and combined them\n\ncombined_nested_dfs=rep(0,nrow(final_df2))\n\nfor (element in dataframe_cols){\n  temp_index=as.character(element)\n  temp_df= final_df2[[temp_index]]\n  combined_nested_dfs=cbind.data.frame(combined_nested_dfs,temp_df)\n}\nfinal_df2=final_df2 |&gt; select(-dataframe_cols)\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(dataframe_cols)\n\n  # Now:\n  data %&gt;% select(all_of(dataframe_cols))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\nfinal_df3=cbind.data.frame(final_df2,combined_nested_dfs)\n\noptions(scipen=999)\nwrite.csv(final_df3,\"plsfkinwork3.csv\")\n\nWe have written lots of code to have an “end scientist” friendly rectangular dataset final look before we end the dataprep.\n\nstr(final_df3)\n\n'data.frame':   24762 obs. of  65 variables:\n $ event_date                   : chr  \"20231221\" \"20231221\" \"20231221\" \"20231221\" ...\n $ event_timestamp              : chr  \"1703146325195000\" \"1703134161235001\" \"1703134235541004\" \"1703134279049008\" ...\n $ event_name                   : chr  \"app_remove\" \"screen_view\" \"level_end\" \"user_engagement\" ...\n $ event_bundle_sequence_id     : chr  \"15\" \"100\" \"100\" \"100\" ...\n $ event_server_timestamp_offset: chr  \"1383899326014\" \"494\" \"494\" \"494\" ...\n $ user_pseudo_id               : chr  \"eae0e04fa3fa69ef646baaeaa716ea6b\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" \"74b842a52de12e4cf74034f998ee98cf\" ...\n $ user_first_touch_timestamp   : chr  \"1701284050034000\" \"1698396193166000\" \"1698396193166000\" \"1698396193166000\" ...\n $ stream_id                    : chr  \"2758285888\" \"2758285888\" \"2758285888\" \"2758285888\" ...\n $ platform                     : chr  \"ANDROID\" \"ANDROID\" \"ANDROID\" \"ANDROID\" ...\n $ is_active_user               : logi  FALSE TRUE TRUE TRUE TRUE TRUE ...\n $ event_previous_timestamp     : chr  NA \"1703035993871001\" \"1703134211021004\" \"1703036243245008\" ...\n $ ga_session_id                : chr  \"1702888810\" \"1703134160\" \"1703134160\" \"1703134160\" ...\n $ ga_session_number            : chr  \"10\" \"52\" \"52\" \"52\" ...\n $ firebase_event_origin        : chr  \"auto\" \"auto\" \"app\" \"auto\" ...\n $ entrances                    : chr  NA \"1\" NA NA ...\n $ firebase_screen_id           : chr  NA \"-8779596609919170096\" \"-8779596609919170096\" \"-8779596609919170096\" ...\n $ engaged_session_event        : chr  NA \"1\" \"1\" \"1\" ...\n $ firebase_screen_class        : chr  NA \"UnityPlayerActivity\" \"UnityPlayerActivity\" \"UnityPlayerActivity\" ...\n $ level                        : chr  NA NA \"1\" NA ...\n $ success                      : chr  NA NA \"0\" NA ...\n $ value                        : chr  NA NA \"0\" NA ...\n $ engagement_time_msec         : chr  NA NA NA \"114189\" ...\n $ session_engaged              : chr  NA NA NA NA ...\n $ firebase_previous_class      : chr  NA NA NA NA ...\n $ firebase_previous_id         : chr  NA NA NA NA ...\n $ update_with_analytics        : chr  NA NA NA NA ...\n $ system_app                   : chr  NA NA NA NA ...\n $ previous_first_open_count    : chr  NA NA NA NA ...\n $ system_app_update            : chr  NA NA NA NA ...\n $ firebase_conversion          : chr  NA NA NA NA ...\n $ source                       : chr  NA NA NA NA ...\n $ medium                       : chr  NA NA NA NA ...\n $ campaign_info_source         : chr  NA NA NA NA ...\n $ previous_os_version          : chr  NA NA NA NA ...\n $ appnava_churn_prob           : chr  NA NA NA NA ...\n $ combined_nested_dfs          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ analytics_storage            : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ ads_storage                  : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ uses_transient_token         : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ category                     : chr  \"mobile\" \"mobile\" \"mobile\" \"mobile\" ...\n $ mobile_brand_name            : chr  \"Samsung\" \"Samsung\" \"Samsung\" \"Samsung\" ...\n $ mobile_model_name            : chr  \"SM-G7102\" \"SM-J110H\" \"SM-J110H\" \"SM-J110H\" ...\n $ mobile_marketing_name        : chr  \"Galaxy Grand 2\" \"Galaxy J1\" \"Galaxy J1\" \"Galaxy J1\" ...\n $ mobile_os_hardware_model     : chr  \"SM-G7102\" \"SM-J110H\" \"SM-J110H\" \"SM-J110H\" ...\n $ operating_system             : chr  \"Android\" \"Android\" \"Android\" \"Android\" ...\n $ operating_system_version     : chr  \"Android 4.4.2\" \"Android 4.4.4\" \"Android 4.4.4\" \"Android 4.4.4\" ...\n $ advertising_id               : chr  \"f28a3de1-340f-4d84-a491-ae5bc5c66a8a\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" \"6d58a8cf-8508-45cb-9f0a-41fbbc14d0c7\" ...\n $ language                     : chr  \"ar-ae\" \"ar-ae\" \"ar-ae\" \"ar-ae\" ...\n $ is_limited_ad_tracking       : chr  \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ time_zone_offset_seconds     : chr  \"7200\" \"7200\" \"7200\" \"7200\" ...\n $ city                         : chr  \"Kafr el-Sheikh\" \"\" \"\" \"\" ...\n $ country                      : chr  \"Egypt\" \"Jordan\" \"Jordan\" \"Jordan\" ...\n $ continent                    : chr  \"Africa\" \"Asia\" \"Asia\" \"Asia\" ...\n $ region                       : chr  \"Kafr El-Sheikh Governorate\" \"Amman Governorate\" \"Amman Governorate\" \"Amman Governorate\" ...\n $ sub_continent                : chr  \"Northern Africa\" \"Western Asia\" \"Western Asia\" \"Western Asia\" ...\n $ metro                        : chr  \"(not set)\" \"(not set)\" \"(not set)\" \"(not set)\" ...\n $ id                           : chr  \"com.elakerem.focus\" \"com.elakerem.focus\" \"com.elakerem.focus\" \"com.elakerem.focus\" ...\n $ version                      : chr  \"2.0.22\" \"2.0.22\" \"2.0.22\" \"2.0.22\" ...\n $ firebase_app_id              : chr  \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" \"1:2474473662:android:5047021a790dce42eb06ef\" ...\n $ install_source               : chr  \"com.android.vending\" \"com.android.vending\" \"com.android.vending\" \"com.android.vending\" ...\n $ name                         : chr  \"(direct)\" \"(direct)\" \"(direct)\" \"(direct)\" ...\n $ medium                       : chr  \"(none)\" \"(none)\" \"(none)\" \"(none)\" ...\n $ source                       : chr  \"(direct)\" \"(direct)\" \"(direct)\" \"(direct)\" ...\n $ manual_source                : chr  NA NA NA NA ...\n $ manual_medium                : chr  NA NA NA NA ...\n\n\nLets save the results before we begin our data analysis.\n\nwrite.csv(final_df3,\"plsfkinwork3.csv\")\n\n\n\nDATA ANALYSIS\nLets load our libraries, you can disable package loading warnings with warning = FALSE argument, bellow chunk is code only\n\nsetwd(here::here())  \ndf=read.csv(\"plsfkinwork3.csv\") \nlibrary(dplyr) \nlibrary(ggplot2) \nlibrary(purrr) \nlibrary(jsonlite) \nlibrary(tidyr)  \noptions(scipen=999) \ndf=df[,-1]  \n\nFirstly lets plot a histogram of Session duration of users. There are two ways to calculate it, first one is to get start timestamp of a session and end timestamp of it, take the difference and that is your duration. However since most users move their app to background while using their phones, scroll instagram, open snapchat and so on, Firebase datasets have another method to calculate user active duration.\nFirebase counts how long they users keep the app on foreground, and send total duration in micro seconds inside the event_params in a key named engagement_time_msec. We can sum engagement_time_msec by session and that would be the average active session duration.\n\ndf$engagement_time_msec=as.numeric(df$engagement_time_msec)\ntotal_session_duration_per_user &lt;- df %&gt;%\n  group_by(user_pseudo_id) %&gt;%\n  summarize(total_engagement_time = sum(engagement_time_msec,na.rm = TRUE))\n\ntotal_session_duration_per_user$total_engagement_time=total_session_duration_per_user$total_engagement_time/60000\n\nhist(total_session_duration_per_user$total_engagement_time,breaks=50)\n\n\n\n\nOur data manipulation have worked and we are now capable of printing some histograms, however submitting such an Instagram (What?) can get you fired, or cause you to receive lower grades for this reason we will make a histogram with beautiful aesthetics , and definitely better breaks\n\nbreaks &lt;- c(0, 1, 2, 3, 5, 10, 20, 60, 1440)  \ntotal_session_duration_per_user$break_group &lt;- cut(total_session_duration_per_user$total_engagement_time, breaks = breaks, labels = FALSE, include.lowest = TRUE)\ncustom_labels &lt;- c(\"0-1\", \"1-2\", \"2-3\", \"3-5\", \"5-10\", \"10-20\", \"20-60\", \"60-1 day\", \"1 day+\")\n\nggplot(total_session_duration_per_user, aes(x = factor(break_group))) +\n  geom_bar(fill = \"blue\", color = \"black\", alpha = 0.7) +\n  scale_x_discrete(breaks = seq_along(breaks) - 1, labels = custom_labels) +\n  labs(title = \"Total Session Duration Histogram\",\n       x = \"Total Session Duration (minutes)\",\n       y = \"Frequency\")\n\n\n\n\nTotal session duration per user is visualized with comprehensible breaks. Gamers mostly prefer playing and closing the game in short durations; however, we suspect that there are also excessively long sessions due to the time when the game is running in the background and not actively being played.\n\ndistinct_sessions_per_user &lt;- df %&gt;%\n  group_by(user_pseudo_id) %&gt;%\n  summarize(distinct_session_count = n_distinct(ga_session_id))\n\n\nggplot(distinct_sessions_per_user, aes(x = distinct_session_count)) +\n  geom_histogram(fill = \"blue\", color = \"black\", alpha = 0.7, bins = 20) +\n  labs(title = \"Distinct Sessions per User Histogram\",\n       x = \"Distinct Session Count\",\n       y = \"Frequency\")\n\n\n\n\nThe repetition of playing the game by players within the range of our data is in line with the results of our previous analysis. Mostly, distinct players seem to have opened the game 1 or 2 times. There are also users who have opened more than 20 sessions, which may be considered as outliers.\n\ndf$user_first_touch_timestamp = as.numeric(df$user_first_touch_timestamp)\ndf$event_timestamp = as.numeric(df$event_timestamp)\n\nuser_ages &lt;- df %&gt;%\n  group_by(user_pseudo_id) %&gt;%\n  summarise(time_difference = max(event_timestamp) - max(user_first_touch_timestamp))\n\nuser_ages$time_difference= user_ages$time_difference/60000. #BURAYLA D0LGD0LD0 ANALD0Z YAPALIM!\n\nevent_counts &lt;- df %&gt;%\n  count(event_name)\n\nggplot(event_counts, aes(x = reorder(event_name, -n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Event Counts\",\n       x = \"Event Name\",\n       y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\nWe explained how we parsed the events in the first part. To get a preliminary idea about the diversity and distribution density of events in our data set, we prepared a bar plot. Despite the recording of event data in different types, level_end has the largest share. This result will contribute to strengthening our interpretations in future analyses.\n\ndf$mobile_brand_name= as.factor(df$mobile_brand_name)\ndf$continent= as.factor(df$continent)\n\ndistinct_users &lt;- df %&gt;%\n  select(user_pseudo_id, continent) %&gt;%\n  distinct()\n\nggplot(distinct_users, aes(x = continent)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Distinct Users Across Continents\",\n       x = \"Continent\",\n       y = \"Number of Distinct Users\") +\n  theme_minimal()\n\n\n\n\nOur first research question was: How does the distribution of players vary across continents and countries in the game, and are there noticeable patterns in user engagement based on continent?\nTo answer this question, distinct users are selected at first. Then, a bar plot for the distribution of distinct users across continents is created. According to this part of the question, it is observed that users from Asia contribute the most to the intercontinental distribution with 175 users. The participation numbers in the continents of Europe and America are below 25.\n\nusers_per_country &lt;- df %&gt;%\n  group_by(continent, country) %&gt;%\n  summarise(distinct_user_count = n_distinct(user_pseudo_id))\n\nusers_per_country_filtered &lt;- users_per_country %&gt;%\n  filter(distinct_user_count &gt; 3)\n\nusers_per_country_filtered &lt;- users_per_country_filtered %&gt;%\n  arrange(desc(distinct_user_count)) %&gt;%\n  mutate(country_display = ifelse(country == \"T&lt;U+00FC&gt;rkiye\", \"Turkey\", country))\n\nggplot(users_per_country_filtered, aes(x = reorder(country_display, distinct_user_count), y = distinct_user_count, size = distinct_user_count, color = continent)) +\n  geom_point() +\n  scale_size_continuous(name = \"Distinct User Count\", range = c(5, 17)) +\n  labs(title = \"Bubble Chart of Distinct Users by Country\",\n       x = \"Country\",\n       y = \"Distinct User Count\",\n       color = \"Continent\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\nWhen we examine the question with a more general visualization, it is noticeable that the blue colored Asian countries - colors are indicating continental differences - are dominant both in terms of diversity and frequency. The size of the bubbles indicates the magnitude of the distinct user count. It can be commented that more than 30 distinct users from Egypt and Turkey have been actively engaged in the game.\n\nmax_level_data &lt;- df %&gt;%\n  group_by(user_pseudo_id, continent) %&gt;%\n  summarize(max_level = max(level, na.rm = TRUE))\n\nggplot(max_level_data, aes(x = continent, y = max_level,fill = continent)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Maximum Level by Continent\",\n       x = \"Continent\",\n       y = \"Max Level\"\n       ) +\n  theme_minimal()\n\n\n\n\nThe second research question was: What is the distribution of the maximum level attained by users in the game, analyzed in conjunction with their respective continents? Are there significant variations in the maximum level achieved, and do these patterns differ across continents?\nThe maximum levels that each user played is selected at first. Then, to answer the question, boxplot of the continents is created. Although there seems to be a similar distribution overall, there are more user outliers in Africa and Asia. One of the reasons for this is the uneven distribution of users on the continents, as seen in our previous graphs. Nevertheless, the difference between the 1st and 3rd quartiles in the African continent is striking, and it is observed that Asia has the highest median value.\n\ndistinct_users_category &lt;- df %&gt;%\n  select(user_pseudo_id, category) %&gt;%\n  distinct()\n\n\nggplot(distinct_users_category, aes(x = category)) +\n  geom_bar(fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Distinct Users Across Categories\",\n       x = \"Category\",\n       y = \"Number of Distinct Users\") +\n  theme_minimal()\n\n\n\n\nResearch Question 3 covers How does user engagement differ across mobile device categories in the game? Furthermore, are there variations in the maximum levels achieved by users within each category?\nThis analysis seeks to uncover patterns and disparities in user behavior based on the distinction between mobile and phone categories. Distinct users and their associated category is selected and it is observed that users generally prefer playing from their mobile phones.\n\nmax_level_data_category &lt;- df %&gt;%\n  group_by(user_pseudo_id, category) %&gt;%\n  summarize(max_level = max(level, na.rm = TRUE))\n\nggplot(max_level_data_category, aes(x = category, y = max_level,fill = category)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of Maximum Level by Category\",\n       x = \"Category\",\n       y = \"Max Level\",\n       fill = \"Category\") +\n  theme_minimal()\n\n\n\n\nWhen comparing users’ achievements in reaching the maximum levels through box plots, no significant difference has been observed.\n\nsuccess_data &lt;- df %&gt;%\n  filter(event_name == \"level_end\" & success == 1)\n\nleveltrialcountbyuser_wsuccess &lt;- success_data %&gt;%\n  group_by(user_pseudo_id, level) %&gt;%\n  summarise(trials = n())\n\nleveltrialcountbyuser_wsuccess &lt;- leveltrialcountbyuser_wsuccess %&gt;%\n  group_by(level) %&gt;%\n  summarise(distinct_user_count = n(), trials = sum(trials))\n\nggplot(leveltrialcountbyuser_wsuccess, aes(x = level, y = distinct_user_count, fill = factor(level))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Frequency of Trials at Each Level\",\n       x = \"Level\",\n       y = \"Frequency\",\n       fill = \"Level\") +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  \n        legend.position = \"none\")\n\n\n\n\nFinal research question is What is the distribution of successful trial frequencies at different levels within the game? As a continuation of the research on levels, we can comment on whether players go back and replay the same level or which level they can pass more easily. It seems that the points where the drop in the graph is broken are around levels 17 and 26. After level 50, the total number of attempts decreases below 15.\n\nleveltrialcountbyuser =df |&gt; filter(event_name==\"level_end\") |&gt; \n  group_by(level) |&gt; count(level)\n\nlevelsuccesscountbyuser =df |&gt; filter(event_name==\"level_end\" & success==\"1\") |&gt; \n  group_by(level) |&gt;count(level)\n\ncolnames(levelsuccesscountbyuser)[2]=\"success\"\nmerged_data &lt;- merge(leveltrialcountbyuser, levelsuccesscountbyuser, by = \"level\", all = TRUE)\n\nmerged_data$success &lt;- ifelse(is.na(merged_data$success), 0, merged_data$success)\n\nhead(merged_data)\n\n  level    n success\n1     1 5213       0\n2     2  127     117\n3     3  189     109\n4     4  219     100\n5     5  268      93\n6     6  354      81\n\nmerged_data[,\"success_rate\"]&lt;- merged_data$success/merged_data$n\n\nmerged_data$level=merged_data$level-1\n\nggplot(merged_data[merged_data$level&lt;10 &merged_data$level!=0 ,], aes(x = level, y = success_rate)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(x = \"Level\", y = \"Success Rate\", title = \"Funnel Plot of Success Rate by Level\") +\n  theme_minimal() +\n  scale_x_continuous(breaks = df$level)\n\n\n\n\nTo understand the success rates of users between levels, we calculated the success rates of the levels. In our data set, success column indicates 1 when users are successful in the level 1 and 0 when unsuccessful. We calculated the success rate by dividing the success rate to total number of attempts. As expected level 1 seems to be the easier level to pass.\n\n\nABOUT US\nAltan Sener kertmeyenkeleilekertenkele@gmail.com\nAbdullah Mert Celikkol\nAlican Aktag alcnaktag@gmail.com\nFeel free to ask all your questions"
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html",
    "href": "Miray_Ecenaz/STAT570 Final Project.html",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the dataset.\nAuthors:\nMiray Çınar - miray.cinar@metu.edu.tr\nEcenaz Tunç - ecenaz.tunc@metu.edu.tr"
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#introduction",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#introduction",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nGlassdoor is an online platform where former or new employees can comment on companies and is also used for job search. In their website, they define themselves as: “Glassdoor is one of the world’s largest job and recruiting sites. We pride ourselves on helping people find a job and company they love; in fact, it’s our mission. Our company was built on the foundation of increasing workplace transparency. With that in mind, we have developed numerous tools to help job seekers make more informed career decisions.”"
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#data-description",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#data-description",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Data Description",
    "text": "Data Description\nData is obtained from Kaggle, in which, the user claims that the data is obtained from Glassdoor.com by using web scrapping. The link for the data could be acquired from the References section. The variables in this data set are defined as follows:\nJob Title: Title of the job posting\nSalary Estimation: Salary range for that particular job\nJob Description: This contains the full description of that job\nRating: Rating of that post\nCompany: Name of company\nLocation: Location of the company\nHeadquarter: Location of the headquater\nSize: Total employee in that company\nType of ownership: Describes the company type i.e. non-profit/public/private farm etc\nIndustry, Sector: Field applicant will work in\nRevenue: Total revenue of the company"
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#import-the-data",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#import-the-data",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Import the Data",
    "text": "Import the Data\nLet’s start by importing the data. We tried to import the data from Kaggle to Rstudio directly, by using several different packages but unfortunately for us, it was not possible. But we uploaded the data into out Github repository, so you can directly obtain it from there or from the Kaggle link that we put in the references. In the Kaggle, you can see that there are actually tow data sets, one is already cleaned, and one is unclean. We used the Uncleaned one here for some struggling :) First, import the required libraries. If you don’t have them, you can use install.packages() function.\n\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(ggplot2)\n\nUncleaned_DS_jobs &lt;- read_csv(\"Uncleaned_DS_jobs.csv\", show_col_types = F)\n\nLet’s start by investigating our data set a little bit, by getting a glimpse and see the structure of the data:\n\nlibrary(dplyr)\nglimpse(Uncleaned_DS_jobs)\n\nRows: 672\nColumns: 15\n$ index               &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …\n$ `Job Title`         &lt;chr&gt; \"Sr Data Scientist\", \"Data Scientist\", \"Data Scien…\n$ `Salary Estimate`   &lt;chr&gt; \"$137K-$171K (Glassdoor est.)\", \"$137K-$171K (Glas…\n$ `Job Description`   &lt;chr&gt; \"Description\\n\\nThe Senior Data Scientist is respo…\n$ Rating              &lt;dbl&gt; 3.1, 4.2, 3.8, 3.5, 2.9, 4.2, 3.9, 3.5, 4.4, 3.6, …\n$ `Company Name`      &lt;chr&gt; \"Healthfirst\\n3.1\", \"ManTech\\n4.2\", \"Analysis Grou…\n$ Location            &lt;chr&gt; \"New York, NY\", \"Chantilly, VA\", \"Boston, MA\", \"Ne…\n$ Headquarters        &lt;chr&gt; \"New York, NY\", \"Herndon, VA\", \"Boston, MA\", \"Bad …\n$ Size                &lt;chr&gt; \"1001 to 5000 employees\", \"5001 to 10000 employees…\n$ Founded             &lt;dbl&gt; 1993, 1968, 1981, 2000, 1998, 2010, 1996, 1990, 19…\n$ `Type of ownership` &lt;chr&gt; \"Nonprofit Organization\", \"Company - Public\", \"Pri…\n$ Industry            &lt;chr&gt; \"Insurance Carriers\", \"Research & Development\", \"C…\n$ Sector              &lt;chr&gt; \"Insurance\", \"Business Services\", \"Business Servic…\n$ Revenue             &lt;chr&gt; \"Unknown / Non-Applicable\", \"$1 to $2 billion (USD…\n$ Competitors         &lt;chr&gt; \"EmblemHealth, UnitedHealth Group, Aetna\", \"-1\", \"…\n\n\nAnd also take a quick summary:\n\nsummary(Uncleaned_DS_jobs)\n\n     index        Job Title         Salary Estimate    Job Description   \n Min.   :  0.0   Length:672         Length:672         Length:672        \n 1st Qu.:167.8   Class :character   Class :character   Class :character  \n Median :335.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   :335.5                                                           \n 3rd Qu.:503.2                                                           \n Max.   :671.0                                                           \n     Rating       Company Name         Location         Headquarters      \n Min.   :-1.000   Length:672         Length:672         Length:672        \n 1st Qu.: 3.300   Class :character   Class :character   Class :character  \n Median : 3.800   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 3.519                                                           \n 3rd Qu.: 4.300                                                           \n Max.   : 5.000                                                           \n     Size              Founded     Type of ownership    Industry        \n Length:672         Min.   :  -1   Length:672         Length:672        \n Class :character   1st Qu.:1918   Class :character   Class :character  \n Mode  :character   Median :1995   Mode  :character   Mode  :character  \n                    Mean   :1636                                        \n                    3rd Qu.:2009                                        \n                    Max.   :2019                                        \n    Sector            Revenue          Competitors       \n Length:672         Length:672         Length:672        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n\nFrom both glimpse() and summary() outputs, we can see that, categorical variables are in character form. We will investigate them one by one later on. But first, let’s change the column names that have blank spaces so that it will be much easy to make the analyses later.\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;% \n  rename(Job_Title = `Job Title`, \n         Salary_Estimate = `Salary Estimate`,\n         Job_Description = `Job Description`,\n         Company_Name = `Company Name`, \n         Type_of_Ownership =  `Type of ownership`)\n\nTake a summary again to see the data:\n\nsummary(Uncleaned_DS_jobs)\n\n     index        Job_Title         Salary_Estimate    Job_Description   \n Min.   :  0.0   Length:672         Length:672         Length:672        \n 1st Qu.:167.8   Class :character   Class :character   Class :character  \n Median :335.5   Mode  :character   Mode  :character   Mode  :character  \n Mean   :335.5                                                           \n 3rd Qu.:503.2                                                           \n Max.   :671.0                                                           \n     Rating       Company_Name         Location         Headquarters      \n Min.   :-1.000   Length:672         Length:672         Length:672        \n 1st Qu.: 3.300   Class :character   Class :character   Class :character  \n Median : 3.800   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 3.519                                                           \n 3rd Qu.: 4.300                                                           \n Max.   : 5.000                                                           \n     Size              Founded     Type_of_Ownership    Industry        \n Length:672         Min.   :  -1   Length:672         Length:672        \n Class :character   1st Qu.:1918   Class :character   Class :character  \n Mode  :character   Median :1995   Mode  :character   Mode  :character  \n                    Mean   :1636                                        \n                    3rd Qu.:2009                                        \n                    Max.   :2019                                        \n    Sector            Revenue          Competitors       \n Length:672         Length:672         Length:672        \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n\nFrom our summary, we can also see some strange values are present in the data. For instance there are some rows marked as “-1” in the Headquarters, Founded."
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#investigating-the-columns",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#investigating-the-columns",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Investigating the Columns",
    "text": "Investigating the Columns\nLet’s investigate the columns one by one:\n\nIndex\n\nIndex column is not necessary for us, so we will remove it from our data set.\n\nUncleaned_DS_jobs$index &lt;- NULL\n\n\nRating\n\nWe realized that from the summary, Rating has a minimum value as -1, but the rating should be between 1 to 5.\nWe need to fix that problem.\nTo fix this, first we need to look how many data are there with Rating = -1:\n\nsum(Uncleaned_DS_jobs$Rating == -1)\n\n[1] 50\n\n\nWe have 50 values with Rating = -1. Rating variable should not be -1. So firstly for the rating variable we give change -1 to 0.\n\nUncleaned_DS_jobs$Rating[Uncleaned_DS_jobs$Rating == -1] &lt;- 0\n\nNow lets check if it worked,\n\nsummary(Uncleaned_DS_jobs$Rating)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   3.300   3.800   3.593   4.300   5.000 \n\n\nAs can be seen from the summary of the rating we fix the “-1” problem. From the histogram of the rating variable below, we can see that majority of the companies recieved 4 on the rating.\n\nintervals &lt;- seq(0, 5, by = 1)\n\nrating_histogram &lt;- ggplot(Uncleaned_DS_jobs, aes(x = Rating)) +\n  geom_histogram(binwidth = 1, boundary = 0.5, col = \"darkblue\", fill = \"lightblue\") +\n  labs(\n    title = \"Distribution of Rating\",\n    x = \"Rating\",\n    y = \"Frequency\",\n    subtitle = \"\"\n  ) +\n  scale_x_continuous(breaks = intervals) +  \n  theme_classic()\nrating_histogram\n\n\n\n\n\nFounded\n\nAfter looking in a more detailed way, we realize that the foundation year of the companies have a value “-1” also we need to check for them:\n\nsum(Uncleaned_DS_jobs$Founded == -1)\n\n[1] 118\n\n\n\nUncleaned_DS_jobs$Founded[Uncleaned_DS_jobs$Founded == -1] &lt;- \"No information\"\n\n\nsummary(as.factor(Uncleaned_DS_jobs$Founded), maxsum = 6)\n\nNo information           2012           2011           1996           1999 \n           118             34             25             22             22 \n       (Other) \n           451 \n\n\nFrom the summary we also fix the problem for Founded variable.\n\nIndustry\n\n\nsum(Uncleaned_DS_jobs$Industry == -1)\n\n[1] 71\n\n\nWe see that industry has 71 “-1” values so, again we assign those values to no information.\n\nUncleaned_DS_jobs$Industry[Uncleaned_DS_jobs$Industry == -1] &lt;- \"No information\"\n\nHistogram of the Industry Variable:\n\nindustry_plot&lt;- ggplot(Uncleaned_DS_jobs, aes(x=Industry)) + \n  labs(title = \"Distribution of Industry\", x = \"Industry\", subtitle = \"\") + \n  geom_bar(colour=\"darkblue\", fill=\"lightblue\") + \n  geom_text(stat='count', aes(label=..count..), vjust=-0.5,size=2.68) +\n  theme_classic()+\n   theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))\nindustry_plot\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nAs can be seen from the graph it is hard to read the x-axis names, so to solve this problem, we picked the 10 industries that have the most frequencies in the data and draw a plot regarding these industries.\n\ntop10_industries &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Industry) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  top_n(10)\n\nSelecting by count\n\n# Reorder the levels of Industry based on frequency\nplot_data &lt;-  Uncleaned_DS_jobs\nplot_data$Industry &lt;- factor(plot_data$Industry, levels = top10_industries$Industry)\n\n# Filter data to include only the top 10 industries\nfiltered_data &lt;- plot_data %&gt;%\n  filter(Industry %in% top10_industries$Industry)\n\nindustry_plot_top10 &lt;- ggplot(filtered_data, \n                              aes(x = Industry)) +\n  labs(title = \"Distribution of Top 10 Industries\", \n       x = \"Industry\", subtitle = \"\") +\n  geom_bar(colour = \"darkblue\", fill = \"lightblue\") +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))\n\nindustry_plot_top10\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nAs can be seen from the graph that, top 10 industries that the companies are in; Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software and so on.\n\nSector\n\nRealizing that sector variable also has “-1” values.\n\nsum(Uncleaned_DS_jobs$Sector == -1) \n\n[1] 71\n\n\nWe change “-1” values to no information for the sector variable.\n\nUncleaned_DS_jobs$Sector[Uncleaned_DS_jobs$Sector == -1] &lt;- \"No information\"\n\nHistogram of the Sector Variable:\n\ntop10_sector &lt;- Uncleaned_DS_jobs %&gt;%   \n  group_by(Sector) %&gt;%   \n  summarise(count = n()) %&gt;%   \n  arrange(desc(count)) %&gt;%   \n  top_n(10)  \n\nSelecting by count\n\n# Reorder the levels of Sector based on frequency \nplot_data &lt;-  Uncleaned_DS_jobs\nplot_data$Sector &lt;- factor(plot_data$Sector, \n                           levels = top10_sector$Sector)  \n\n# Filter data to include only the top 10 sector \nfiltered_data &lt;- plot_data %&gt;%   \n  filter(Sector %in% top10_sector$Sector)  \n\nSector_plot_top10 &lt;- ggplot(filtered_data, aes(x = Sector)) +   \n  labs(title = \"Distribution of Top 10 Sector\", x = \"Sector\", subtitle = \"\") +    \n  geom_bar(colour = \"darkblue\", fill = \"lightblue\") +  \n  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +   \n  theme_classic() +   \n  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))   \n\nSector_plot_top10\n\n\n\n\nAgain for the sectors, similar to the industries, we had a lot of categories. So we decided to show the top 10. It can be seen that Information Technology has the highest job openings, followed by Business Services and again Biotech & Pharmaceuticals and so on.\n\nRevenue\n\nIn revenue column there are some “-1” values\n\nsum(Uncleaned_DS_jobs$Revenue == -1)\n\n[1] 27\n\n\nAnd this column has a value called “Unknown / Non-Applicable”:\n\nhead(Uncleaned_DS_jobs$Revenue)\n\n[1] \"Unknown / Non-Applicable\"   \"$1 to $2 billion (USD)\"    \n[3] \"$100 to $500 million (USD)\" \"$100 to $500 million (USD)\"\n[5] \"Unknown / Non-Applicable\"   \"Unknown / Non-Applicable\"  \n\n\nWe convert “-1” to that value.\n\nUncleaned_DS_jobs$Revenue[Uncleaned_DS_jobs$Revenue == -1] &lt;- \"Unknown / Non-Applicable\"  \n\nHistogram of the Revenue variable:\n\nrevenue_plot&lt;- ggplot(Uncleaned_DS_jobs, aes(x=Revenue)) + \n  labs(title = \"Distribution of Revenue\", x = \"Revenue\", subtitle = \"\") + \n  geom_bar(colour=\"darkblue\", fill=\"lightblue\") + \n  geom_text(stat='count', aes(label=..count..), vjust=-0.5,size=2.2) +\n  theme_classic()+\n   theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nrevenue_plot\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nFor the Revenue variable, from the graph it can be seen that unfortunately we had a lot of missing data. But among the non-missing ones, the highest frequency that we detected in our data set was for the companies with $100 to $500 million revenue.\n\nCompany Name\n\nWhen we check the Company Name variable, we see that it also has the Rating next to it:\n\nhead(Uncleaned_DS_jobs[, c(\"Company_Name\", \"Rating\")])\n\n# A tibble: 6 × 2\n  Company_Name              Rating\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 \"Healthfirst\\n3.1\"           3.1\n2 \"ManTech\\n4.2\"               4.2\n3 \"Analysis Group\\n3.8\"        3.8\n4 \"INFICON\\n3.5\"               3.5\n5 \"Affinity Solutions\\n2.9\"    2.9\n6 \"HG Insights\\n4.2\"           4.2\n\n\nWe can separate them and get rid of the Rating variable inside of Company Name to clean this variable. We can do this by str_replace() function.\n\nUncleaned_DS_jobs$Company_Name &lt;- str_replace(Uncleaned_DS_jobs$Company_Name, \"\\n[0-9.]+$\", \"\")\n\nNow we have cleaned the Company Name variable:\n\nhead(Uncleaned_DS_jobs[, c(\"Company_Name\", \"Rating\")])\n\n# A tibble: 6 × 2\n  Company_Name       Rating\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Healthfirst           3.1\n2 ManTech               4.2\n3 Analysis Group        3.8\n4 INFICON               3.5\n5 Affinity Solutions    2.9\n6 HG Insights           4.2\n\n\nAnd we can see that how many of the Company Names:\n\nUncleaned_DS_jobs |&gt; \n  count(Company_Name, sort = TRUE)\n\n# A tibble: 432 × 2\n   Company_Name                     n\n   &lt;chr&gt;                        &lt;int&gt;\n 1 Hatch Data Inc                  12\n 2 Maxar Technologies              12\n 3 Tempus Labs                     11\n 4 AstraZeneca                     10\n 5 Klaviyo                          8\n 6 Autodesk                         7\n 7 Phoenix Operations Group         7\n 8 Novetta                          6\n 9 Southwest Research Institute     6\n10 MassMutual                       5\n# ℹ 422 more rows\n\n\nWe can see that the company with the most positions opened is “Hatch Data Inc” and “Maxar Technologies” with 12 positions opened.\nHistogram of the Company name:\nSame problem occurred here like industries so, to see the plot again the 10 company names with the highest frequencies. Histogram of the company name:\n\ntop10_companies &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Company_Name) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  top_n(10)\n\nSelecting by count\n\n# Reorder the levels of Company names based on frequency\nplot_data &lt;-  Uncleaned_DS_jobs\nplot_data$Company_Name &lt;- factor(plot_data$Company_Name, levels = top10_companies$Company_Name)\n\n# Filter data to include only the top 10 industries\nfiltered_data &lt;- plot_data %&gt;%\n  filter(Company_Name %in% top10_companies$Company_Name)\n\ncompany_plot_top10 &lt;- ggplot(filtered_data, aes(x = Company_Name)) +\n  labs(title = \"Distribution of Top 10 Companies\", x = \"Company Name\", subtitle = \"\") +\n  geom_bar(colour = \"darkblue\", fill = \"lightblue\") +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, size = 2.68) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))\n\ncompany_plot_top10\n\n\n\n\n\nSize\n\nAs can be seen from the summary that we have “-1” for the Size. But we have unknown category for this variable.\n\nsummary(as.factor(Uncleaned_DS_jobs$Size))\n\n                     -1       1 to 50 employees        10000+ employees \n                     27                      86                      80 \n 1001 to 5000 employees    201 to 500 employees 5001 to 10000 employees \n                    104                      85                      61 \n  501 to 1000 employees     51 to 200 employees                 Unknown \n                     77                     135                      17 \n\n\nSo we can assign “-1” to “Unknown” category for this variable:\n\nUncleaned_DS_jobs$Size[Uncleaned_DS_jobs$Size == -1] &lt;- \"Unknown\"\n\n\nsummary(as.factor(Uncleaned_DS_jobs$Size))\n\n      1 to 50 employees        10000+ employees  1001 to 5000 employees \n                     86                      80                     104 \n   201 to 500 employees 5001 to 10000 employees   501 to 1000 employees \n                     85                      61                      77 \n    51 to 200 employees                 Unknown \n                    135                      44 \n\n\nHistogram of the Size variable:\n\nlibrary(dplyr)\n\n# Create a frequency table for Size\nsize_counts &lt;- count(Uncleaned_DS_jobs, Size)\n\n# Sort the data by count in ascending order\nsize_counts &lt;- arrange(size_counts, desc(n))\n\n# Create the plot\nsize_plot &lt;- ggplot(size_counts, aes(x = reorder(Size, n), y = n)) + \n  labs(title = \"Distribution of Size of the Companies\", x = \"Size\", y = \"Frequency\") + \n  geom_col(colour = \"darkblue\", fill = \"lightblue\") + \n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nsize_plot\n\n\n\n\nInterestingly, from the plot we can see that the size of the companies in terms of employees, we see the highest frequency in 51 to 200 employees, however, other categories also have numbers close to each other.\n\nCompetitors\n\nThere are -1 values in Competitors. We don’t know their competitors’ name so we can attribute them to no information:\n\nUncleaned_DS_jobs$Competitors[Uncleaned_DS_jobs$Competitors == \"-1\"] &lt;- \"No information\"\n\n\nsummary(as.factor(Uncleaned_DS_jobs$Competitors), maxsum = 6)\n\n                                             No information \n                                                        501 \n                           Roche, GlaxoSmithKline, Novartis \n                                                         10 \n            Leidos, CACI International, Booz Allen Hamilton \n                                                          6 \nLos Alamos National Laboratory, Battelle, SRI International \n                                                          6 \n                            Battelle, General Atomics, SAIC \n                                                          3 \n                                                    (Other) \n                                                        146 \n\n\nFor the competitor companies of the job posting companies we have a lot of different values, but the most repetitive ones could be seen from the output above.\n\nLocation\n\nLet’s see the location variable first.\n\nsummary(as.factor(Uncleaned_DS_jobs$Location), maxsum = 6)\n\nSan Francisco, CA      New York, NY    Washington, DC        Boston, MA \n               69                50                26                24 \n      Chicago, IL           (Other) \n               22               481 \n\n\nIn the Location variable, we can see that they are written with the state which they are in. So we want to separate them. But, before that, in our data there are some problematic rows:\nSome rows are too short. Let’s see that columns:\n\nUncleaned_DS_jobs %&gt;% \n  filter(\n    str_count(Location, \",\\\\s+\") != 1\n    ) %&gt;% \n  select(Location) %&gt;% distinct_all()\n\n# A tibble: 7 × 1\n  Location                  \n  &lt;chr&gt;                     \n1 Remote                    \n2 United States             \n3 Utah                      \n4 New Jersey                \n5 Texas                     \n6 Patuxent, Anne Arundel, MD\n7 California                \n\n\nFrom this output, we can see that we have “Remote”, “United States”, locations that have the same names as their states; “Utah”, “New Jersey”, “Texas” and “California”, and “Patuxent, Anne Arundel, MD” which is a region for the Anne Arundel county. So, we will add information for this columns firstly, then we will separate the Location and States. For this, we will use str_replace() function.\n\n# Define replacements using case_when\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;%\n  mutate(\n    Location = case_when(\n      Location == \"Remote\" ~ str_replace(Location, \"Remote\", \"Remote, R\"),\n      Location == \"United States\" ~ str_replace(Location, \"United States\", \"United States, US\"),\n      Location == \"Utah\" ~ str_replace(Location, \"Utah\", \"Utah, UT\"),\n      Location == \"New Jersey\" ~ str_replace(Location, \"New Jersey\", \"New Jersey, NJ\"),\n      Location == \"Texas\" ~ str_replace(Location, \"Texas\", \"Texas, TX\"),\n      Location == \"California\" ~ str_replace(Location, \"California\", \"California, CA\"),\n      Location == \"Patuxent, Anne Arundel, MD\" ~ str_replace(Location, \"Patuxent, Anne Arundel, MD\", \"Anne Arundel, MD\"),\n      TRUE ~ Location\n    )\n  )\n\nNow we can separate them:\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;%\n  separate(\n    Location,\n    into = c(\"Location\", \"Location_State\"),\n    sep = \",\\\\s+\")\n\nLet’s visualize Location_State variable:\n\nlocationstates_counts &lt;- count(Uncleaned_DS_jobs, Location_State)\n\n# Sort the data by count in ascending order\nlocationstates_counts &lt;- arrange(locationstates_counts, desc(n))\n\n# Create the plot\nlocationstate_plot &lt;- ggplot(locationstates_counts, aes(x = reorder(Location_State, n), y = n)) + \n  labs(title = \"Distribution of States of the Locations of the Companies\", x = \"States\", y = \"Frequency\") + \n  geom_col(colour = \"darkblue\", fill = \"lightblue\") + \n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\nlocationstate_plot\n\n\n\n\nWe can see that majority of the states for the job postings are from California.\n\nHeadquarters\n\nFor the headquarters we have “-1” values also,\n\nsum(Uncleaned_DS_jobs$Headquarters == -1)\n\n[1] 31\n\n\n\nUncleaned_DS_jobs$Headquarters[Uncleaned_DS_jobs$Headquarters == \"-1\"] &lt;- \"No information\"\n\n\nsummary(as.factor(Uncleaned_DS_jobs$Headquarters), maxsum = 6)\n\n     New York, NY    No information San Francisco, CA       Chicago, IL \n               33                31                31                23 \n       Boston, MA           (Other) \n               19               535 \n\n\nSimilar to Location, we want to separate the states and the city. We will apply the similar approach to fix this column.\n\nUncleaned_DS_jobs %&gt;% \n  filter(\n    str_count(Headquarters, \",\\\\s+\") != 1\n    ) %&gt;% \n  select(Location) %&gt;% distinct_all()\n\n# A tibble: 14 × 1\n   Location     \n   &lt;chr&gt;        \n 1 Hauppauge    \n 2 Reston       \n 3 New York     \n 4 Palo Alto    \n 5 San Francisco\n 6 Brooklyn     \n 7 Sterling     \n 8 Chantilly    \n 9 Cambridge    \n10 Omaha        \n11 Fort Belvoir \n12 Naperville   \n13 Redmond      \n14 Irwindale    \n\n\nWe will assign the states for this rows too. and also to not get a warning regarding the NA values later, “No Information” is also going to be fixed:\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;%\n  mutate(\n    Headquarters = case_when(\n      Headquarters == \"Hauppauge\" ~ str_replace(Headquarters, \"Hauppauge\", \"Hauppauge, NY\"),\n      Headquarters == \"Reston\" ~ str_replace(Headquarters, \"Reston\", \"Reston, VA\"),\n      Headquarters == \"New York\" ~ str_replace(Headquarters, \"New York\", \"New York, NY\"),\n      Headquarters == \"Palo Alto\" ~ str_replace(Headquarters, \"Palo Alto\", \"Palo Alto, CA\"),\n      Headquarters == \"San Francisco\" ~ str_replace(Headquarters, \"San Francisco\", \"San Francisco, CA\"),\n      Headquarters == \"Brooklyn\" ~ str_replace(Headquarters, \"Brooklyn\", \"Brooklyn, NY\"),\n      Headquarters == \"Sterling\" ~ str_replace(Headquarters, \"Sterling\", \"Sterling, IL\"),\n      Headquarters == \"Chantilly\" ~ str_replace(Headquarters, \"Chantilly\", \"Chantilly, VA\"),\n      Headquarters == \"Cambridge\" ~ str_replace(Headquarters, \"Cambridge\", \"Cambridge, MA\"),\n      Headquarters == \"Fort Belvoir\" ~ str_replace(Headquarters, \"Fort Belvoir\", \"Fort Belvoir, VA\"),\n      Headquarters == \"Naperville\" ~ str_replace(Headquarters, \"Naperville\", \"Naperville, IL\"),\n      Headquarters == \"Redmond\" ~ str_replace(Headquarters, \"Redmond\", \"Redmond, WA\"),\n      Headquarters == \"Irwindale\" ~ str_replace(Headquarters, \"Irwindale\", \"Irwindale, CA\"),\n      Headquarters == \"No information\" ~ str_replace(Headquarters, \"No information\", \"No information, No information\"),\n      TRUE ~ Headquarters\n    )\n  )\n\nAnd finally, we will separate the Headquarters and their states:\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;%\n  separate(\n    Headquarters,\n    into = c(\"Headquarters\", \"Headquarters_State\"),\n    sep = \",\\\\s+\")\n\nLet’s visualize Headquarters States:\n\nhqstates_counts &lt;- count(Uncleaned_DS_jobs, Headquarters_State)\n\n# Sort the data by count in ascending order\nhqstates_counts &lt;- arrange(hqstates_counts, desc(n))\n\n# Create the plot\nhqstate_plot &lt;- ggplot(hqstates_counts, aes(x = reorder(Headquarters_State, n), y = n)) + \n  labs(title = \"Distribution of States of the Locations of the Headquarters\", x = \"States\", y = \"Frequency\") + \n  geom_col(colour = \"darkblue\", fill = \"lightblue\") + \n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\nhqstate_plot\n\n\n\n\nWe can see that majority of the states for the job postings companies’ headquartes are located in California. But we can also see that some of Campanies’ Headqurters are located in other countries like UK, Switzerland, France, Canada etc.\n\nType Of Ownership\n\nThere are “-1” values in the Type of Ownership also.\nWe check how many “-1” values are in the variable.\n\nsum(Uncleaned_DS_jobs$Type_of_Ownership == -1)\n\n[1] 27\n\n\n\nUncleaned_DS_jobs$Type_of_Ownership[Uncleaned_DS_jobs$Type_of_Ownership == \"-1\"] &lt;- \"Unknown\"\n\nLet’s see the summary:\n\nsummary(as.factor(Uncleaned_DS_jobs$Type_of_Ownership))\n\n          College / University              Company - Private \n                             3                            397 \n              Company - Public                       Contract \n                           153                              2 \n                    Government                       Hospital \n                            10                              1 \n        Nonprofit Organization             Other Organization \n                            36                              5 \n       Private Practice / Firm                  Self-employed \n                             4                              2 \nSubsidiary or Business Segment                        Unknown \n                            28                             31 \n\n\nLet’s visualize type of ownership’s of the companies:\n\nType_of_Ownership_counts &lt;- count(Uncleaned_DS_jobs, Type_of_Ownership)\n\n# Sort the data by count in ascending order\nType_of_Ownership_counts &lt;- arrange(Type_of_Ownership_counts, desc(n))\n\n# Create the plot\ntow_plot &lt;- ggplot(Type_of_Ownership_counts, aes(x = reorder(Type_of_Ownership, n), y = n)) + \n  labs(title = \"Distribution of Type of Ownerships of the Companies\", x = \"Type of Ownership\", y = \"Frequency\") + \n  geom_col(colour = \"darkblue\", fill = \"lightblue\") + \n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\ntow_plot\n\n\n\n\nWe can see that majority, more than half, of the companies are made up of private companies, followed by public and nonprofit organizations.\n\nSalary Estimation\n\nFor Salary Estimate column, let’s see the unique values we have:\n\nlevels(as.factor(Uncleaned_DS_jobs$Salary_Estimate))\n\n [1] \"$101K-$165K (Glassdoor est.)\" \"$105K-$167K (Glassdoor est.)\"\n [3] \"$110K-$163K (Glassdoor est.)\" \"$112K-$116K (Glassdoor est.)\"\n [5] \"$122K-$146K (Glassdoor est.)\" \"$124K-$198K (Glassdoor est.)\"\n [7] \"$128K-$201K (Glassdoor est.)\" \"$137K-$171K (Glassdoor est.)\"\n [9] \"$138K-$158K (Glassdoor est.)\" \"$141K-$225K (Glassdoor est.)\"\n[11] \"$145K-$225K(Employer est.)\"   \"$212K-$331K (Glassdoor est.)\"\n[13] \"$31K-$56K (Glassdoor est.)\"   \"$56K-$97K (Glassdoor est.)\"  \n[15] \"$66K-$112K (Glassdoor est.)\"  \"$69K-$116K (Glassdoor est.)\" \n[17] \"$71K-$123K (Glassdoor est.)\"  \"$75K-$131K (Glassdoor est.)\" \n[19] \"$79K-$106K (Glassdoor est.)\"  \"$79K-$131K (Glassdoor est.)\" \n[21] \"$79K-$133K (Glassdoor est.)\"  \"$79K-$147K (Glassdoor est.)\" \n[23] \"$80K-$132K (Glassdoor est.)\"  \"$87K-$141K (Glassdoor est.)\" \n[25] \"$90K-$109K (Glassdoor est.)\"  \"$90K-$124K (Glassdoor est.)\" \n[27] \"$91K-$150K (Glassdoor est.)\"  \"$92K-$155K (Glassdoor est.)\" \n[29] \"$95K-$119K (Glassdoor est.)\"  \"$99K-$132K (Glassdoor est.)\" \n\n\n\nsum(is.na(as.factor(Uncleaned_DS_jobs$Salary_Estimate)))\n\n[1] 0\n\n\nFrom this output, we can see that we have common shape for the salary estimates with 0 NA values. We can separate this column into two separate columns for obtaining lower and upper limits for the salary estimates.\n\n# Remove spaces in the column \nUncleaned_DS_jobs$Salary_Estimate_wo_spaces &lt;- Uncleaned_DS_jobs$Salary_Estimate\n\nUncleaned_DS_jobs$Salary_Estimate_wo_spaces &lt;- gsub(\" \", \"\",Uncleaned_DS_jobs$Salary_Estimate)  \n# Display the updated data frame \nhead(Uncleaned_DS_jobs$Salary_Estimate_wo_spaces) \n\n[1] \"$137K-$171K(Glassdoorest.)\" \"$137K-$171K(Glassdoorest.)\"\n[3] \"$137K-$171K(Glassdoorest.)\" \"$137K-$171K(Glassdoorest.)\"\n[5] \"$137K-$171K(Glassdoorest.)\" \"$137K-$171K(Glassdoorest.)\"\n\n\nNow we have no blank space between the words. Let’s get rid of the parts at the end; Glassdoor est. and Employer est.\n\nUncleaned_DS_jobs$Salary_Estimate_wo_spaces &lt;- \n  gsub(\"K\\\\(Glassdoorest.\\\\)\",\n       \"\",\n       Uncleaned_DS_jobs$Salary_Estimate_wo_spaces) \n\n\nUncleaned_DS_jobs$Salary_Estimate_wo_spaces &lt;- \n  gsub(\"K\\\\(Employerest.\\\\)\",\n       \"\",\n       Uncleaned_DS_jobs$Salary_Estimate_wo_spaces)\n\nhead(Uncleaned_DS_jobs$Salary_Estimate_wo_spaces)\n\n[1] \"$137K-$171\" \"$137K-$171\" \"$137K-$171\" \"$137K-$171\" \"$137K-$171\"\n[6] \"$137K-$171\"\n\n\nLet’s see how we can select the numbers that are remaining in the rows: we can use [0-9]+ for this part:\n\nstr_view(\n  Uncleaned_DS_jobs$Salary_Estimate_wo_spaces, \n  \"[0-9]+\")\n\n [1] │ $&lt;137&gt;K-$&lt;171&gt;\n [2] │ $&lt;137&gt;K-$&lt;171&gt;\n [3] │ $&lt;137&gt;K-$&lt;171&gt;\n [4] │ $&lt;137&gt;K-$&lt;171&gt;\n [5] │ $&lt;137&gt;K-$&lt;171&gt;\n [6] │ $&lt;137&gt;K-$&lt;171&gt;\n [7] │ $&lt;137&gt;K-$&lt;171&gt;\n [8] │ $&lt;137&gt;K-$&lt;171&gt;\n [9] │ $&lt;137&gt;K-$&lt;171&gt;\n[10] │ $&lt;137&gt;K-$&lt;171&gt;\n[11] │ $&lt;137&gt;K-$&lt;171&gt;\n[12] │ $&lt;137&gt;K-$&lt;171&gt;\n[13] │ $&lt;137&gt;K-$&lt;171&gt;\n[14] │ $&lt;137&gt;K-$&lt;171&gt;\n[15] │ $&lt;137&gt;K-$&lt;171&gt;\n[16] │ $&lt;137&gt;K-$&lt;171&gt;\n[17] │ $&lt;137&gt;K-$&lt;171&gt;\n[18] │ $&lt;137&gt;K-$&lt;171&gt;\n[19] │ $&lt;137&gt;K-$&lt;171&gt;\n[20] │ $&lt;137&gt;K-$&lt;171&gt;\n... and 652 more\n\n\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs |&gt;    \n  separate_wider_regex(     \n    Salary_Estimate_wo_spaces,     \n    patterns = c(\n      \"\\\\$\",\n      Low_Limit_For_Salary = \"[0-9]+\",       \n      \"K-\\\\$\",       \n      High_Limit_For_Salary = \"[0-9]+\"  \n      )   \n    ) \n\nBy using separate_wider_regex() function, we defined the pattern in the data, and we got the new columns as Low_Limit_For_Salary and High_Limit_For_Salary as we wished.\n\nhead(Uncleaned_DS_jobs[c(\"Salary_Estimate\",\n                         \"Low_Limit_For_Salary\", \n                         \"High_Limit_For_Salary\")])\n\n# A tibble: 6 × 3\n  Salary_Estimate              Low_Limit_For_Salary High_Limit_For_Salary\n  &lt;chr&gt;                        &lt;chr&gt;                &lt;chr&gt;                \n1 $137K-$171K (Glassdoor est.) 137                  171                  \n2 $137K-$171K (Glassdoor est.) 137                  171                  \n3 $137K-$171K (Glassdoor est.) 137                  171                  \n4 $137K-$171K (Glassdoor est.) 137                  171                  \n5 $137K-$171K (Glassdoor est.) 137                  171                  \n6 $137K-$171K (Glassdoor est.) 137                  171                  \n\n\nFor not confusing the numbers later, lets multiply the low limit and high limit numbers with 1000 and make Salary Estimate factor.\n\nUncleaned_DS_jobs &lt;- Uncleaned_DS_jobs %&gt;%  \n  mutate(\n    Low_Limit_For_Salary = as.numeric(Low_Limit_For_Salary)*1000,\n    High_Limit_For_Salary = as.numeric(High_Limit_For_Salary)*1000)\n\n\nUncleaned_DS_jobs$Salary_Estimate &lt;- as.factor(Uncleaned_DS_jobs$Salary_Estimate)\n\nhead(Uncleaned_DS_jobs[c(\"Salary_Estimate\",\n                         \"Low_Limit_For_Salary\", \n                         \"High_Limit_For_Salary\")])\n\n# A tibble: 6 × 3\n  Salary_Estimate              Low_Limit_For_Salary High_Limit_For_Salary\n  &lt;fct&gt;                                       &lt;dbl&gt;                 &lt;dbl&gt;\n1 $137K-$171K (Glassdoor est.)               137000                171000\n2 $137K-$171K (Glassdoor est.)               137000                171000\n3 $137K-$171K (Glassdoor est.)               137000                171000\n4 $137K-$171K (Glassdoor est.)               137000                171000\n5 $137K-$171K (Glassdoor est.)               137000                171000\n6 $137K-$171K (Glassdoor est.)               137000                171000\n\n\nNow we obtained two new columns as Low_Limit_For_salary and High_Limit_For_Salary for the salary estimates.\n\nJob Title\n\nFor Job Title column, first let’s examine it:\n\nglimpse(\n  as.factor(\n    Uncleaned_DS_jobs$Job_Title))\n\n Factor w/ 172 levels \"(Sr.) Data Scientist -\",..: 156 50 50 50 50 50 63 50 165 50 ...\n\n\nAs we can see, we have 172 different levels for Job Titles. We can try to group them by searching common words.\n\nhead(\n  levels(\n    as.factor(\n      Uncleaned_DS_jobs$Job_Title)))\n\n[1] \"(Sr.) Data Scientist -\"                                \n[2] \"AI Data Scientist\"                                     \n[3] \"AI Ops Data Scientist\"                                 \n[4] \"AI/ML - Machine Learning Scientist, Siri Understanding\"\n[5] \"Analytics - Business Assurance Data Analyst\"           \n[6] \"Analytics Manager\"                                     \n\n\nBut before that, we can see that some columns have “Senior”, “Experience” words. By using this information, we can create a new column for seniority of the job. By using str_view() function, first, let’s see that columns;\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\"^Senior|^Sr|^Experience\", \n        multiline = TRUE))\n\n  [1] │ &lt;Sr&gt; Data Scientist\n [16] │ &lt;Experience&gt;d Data Scientist\n [34] │ &lt;Senior&gt; Research Statistician- Data Scientist\n [40] │ &lt;Senior&gt; Analyst/Data Scientist\n [47] │ &lt;Senior&gt; Data Scientist\n [57] │ &lt;Senior&gt; Data Scientist\n [93] │ &lt;Senior&gt; Data Scientist\n [99] │ &lt;Senior&gt; Data Scientist\n[104] │ &lt;Senior&gt; Data Scientist\n[107] │ &lt;Sr&gt; Data Engineer (Sr BI Developer)\n[122] │ &lt;Senior&gt; Data Engineer\n[123] │ &lt;Senior&gt; Data Scientist\n[126] │ &lt;Sr&gt;. ML/Data Scientist - AI/NLP/Chatbot\n[130] │ &lt;Sr&gt;. ML/Data Scientist - AI/NLP/Chatbot\n[132] │ &lt;Senior&gt; Data Engineer\n[137] │ &lt;Senior&gt; Data Engineer\n[143] │ &lt;Senior&gt; Data Scientist\n[154] │ &lt;Sr&gt; Scientist - Extractables & Leachables\n[156] │ &lt;Sr&gt; Data Scientist\n[158] │ &lt;Experience&gt;d Data Scientist\n... and 51 more\n\n\nBy using str_detect() function, we can detect the rows including “Senior”, “Sr”, “Experienced” words. This function returns TRUE if they exist, and returns FALSE if they don’t exist.\nBy using as.integer() , we assign 1 to exists and 0 to nonexistent.\n\nUncleaned_DS_jobs$Senior_Position &lt;- as.integer(\n  str_detect(Uncleaned_DS_jobs$Job_Title, \n             regex(\"^(Senior|Sr|Experienced)\")\n             ) )\n\nNow that we defined the senior roles, we can assign the same titles to same jobs. Let’s start by Data Scientist and find the columns including “Data Scientist” word.\n\nstr_view(Uncleaned_DS_jobs$Job_Title, \n         regex(\".*Data\\\\s+Scientist.*\", \n         multiline = TRUE))\n\n [1] │ &lt;Sr Data Scientist&gt;\n [2] │ &lt;Data Scientist&gt;\n [3] │ &lt;Data Scientist&gt;\n [4] │ &lt;Data Scientist&gt;\n [5] │ &lt;Data Scientist&gt;\n [6] │ &lt;Data Scientist&gt;\n [7] │ &lt;Data Scientist / Machine Learning Expert&gt;\n [8] │ &lt;Data Scientist&gt;\n [9] │ &lt;Staff Data Scientist - Analytics&gt;\n[10] │ &lt;Data Scientist&gt;\n[11] │ &lt;Data Scientist&gt;\n[12] │ &lt;Data Scientist&gt;\n[13] │ &lt;Data Scientist - Statistics, Early Career&gt;\n[15] │ &lt;Data Scientist&gt;\n[16] │ &lt;Experienced Data Scientist&gt;\n[17] │ &lt;Data Scientist - Contract&gt;\n[18] │ &lt;Data Scientist&gt;\n[21] │ &lt;Data Scientist&gt;\n[22] │ &lt;Data Scientist/Machine Learning&gt;\n[25] │ &lt;Data Scientist&gt;\n... and 435 more\n\n\nBu using str_replace_all() we can replace all the rows including Data Scientist word in some way, directly with “Data Scientist”.\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title,\n    \".*Data\\\\s+Scientist.*\",\n    \"Data Scientist\")\n\nNow let’s get Data Analyst titles:\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\".*Data\\\\s+Analyst.*\", \n        multiline = TRUE,\n        ignore_case = TRUE))\n\n [19] │ &lt;Data Analyst II&gt;\n [41] │ &lt;Data Analyst&gt;\n [43] │ &lt;Data Analyst I&gt;\n [51] │ &lt;Data Analyst&gt;\n [55] │ &lt;E-Commerce Data Analyst&gt;\n [61] │ &lt;Data Analyst&gt;\n [65] │ &lt;Global Data Analyst&gt;\n [74] │ &lt;Business Data Analyst&gt;\n [76] │ &lt;Data Analyst&gt;\n [87] │ &lt;Data Analyst&gt;\n[111] │ &lt;RFP Data Analyst&gt;\n[112] │ &lt;Data Analyst&gt;\n[118] │ &lt;Data Analyst/Engineer&gt;\n[139] │ &lt;Data Analyst&gt;\n[162] │ &lt;Say Business Data Analyst&gt;\n[164] │ &lt;Data Analyst&gt;\n[167] │ &lt;Senior Data Analyst&gt;\n[168] │ &lt;Senior Data Analyst&gt;\n[170] │ &lt;Sr Data Analyst&gt;\n[175] │ &lt;Data Analyst&gt;\n... and 27 more\n\n\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title,\n    \".*Data\\\\s+Analyst.*\",\n    \"Data Analyst\")\n\nNow let’s get Data Engineer titles:\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\".*Data\\\\s+Engineer.*\", \n        multiline = TRUE,\n        ignore_case = TRUE))\n\n [35] │ &lt;Data Engineer&gt;\n [54] │ &lt;Jr. Data Engineer&gt;\n [66] │ &lt;Data Engineer&gt;\n [71] │ &lt;Data Engineer (Remote)&gt;\n [77] │ &lt;Data Engineer, Enterprise Analytics&gt;\n [83] │ &lt;Data Engineer&gt;\n[107] │ &lt;Sr Data Engineer (Sr BI Developer)&gt;\n[108] │ &lt;Data Engineer&gt;\n[114] │ &lt;Data Engineer&gt;\n[120] │ &lt;Data Engineer&gt;\n[122] │ &lt;Senior Data Engineer&gt;\n[124] │ &lt;Data Engineer&gt;\n[129] │ &lt;Data Engineer&gt;\n[132] │ &lt;Senior Data Engineer&gt;\n[133] │ &lt;Data Engineer&gt;\n[137] │ &lt;Senior Data Engineer&gt;\n[140] │ &lt;Data Engineer&gt;\n[142] │ &lt;Tableau Data Engineer 20-0117&gt;\n[153] │ &lt;Data Engineer&gt;\n[169] │ &lt;Data Engineer&gt;\n... and 27 more\n\n\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title,\n    \".*Data\\\\s+Engineer.*\",\n    \"Data Engineer\")\n\nAnd Machine Learning Engineers:\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\".*Machine\\\\s+Learning.*\", \n        multiline = TRUE, \n        ignore_case = TRUE))\n\n [42] │ &lt;Machine Learning Engineer&gt;\n [46] │ &lt;Computational Scientist, Machine Learning&gt;\n [62] │ &lt;Machine Learning Engineer&gt;\n [69] │ &lt;Data & Machine Learning Scientist&gt;\n [89] │ &lt;Machine Learning Engineer&gt;\n [92] │ &lt;Machine Learning Engineer&gt;\n[102] │ &lt;Machine Learning Engineer&gt;\n[135] │ &lt;Machine Learning Engineer&gt;\n[136] │ &lt;Machine Learning Engineer&gt;\n[145] │ &lt;Machine Learning Engineer&gt;\n[159] │ &lt;Machine Learning Engineer&gt;\n[172] │ &lt;Machine Learning Scientist - Bay Area, CA&gt;\n[176] │ &lt;Senior Data & Machine Learning Scientist&gt;\n[180] │ &lt;Machine Learning Engineer&gt;\n[191] │ &lt;Principal Machine Learning Scientist&gt;\n[203] │ &lt;Machine Learning Engineer&gt;\n[220] │ &lt;Senior Machine Learning Scientist - Bay Area, CA&gt;\n[229] │ &lt;Machine Learning Engineer&gt;\n[261] │ &lt;Principal Machine Learning Scientist&gt;\n[331] │ &lt;Machine Learning Engineer&gt;\n... and 16 more\n\n\n\nUncleaned_DS_jobs$Job_Title &lt;-  str_replace_all(\n  Uncleaned_DS_jobs$Job_Title,\n  \".*Machine\\\\s+Learning.*\",\n  \"Machine Learning Engineer\")\n\nLet’s examine Managers this time:\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\".*Analytics\\\\s+Manager.*|.*Data\\\\s+Science\\\\sManager.*|.*Director.*|.*Vice\\\\sPresident.*|.*VP.*|.*Principal.*|.*Manager.*\", \n        multiline = TRUE, \n        ignore_case = TRUE))\n\n [86] │ &lt;Data Science Manager, Payment Acceptance - USA&gt;\n[150] │ &lt;Analytics Manager&gt;\n[198] │ &lt;Principal Scientist/Associate Director, Quality Control and Analytical Technologies&gt;\n[218] │ &lt;Analytics Manager - Data Mart&gt;\n[266] │ &lt;Director of Data Science&gt;\n[272] │ &lt;Manager / Lead, Data Science & Analytics&gt;\n[313] │ &lt;Principal Scientist/Associate Director, Quality Control and Analytical Technologies&gt;\n[332] │ &lt;Principal Data & Analytics Platform Engineer&gt;\n[343] │ &lt;VP, Data Science&gt;\n[381] │ &lt;Analytics Manager - Data Mart&gt;\n[470] │ &lt;VP, Data Science&gt;\n[523] │ &lt;Manager, Field Application Scientist, Southeast&gt;\n[564] │ &lt;Data Science Manager&gt;\n[581] │ &lt;Vice President, Biometrics and Clinical Data Management&gt;\n\n\nLet’s replace them with “Data Science and Analytics Manager”\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title,\n    \".*Analytics\\\\s+Manager.*|.*Data\\\\s+Science\\\\sManager.*|.*Director.*|.*Vice\\\\sPresident.*|.*VP.*|.*Principal.*|.*Manager.*\",\n    \"Data Science and Analytics Manager\")\n\nNow, bu using str_view() function, we want to see all the jobs that have “Data” in it, but not “Data Analyst”, “Data Scientist,”Data Engineer” or “Data Science and Analytics Manager” because we already took care of that titles.\n\nstr_view(\n  Uncleaned_DS_jobs$Job_Title, \n  regex(\"^(?!.*Data\\\\s+Analyst.*|.*Data\\\\s+Scientist.*|.*Data\\\\s+Science\\\\s+and\\\\s+Analytics\\\\s+Manager.*|.*Data\\\\sEngineer.*).*data.*\", \n        ignore_case = TRUE))\n\n [14] │ &lt;Data Modeler&gt;\n [24] │ &lt;Business Intelligence Analyst I- Data Insights&gt;\n [56] │ &lt;Data Analytics Engineer&gt;\n [97] │ &lt;Data Analytics Engineer&gt;\n[117] │ &lt;Software Engineer - Data Science&gt;\n[141] │ &lt;Data Integration and Modeling Engineer&gt;\n[187] │ &lt;Production Engineer - Statistics/Data Analysis&gt;\n[207] │ &lt;Data Science Instructor&gt;\n[214] │ &lt;Data Science Software Engineer&gt;\n[219] │ &lt;Data Modeler (Analytical Systems)&gt;\n[230] │ &lt;Equity Data Insights Analyst - Quantitative Analyst&gt;\n[257] │ &lt;Environmental Data Science&gt;\n[370] │ &lt;Data Science Software Engineer&gt;\n[382] │ &lt;Data Modeler (Analytical Systems)&gt;\n[388] │ &lt;IT Partner Digital Health Technology and Data Science&gt;\n[396] │ &lt;Data Solutions Engineer - Data Modeler&gt;\n[519] │ &lt;Data Science Software Engineer&gt;\n[540] │ &lt;Data Science Analyst&gt;\n[545] │ &lt;Data Modeler (Analytical Systems)&gt;\n[555] │ &lt;IT Partner Digital Health Technology and Data Science&gt;\n... and 3 more\n\n\nWe will save these as “Other Data Positions”\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title,\n     regex(\"(?!Data\\\\s+(Analyst|Scientist|Engineer|Science\\\\s+and\\\\s+Analytics\\\\s+Manager)).*Data.*\"),\n    \"Other Data Positions\" )\n\nFinally, we will save all the jobs that are not include “Data” word in it and not “Machine Learning Engineer” into “Others” category because there are a lot of jobs with the titles like Scientist, Researcher etc.\n\nUncleaned_DS_jobs$Job_Title &lt;-  \n  str_replace_all(\n    Uncleaned_DS_jobs$Job_Title, \n    regex(\"^(?!.*(Data|Machine\\\\s+Learning\\\\s+Engineer)).*$\"), \n    \"Others\")\n\nFinally, let’s see our clean job titles:\n\nUncleaned_DS_jobs$Job_Title &lt;- as.factor(Uncleaned_DS_jobs$Job_Title)\n\nsummary(Uncleaned_DS_jobs$Job_Title)\n\n                      Data Analyst                      Data Engineer \n                                47                                 47 \nData Science and Analytics Manager                     Data Scientist \n                                14                                455 \n         Machine Learning Engineer               Other Data Positions \n                                36                                 23 \n                            Others \n                                50 \n\n\nLet’s visualize Job Titles:\n\njt_counts &lt;- count(Uncleaned_DS_jobs, Job_Title)\n\n# Sort the data by count in ascending order\njt_counts &lt;- arrange(jt_counts, desc(n))\n\njt_plot &lt;- ggplot(jt_counts, aes(x = reorder(Job_Title, n), y = n)) + \n   labs(title = \"Distribution of Job Titles\", x = \"Job Titles\", y = \"Frequency\") + \n   geom_col(colour = \"darkblue\", fill = \"lightblue\") + \n   geom_text(aes(label = n), vjust = -0.5, size = 3) +\n   theme_classic() +\n   theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\njt_plot\n\n\n\n\nWe can see that we have 455 “Data Scientist” postings, followed by 50 “Others”, 47 “Data Engineers” , 47 “Data Analysts” and so on.\n\nJob Description\n\nWhen we look at the job description column,\nWe have so many different values but we can differentiate them into other columns like we can say that a job wants the skill SQL.\nFirst, we need to look at the job description column in a detailed way.\n\nhead(Uncleaned_DS_jobs$Job_Description,1)\n\n[1] \"Description\\n\\nThe Senior Data Scientist is responsible for defining, building, and improving statistical models to improve business processes and outcomes in one or more healthcare domains such as Clinical, Enrollment, Claims, and Finance. As part of the broader analytics team, Data Scientist will gather and analyze data to solve and address complex business problems and evaluate scenarios to make predictions on future outcomes and work with the business to communicate and support decision-making. This position requires strong analytical skills and experience in analytic methods including multivariate regressions, hierarchical linear models, regression trees, clustering methods and other complex statistical techniques.\\n\\nDuties & Responsibilities:\\n\\n• Develops advanced statistical models to predict, quantify or forecast various operational and performance metrics in multiple healthcare domains\\n• Investigates, recommends, and initiates acquisition of new data resources from internal and external sources\\n• Works with multiple teams to support data collection, integration, and retention requirements based on business needs\\n• Identifies critical and emerging technologies that will support and extend quantitative analytic capabilities\\n• Collaborates with business subject matter experts to select relevant sources of information\\n• Develops expertise with multiple machine learning algorithms and data science techniques, such as exploratory data analysis and predictive modeling, graph theory, recommender systems, text analytics and validation\\n• Develops expertise with Healthfirst datasets, data repositories, and data movement processes\\n• Assists on projects/requests and may lead specific tasks within the project scope\\n• Prepares and manipulates data for use in development of statistical models\\n• Other duties as assigned\\n\\nMinimum Qualifications:\\n\\n-Bachelor's Degree\\n\\nPreferred Qualifications:\\n\\n- Master’s degree in Computer Science or Statistics\\nFamiliarity with major cloud platforms such as AWS and Azure\\nHealthcare Industry Experience\\n\\nMinimum Qualifications:\\n\\n-Bachelor's Degree\\n\\nPreferred Qualifications:\\n\\n- Master’s degree in Computer Science or Statistics\\nFamiliarity with major cloud platforms such as AWS and Azure\\nHealthcare Industry Experience\\n\\nWE ARE AN EQUAL OPPORTUNITY EMPLOYER. Applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected Federal, State/Province or Local status unrelated to the performance of the work involved.\\n\\nIf you have a disability under the Americans with Disability Act or a similar law, and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services.\\nEEO Law Poster and Supplement\\n\\n]]&gt;\"\n\n\nWe see some common requirements and common job descriptions for jobs. For this, we can separate the columns like SQL and we can say that this jobs wants an SQL bu using factor 1 or 0. Let’s start with SQL and check if SQL is mentioned in the variable Job_Description:\n\nsql_mentioned &lt;- function(description) {\n  # We use tolower to match the SQL in the job description\n  description &lt;- tolower(description)\n  \n  # Check if SQL is mentioned\n  if (grepl(\"\\\\bsql\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\nNow we need to create a column called SQL, in this column we will see if SQL is a requirement in the job description or not.\n\nUncleaned_DS_jobs$sql_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, sql_mentioned)\n\n\nUncleaned_DS_jobs$sql_needed &lt;- as.factor(Uncleaned_DS_jobs$sql_needed)\n\nNow for Python we repeat the same process.\n\npython_mentioned &lt;- function(description) {\n  # We use tolower to match the python in the job description\n  description &lt;- tolower(description)\n  \n  # Check if python is mentioned\n  if (grepl(\"\\\\bpython\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$python_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, python_mentioned)\n\n\nUncleaned_DS_jobs$python_needed &lt;- as.factor(Uncleaned_DS_jobs$python_needed)\n\nNow for Excel:\n\nexcel_mentioned &lt;- function(description) {\n  # We use tolower to match the excel in the job description\n  description &lt;- tolower(description)\n  \n  # Check if excel is mentioned\n  if (grepl(\"\\\\bexcel\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$excel_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, excel_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$excel_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.1161  0.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$excel_needed &lt;- as.factor(Uncleaned_DS_jobs$excel_needed)\n\nFor Hadoop:\n\nhadoop_mentioned &lt;- function(description) {\n  # We use tolower to match the hadoop in the job description\n  description &lt;- tolower(description)\n  \n  # Check if hadoop is mentioned\n  if (grepl(\"\\\\bhadoop\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$hadoop_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, hadoop_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$hadoop_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2128  0.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$hadoop_needed&lt;- as.factor(Uncleaned_DS_jobs$hadoop_needed)\n\nFor Spark:\n\nspark_mentioned &lt;- function(description) {\n  # We use tolower to match the spark in the job description\n  description &lt;- tolower(description)\n  \n  # Check if spark is mentioned\n  if (grepl(\"\\\\bspark\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$spark_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, spark_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$spark_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2664  1.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$spark_needed &lt;- as.factor(Uncleaned_DS_jobs$spark_needed)\n\nFor AWS:\n\naws_mentioned &lt;- function(description) {\n  # We use tolower to match the AWS in the job description\n  description &lt;- tolower(description)\n  \n  # Check if AWS is mentioned\n  if (grepl(\"\\\\baws\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$aws_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, aws_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$aws_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2009  0.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$aws_needed &lt;- as.factor(Uncleaned_DS_jobs$aws_needed)\n\nFor Tableau:\n\ntableau_mentioned &lt;- function(description) {\n  # We use tolower to match the Tableau in the job description\n  description &lt;- tolower(description)\n  \n  # Check if Tableau is mentioned\n  if (grepl(\"\\\\btableau\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$tableau_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, tableau_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$tableau_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   0.000   0.183   0.000   1.000 \n\n\n\nUncleaned_DS_jobs$tableau_needed &lt;- as.factor(Uncleaned_DS_jobs$tableau_needed)\n\nFor Big Data:\n\nbigdata_mentioned &lt;- function(description) {\n  # We use tolower to match the Big data in the job description\n  description &lt;- tolower(description)\n  \n  # Check if Big data is mentioned\n  if (grepl(\"\\\\bbig-data\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$bigdata_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, bigdata_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$bigdata_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00000 0.00000 0.01042 0.00000 1.00000 \n\n\n\nUncleaned_DS_jobs$bigdata_needed&lt;- as.factor(Uncleaned_DS_jobs$bigdata_needed)\n\nFor Numpy:\n\nnumpy_mentioned &lt;- function(description) {\n  # We use tolower to match the Numpy in the job description\n  description &lt;- tolower(description)\n  \n  # Check if Numpy is mentioned\n  if (grepl(\"\\\\bnumpy\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$numpy_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, numpy_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$numpy_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00000 0.00000 0.08482 0.00000 1.00000 \n\n\n\nUncleaned_DS_jobs$numpy_needed &lt;- as.factor(Uncleaned_DS_jobs$numpy_needed)\n\nFor Machine Learning:\n\nML_mentioned &lt;- function(description) {\n  # We use tolower to match the ML in the job description\n  description &lt;- tolower(description)\n  \n  # Check if ML is mentioned\n  if (grepl(\"\\\\bmachine learning\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$ML_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, ML_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$ML_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   1.000   0.619   1.000   1.000 \n\n\n\nUncleaned_DS_jobs$ML_needed&lt;- as.factor(Uncleaned_DS_jobs$ML_needed)\n\nFor Deep Learning:\n\nDL_mentioned &lt;- function(description) {\n  # We use tolower to match the DL in the job description\n  description &lt;- tolower(description)\n  \n  # Check if DL is mentioned\n  if (grepl(\"\\\\bdeep learning\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$DL_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, DL_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$DL_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.1429  0.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$DL_needed &lt;- as.factor(Uncleaned_DS_jobs$DL_needed)\n\nFor Statistics:\n\nstat_mentioned &lt;- function(description) {\n  # We use tolower to match the statistics in the job description\n  description &lt;- tolower(description)\n  \n  # Check if statistics is mentioned\n  if (grepl(\"\\\\bstatistics\\\\b\", description)) {\n    return(1)\n  } else {\n    return(0)\n  }\n}\n\n\nUncleaned_DS_jobs$stat_needed &lt;- sapply(Uncleaned_DS_jobs$Job_Description, stat_mentioned)\n\n\nsummary(Uncleaned_DS_jobs$stat_needed)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4926  1.0000  1.0000 \n\n\n\nUncleaned_DS_jobs$stat_needed &lt;- as.factor(Uncleaned_DS_jobs$stat_needed)\n\nNow Let’s check the new columns in our dataset:\n\nsummary(Uncleaned_DS_jobs)\n\n                              Job_Title                       Salary_Estimate\n Data Analyst                      : 47   $75K-$131K (Glassdoor est.) : 32   \n Data Engineer                     : 47   $79K-$131K (Glassdoor est.) : 32   \n Data Science and Analytics Manager: 14   $99K-$132K (Glassdoor est.) : 32   \n Data Scientist                    :455   $137K-$171K (Glassdoor est.): 30   \n Machine Learning Engineer         : 36   $90K-$109K (Glassdoor est.) : 30   \n Other Data Positions              : 23   $56K-$97K (Glassdoor est.)  : 22   \n Others                            : 50   (Other)                     :494   \n Job_Description        Rating      Company_Name         Location        \n Length:672         Min.   :0.000   Length:672         Length:672        \n Class :character   1st Qu.:3.300   Class :character   Class :character  \n Mode  :character   Median :3.800   Mode  :character   Mode  :character  \n                    Mean   :3.593                                        \n                    3rd Qu.:4.300                                        \n                    Max.   :5.000                                        \n                                                                         \n Location_State     Headquarters       Headquarters_State     Size          \n Length:672         Length:672         Length:672         Length:672        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Founded          Type_of_Ownership    Industry            Sector         \n Length:672         Length:672         Length:672         Length:672        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Revenue          Competitors        Low_Limit_For_Salary\n Length:672         Length:672         Min.   : 31000      \n Class :character   Class :character   1st Qu.: 79000      \n Mode  :character   Mode  :character   Median : 91000      \n                                       Mean   : 99196      \n                                       3rd Qu.:122000      \n                                       Max.   :212000      \n                                                           \n High_Limit_For_Salary Senior_Position  sql_needed python_needed excel_needed\n Min.   : 56000        Min.   :0.0000   0:347      0:183         0:594       \n 1st Qu.:119000        1st Qu.:0.0000   1:325      1:489         1: 78       \n Median :133000        Median :0.0000                                        \n Mean   :148131        Mean   :0.1057                                        \n 3rd Qu.:165000        3rd Qu.:0.0000                                        \n Max.   :331000        Max.   :1.0000                                        \n                                                                             \n hadoop_needed spark_needed aws_needed tableau_needed bigdata_needed\n 0:529         0:493        0:537      0:549          0:665         \n 1:143         1:179        1:135      1:123          1:  7         \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n numpy_needed ML_needed DL_needed stat_needed\n 0:615        0:256     0:576     0:341      \n 1: 57        1:416     1: 96     1:331"
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#visualizations",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#visualizations",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Visualizations",
    "text": "Visualizations\nFirst, let’s see our variables:\n\nglimpse(Uncleaned_DS_jobs)\n\nRows: 672\nColumns: 31\n$ Job_Title             &lt;fct&gt; Data Scientist, Data Scientist, Data Scientist, …\n$ Salary_Estimate       &lt;fct&gt; $137K-$171K (Glassdoor est.), $137K-$171K (Glass…\n$ Job_Description       &lt;chr&gt; \"Description\\n\\nThe Senior Data Scientist is res…\n$ Rating                &lt;dbl&gt; 3.1, 4.2, 3.8, 3.5, 2.9, 4.2, 3.9, 3.5, 4.4, 3.6…\n$ Company_Name          &lt;chr&gt; \"Healthfirst\", \"ManTech\", \"Analysis Group\", \"INF…\n$ Location              &lt;chr&gt; \"New York\", \"Chantilly\", \"Boston\", \"Newton\", \"Ne…\n$ Location_State        &lt;chr&gt; \"NY\", \"VA\", \"MA\", \"MA\", \"NY\", \"CA\", \"MA\", \"MA\", …\n$ Headquarters          &lt;chr&gt; \"New York\", \"Herndon\", \"Boston\", \"Bad Ragaz\", \"N…\n$ Headquarters_State    &lt;chr&gt; \"NY\", \"VA\", \"MA\", \"Switzerland\", \"NY\", \"CA\", \"Sw…\n$ Size                  &lt;chr&gt; \"1001 to 5000 employees\", \"5001 to 10000 employe…\n$ Founded               &lt;chr&gt; \"1993\", \"1968\", \"1981\", \"2000\", \"1998\", \"2010\", …\n$ Type_of_Ownership     &lt;chr&gt; \"Nonprofit Organization\", \"Company - Public\", \"P…\n$ Industry              &lt;chr&gt; \"Insurance Carriers\", \"Research & Development\", …\n$ Sector                &lt;chr&gt; \"Insurance\", \"Business Services\", \"Business Serv…\n$ Revenue               &lt;chr&gt; \"Unknown / Non-Applicable\", \"$1 to $2 billion (U…\n$ Competitors           &lt;chr&gt; \"EmblemHealth, UnitedHealth Group, Aetna\", \"No i…\n$ Low_Limit_For_Salary  &lt;dbl&gt; 137000, 137000, 137000, 137000, 137000, 137000, …\n$ High_Limit_For_Salary &lt;dbl&gt; 171000, 171000, 171000, 171000, 171000, 171000, …\n$ Senior_Position       &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ sql_needed            &lt;fct&gt; 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, …\n$ python_needed         &lt;fct&gt; 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, …\n$ excel_needed          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hadoop_needed         &lt;fct&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ spark_needed          &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, …\n$ aws_needed            &lt;fct&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, …\n$ tableau_needed        &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, …\n$ bigdata_needed        &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ numpy_needed          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ ML_needed             &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, …\n$ DL_needed             &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n$ stat_needed           &lt;fct&gt; 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, …\n\n\n\nSalary Distribution by Job Titles: Let’s display the distribution of salaries for different job titles; both for low limit estimate for salary and high limit estimate for salary:\nLow Limit Estimate for Salary\n\n\nsalary_distribution &lt;- ggplot(Uncleaned_DS_jobs, aes(x = Job_Title, y = Low_Limit_For_Salary)) +\n  geom_boxplot() +\n  labs(title = \"Low Limit Salary Distribution by Job Title\",\n       x = \"Job Title\",\n       y = \"Low Limit for Salary\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nsalary_distribution\n\n\n\n\nFrom the plot, we can see that the medians are not really different from each other for the several Job Titles. We can see that, interestingly, “Other” type of jobs seem to have a higher median in their salaries for the low limit compared to the jobs including “Data” in their titles.\n\n# Calculate median low salary limits for each job title\nsalary_median &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Job_Title) %&gt;%\n  summarise(median_salary = median(Low_Limit_For_Salary))\n\n# Sort the data by median low salary in descending order\nsalary_median &lt;- salary_median %&gt;%\n  arrange(desc(median_salary))\n\n\nsalary_distribution &lt;- ggplot(salary_median, \n                              aes(x = reorder(Job_Title, \n                                              median_salary), \n                                  y = median_salary)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", col = \"darkblue\") +\n  geom_text(aes(label = scales::comma(median_salary)), vjust = -0.5, size = 3) +\n  labs(title = \"Median Low Limit for Salary by Job Title\",\n       x = \"Job Title\",\n       y = \"Median Low Limit for Salary\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nsalary_distribution\n\n\n\n\nJust to be sure, we would like to check the medians in a bar graph anyway. From the graph, as we suggested earlier, we can see that “Other” job titles indeed have a higher median in their pays with 100K. Let’s examine the High Limit Estimate for Salary:\n\nHigh Limit Estimate for Salary\n\n\nsalary_distribution_high &lt;- ggplot(Uncleaned_DS_jobs, aes(x = Job_Title, y = High_Limit_For_Salary)) +\n  geom_boxplot() +\n  labs(title = \"High Limit Salary Distribution by Job Title\",\n       x = \"Job Title\",\n       y = \"High Limit for Salary\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nsalary_distribution_high\n\n\n\n\nFrom the box-plot of the pays, as may be expected, “Data Science and Analytics Managers” have the highest median in pay compared to the other job titles. But we can see that we also have outliers in both managers, as well as other jobs too. Let’s see them just out of curiosity:\n\n# Arrange the data by 'Job_Title' and 'Salary' in descending order\nsorted_data &lt;- Uncleaned_DS_jobs %&gt;% dplyr::arrange(Job_Title, desc(High_Limit_For_Salary))\n\n# Filter to keep the rows with the highest salary for each job title\nhighest_salary_rows &lt;- sorted_data %&gt;% group_by(Job_Title) %&gt;% slice(1)\n\n# Print the filtered rows\nhighest_salary_rows |&gt; select(Job_Title, Salary_Estimate,Company_Name)\n\n# A tibble: 7 × 3\n# Groups:   Job_Title [7]\n  Job_Title                          Salary_Estimate              Company_Name  \n  &lt;fct&gt;                              &lt;fct&gt;                        &lt;chr&gt;         \n1 Data Analyst                       $141K-$225K (Glassdoor est.) ShorePoint    \n2 Data Engineer                      $128K-$201K (Glassdoor est.) Kingfisher Sy…\n3 Data Science and Analytics Manager $212K-$331K (Glassdoor est.) 10x Genomics  \n4 Data Scientist                     $212K-$331K (Glassdoor est.) Roche         \n5 Machine Learning Engineer          $212K-$331K (Glassdoor est.) Allen Institu…\n6 Other Data Positions               $212K-$331K (Glassdoor est.) Klaviyo       \n7 Others                             $212K-$331K (Glassdoor est.) Southwest Res…\n\n\nHere we can see the highest salary range for each of the job titles.\n\n# Calculate median low salary limits for each job title\nsalary_median &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Job_Title) %&gt;%\n  summarise(median_salary = median(High_Limit_For_Salary))\n\n# Sort the data by median low salary in descending order\nsalary_median &lt;- salary_median %&gt;%\n  arrange(desc(median_salary))\n\n\nsalary_distribution &lt;- ggplot(salary_median, \n                              aes(x = reorder(Job_Title, \n                                              median_salary), \n                                  y = median_salary)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\", col = \"darkblue\") +\n  geom_text(aes(label = scales::comma(median_salary)), vjust = -0.5, size = 3) +\n  labs(title = \"Median High Limit for Salary by Job Title\",\n       x = \"Job Title\",\n       y = \"Median High Limit for Salary\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(size = 9.2, angle = 45, hjust = 1))\n\nsalary_distribution\n\n\n\n\nAs can be seen from the medians graph that there is not a significant difference between the medians of the different job titles but Data Science and Analytics Managers have the highest median pay with 147K.\n\nCompany Size vs. Salaries: To explore how salary ranges vary across different company sizes:\n\n# Create a new variable with sorted factor levels\ndata_sorted &lt;- transform(Uncleaned_DS_jobs, Size_Sorted = factor(Size, levels = sort(unique(Size))))\n\n# Create boxplot for Low_Limit_For_Salary with adjusted y-axis limits\nplot_low &lt;- ggplot(data_sorted, aes(x = Size_Sorted, y = Low_Limit_For_Salary, fill = factor(Size_Sorted))) +\n  geom_boxplot(alpha = 0.8) +\n  labs(title = \"Low Limit Salary Ranges across Company Sizes\",\n       x = \"Company Size\",\n       y = \"Low Limit Salary\") +\n  scale_fill_discrete(name = \"Company Size\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  coord_cartesian(ylim = c(0, 340000))  # Set y-axis limits\n\n# Create boxplot for High_Limit_For_Salary with the same y-axis limits\nplot_high &lt;- ggplot(data_sorted, aes(x = Size_Sorted, y = High_Limit_For_Salary, fill = factor(Size_Sorted))) +\n  geom_boxplot(alpha = 0.8) +\n  labs(title = \"High Limit Salary Ranges across Company Sizes\",\n       x = \"Company Size\",\n       y = \"High Limit Salary\") +\n  scale_fill_discrete(name = \"Company Size\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  coord_cartesian(ylim = c(0, 340000))  # Set y-axis limits\n\nplot_low\n\n\n\nplot_high\n\n\n\n\n\nFrom the box-plot of the High and Low Limit Ranges for the salaries versus company sizes, we can see that 5001 to 10000 employees have the highest median in the high limit salary ranges.\n\nJob Titles vs. Senior Positions: Visualize the proportion of senior positions against different job titles using a bar chart:\n\nFirstly, we can see the distribution of Senior Position among the job titles:\n\nUncleaned_DS_jobs %&gt;% \n  select(Senior_Position, Job_Title)  %&gt;% \n  group_by(Job_Title, Senior_Position)  %&gt;% \n  count()\n\n# A tibble: 12 × 3\n# Groups:   Job_Title, Senior_Position [12]\n   Job_Title                          Senior_Position     n\n   &lt;fct&gt;                                        &lt;int&gt; &lt;int&gt;\n 1 Data Analyst                                     0    37\n 2 Data Analyst                                     1    10\n 3 Data Engineer                                    0    41\n 4 Data Engineer                                    1     6\n 5 Data Science and Analytics Manager               0    14\n 6 Data Scientist                                   0   413\n 7 Data Scientist                                   1    42\n 8 Machine Learning Engineer                        0    30\n 9 Machine Learning Engineer                        1     6\n10 Other Data Positions                             0    23\n11 Others                                           0    43\n12 Others                                           1     7\n\n\nThen, we will calculate the percentage of the senior positions for every title:\n\nsenior_proportion &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Job_Title) %&gt;%\n  summarise(Percentage_Senior = mean(Senior_Position) * 100) %&gt;%\n  arrange(desc(Percentage_Senior))\nsenior_proportion\n\n# A tibble: 7 × 2\n  Job_Title                          Percentage_Senior\n  &lt;fct&gt;                                          &lt;dbl&gt;\n1 Data Analyst                                   21.3 \n2 Machine Learning Engineer                      16.7 \n3 Others                                         14   \n4 Data Engineer                                  12.8 \n5 Data Scientist                                  9.23\n6 Data Science and Analytics Manager              0   \n7 Other Data Positions                            0   \n\n\nLet’s create bar graph:\n\nsenior_plot &lt;- ggplot(senior_proportion, aes(x = reorder(Job_Title, Percentage_Senior), y = Percentage_Senior)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(Percentage_Senior), \"%\")), vjust = -0.5, size = 3.5, color = \"black\") +\n  labs(title = \"Proportion of Senior Positions by Job Title\",\n       x = \"Job Title\",\n       y = \"Percentage of Senior Positions\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nsenior_plot\n\n\n\n\nWe can see that the title that is searched for seniority is Data Analyst, however, only 21% of the Data Analyst positions are senior.\n\nIndustry Analysis: Use a tree-map to display the distribution of job positions across different industries.\n\n\nlibrary(treemap)\n\njob_count_by_industry &lt;- Uncleaned_DS_jobs %&gt;%\n  group_by(Industry) %&gt;%\n  summarise(Job_Count = n()) %&gt;%\n  arrange(desc(Job_Count))\n\n# Create a treemap for job positions across different industries with custom theme\ntreemap_plot &lt;- treemap(job_count_by_industry, index = \"Industry\", vSize = \"Job_Count\",\n                        title = \"Distribution of Job Positions across Industries\",\n                        palette = \"Blues\")\n\n\n\n\nWe can see that the industries that are hiring the most are: Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software, Aerospace & Defense and so on.\n\nRevenue vs. Ratings: Create a grouped box-plot to show the distribution of ratings for different revenue categories.\n\n\nboxplot &lt;- ggplot(Uncleaned_DS_jobs, \n                  aes(x = Revenue, \n                      y = Rating, \n                      fill = Revenue)) +\n  geom_boxplot(alpha = 0.8) +\n  labs(title = \"Rating Distribution across Revenue Categories\",\n       x = \"Revenue Categories\",\n       y = \"Rating\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nboxplot\n\n\n\n\nFrom the plot we can see that highest median rating for the companies according to their revenues was for the companies with $25 to $50 million (USD).\n\nOverview of Skills Needed in the Job Postings:\n\n\nskills_df &lt;- Uncleaned_DS_jobs[, c(\"sql_needed\", \"python_needed\", \"numpy_needed\", \"stat_needed\",\"excel_needed\",\"hadoop_needed\",\"spark_needed\",\n                    \"aws_needed\",\"tableau_needed\",\"bigdata_needed\",\"ML_needed\",\"DL_needed\")]\n\nskills_long &lt;- tidyr::gather(skills_df, key = \"Skill\", value = \"Needed\")\n\n\nmy_colors &lt;- c(\"lightblue\", \"darkblue\")\n\nskills_long$Needed &lt;- factor(skills_long$Needed, levels = c(\"0\", \"1\"))\n\nskills_plot &lt;- ggplot(skills_long, aes(x = Skill, fill = Needed)) +\n  geom_bar(position = \"dodge\", width = 0.7) +\n  scale_fill_manual(values = my_colors) + \n  labs(title = \"Overview of Skills Needed\",\n       x = \"Skill\",\n       y = \"Count\") +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nskills_plot\n\n\n\n\nAs can be seen from the graph that python and ML are most mentioned skills that are required in the job postings."
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#conclusion",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#conclusion",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, for this data handling and visualization project, we use the data from Kaggle named “Data Science Job Posting on Glassdoor”. This data set is originally obtained from Glassdoor website by using web scrapping. We started our analysis by data cleaning and tidied the columns by separating the columns that requiring more than one information. We grouped job titles by their similarities and we obtained new variables from them. At start, we had a total of 672 rows with 15 columns but at the end, we managed to have 31 columns from all the information we have found within the data set. As for findings, when we investigate the lower limit of salaries for the job postings, we saw that “Other” job titles, meaning jobs that does not have “data” in their titles at all, jobs like scientist, researcher etc. have a higher median in their pays with 100K. However, for the high limit for the salaries are investigated, as may be expected, “Data Science and Analytics Managers” have found to be the highest median in pay compared to the other job titles. When we examine High and Low Limit Ranges for the salaries versus company sizes, we can see that 5001 to 10000 employees have the highest median in the high limit salary ranges. Among the jobs postings, 21% of the Data Analyst positions are found as senior position. We saw that the industries that are hiring the most are: Biotech & Pharmaceuticals, IT Services, Computer Hardware & Software, Aerospace & Defense and so on. Furthermore, highest median rating for the companies according to their revenues was for the companies with $25 to $50 million (USD). Finally, Python and ML are most mentioned skills that are required in the job postings."
  },
  {
    "objectID": "Miray_Ecenaz/STAT570 Final Project.html#references",
    "href": "Miray_Ecenaz/STAT570 Final Project.html#references",
    "title": "Data Scientist Job Posting Data Analysis",
    "section": "References",
    "text": "References\nAbout us | glassdoor. (n.d.). https://www.glassdoor.com/about/\nBarr, D., & DeBruine, L. (n.d.). Data Cleaning. https://rgup.gitlab.io/research_cycle/03_tidyr.html\nCotton, R. (2023, February 16). Quarto cheat sheet (previously known as RMarkdown). DataCamp. https://www.datacamp.com/cheat-sheet/quarto-cheat-sheet-previously-known-as-r-markdown\nGREP: Pattern matching and replacement. RDocumentation. (n.d.). https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/grep\nRahman, R. (n.d.). [Dataset] Data Science Job Posting on Glassdoor. Retrieved December 20, 2023,. https://www.kaggle.com/datasets/rashikrahmanpritom/data-science-job-posting-on-glassdoor/data\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science: Import, Tidy, transform, visualize, and model data. O’Reilly Media, Inc."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Projects",
    "section": "",
    "text": "This website showcases the Final Projects from the Data Handling and Visualization Lecture (STAT570) during the Fall Semester (2023-2024) in the Statistics Department of METU offered by Fulya Gökalp Yavuz. All projects featured here have been created by the students of the lecture and edited by the lecturer.\n\n\n\n\n\n\nNote\n\n\n\nIt is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the data sets.\n\n\nAll groups aimed to construct a narrative around a data set with relevance to environmental impact, human well-being, and other related issues.\nPrior to these projects, our primary focus centered on R4DS(2e) and Building reproducible analytical pipelines with R books for practical exercises and hands-on projects. Each group established GitHub accounts and collaborated on shared projects by forking each other’s repositories. This approach was designed to provide practical experience in reproducible and open-source research, incorporating version control practices."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html",
    "href": "Maksoy_Sakil/WRF.html",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "",
    "text": "Note\n\n\n\nThis document is a product of the final project for the STAT 570 lecture, focusing on data handling and visualization tools. It is essential to acknowledge that minor errors may be present, and the methods employed may not necessarily reflect the optimal approach related to the dataset.\n#domain1\nfname_nas &lt;- paste0(setwd(here::here()),\n\"/wout/wrfout_d01_2004-08-11_00_00_00\")\nnc_data_nas &lt;- nc_open(fname_nas)\n\n{sink(paste0(fname_nas,\".txt\"))\n  print(nc_data_nas)\n  sink()}\n\nlong_nas&lt;- ncvar_get(nc_data_nas, \"XLONG\")\nlat_nas&lt;- ncvar_get(nc_data_nas, \"XLAT\", verbose = F)\ntemp_nas&lt;- ncvar_get(nc_data_nas, \"T2\") \nt_nas &lt;- ncvar_get(nc_data_nas, \"Times\")\n\nraster_temp_nas&lt;- list()\nfor (i in 1:dim(temp_nas)[3])   {\n    raster_temp_nas[[i]] &lt;- raster(t(temp_nas[, , i] - 273.15), \n       xmn=min(long_nas), xmx=max(long_nas),\n       ymn=min(lat_nas), ymx=max(lat_nas), \n       crs=CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0\"))\n}\n\n#domain2\nfname2_nas &lt;- paste0(setwd(here::here()),\n\"/wout/wrfout_d02_2004-08-11_00_00_00\")\nnc_data2_nas &lt;- nc_open(fname2_nas)\n\n{sink(paste0(fname2_nas,\".txt\"))\n  print(nc_data2_nas)\n  sink()}\n\nlong_2_nas&lt;- ncvar_get(nc_data2_nas, \"XLONG\")\nlat_2_nas&lt;- ncvar_get(nc_data2_nas, \"XLAT\", verbose = F)\ntemp_2_nas&lt;- ncvar_get(nc_data2_nas, \"T2\") \nt2_nas &lt;- ncvar_get(nc_data2_nas, \"Times\")\n\nraster_temp_2_nas&lt;- list()\nfor (i in 1:dim(temp_2_nas)[3])   {\n    raster_temp_2_nas[[i]] &lt;- raster(t(temp_2_nas[, , i] - 273.15), \n       xmn=min(long_2_nas), xmx=max(long_2_nas),\n       ymn=min(lat_2_nas), ymx=max(lat_2_nas), \n       crs=CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0\"))\n}\n\n# domain1\nraster_temp_stack_nas&lt;- stack(raster_temp_nas)\nraster_temp_value_nas&lt;- raster::extract(raster_temp_stack_nas, centroids)\n\nrt_cpv_nas &lt;- cbind(centroids,raster_temp_value_nas)\nrt_cpv_df_nas&lt;- data.frame(rt_cpv_nas)\nrt_cpv_df_nas&lt;- rt_cpv_df_nas[,-ncol(rt_cpv_df_nas)]\n\nrt_cpv_df_nas&lt;- \n  rt_cpv_df_nas |&gt; \n  dplyr::select(Station, Longitude, Latitude,  everything() )\ncolnames(rt_cpv_df_nas) &lt;- append(colnames(rt_cpv_df_nas[1:3]),as.character(t_nas))\n\n# domain2\nraster_temp_stack_2_nas&lt;- stack(raster_temp_2_nas)\nraster_temp_value_2_nas&lt;- raster::extract(raster_temp_stack_2_nas, centroids)\n\nrt_cpv_2_nas &lt;- cbind(centroids,raster_temp_value_2_nas)\nrt_cpv_df_2_nas&lt;- data.frame(rt_cpv_2_nas) \nrt_cpv_df_2_nas&lt;- rt_cpv_df_2_nas[,-ncol(rt_cpv_df_2_nas)]\n\nrt_cpv_df_2_nas&lt;- \n  rt_cpv_df_2_nas |&gt; \n  dplyr::select(Station, Longitude, Latitude,  everything() )\ncolnames(rt_cpv_df_2_nas)&lt;- append(colnames(rt_cpv_df_2_nas[1:3]),as.character(t2_nas))\n\nhead(rt_cpv_df_2_nas)[,1:5]\n\n  Station Longitude Latitude 2004-08-11_00:00:00 2004-08-11_01:00:00\n1   17020  32.35690 41.62480            16.26486            12.82797\n2   17022  31.77792 41.44924            16.93813            14.35607\n3   17026  35.15450 42.02990            17.94399            17.90621\n4   17069  30.39340 40.76760            20.76284            23.27804\n5   17070  31.60220 40.73290            20.05142            22.38864\n6   17072  31.14880 40.84370            20.31973            22.20229"
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#numerical-weather-prediction-nwp-models",
    "href": "Maksoy_Sakil/WRF.html#numerical-weather-prediction-nwp-models",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "1.1 Numerical Weather Prediction (NWP) Models",
    "text": "1.1 Numerical Weather Prediction (NWP) Models\nNumerical Weather Prediction (NWP) models are used to predict weather conditions by simulating the atmosphere, oceans, land surface and their interactions. NWP models are based on mathematical equations representing the physical behavior of the atmosphere. These equations are translated into computer code and use governing equations, numerical methods, parameterizations of other physical processes and combined with initial and boundary conditions before running over a geographic area (we call that geographic area as domain). These physical processes need to be approximated in models because of huge simulation computer time which is called as parameterization.\nAccording to ECMWF report, these processes affect too a small area to be predicted in full detail by NWP models. The major reason lies in the limited computing power that still does not allow us to calculate the all processes for any place on Earth (Frnda et al. 2022)."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#data-assimilation",
    "href": "Maksoy_Sakil/WRF.html#data-assimilation",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "1.2 Data Assimilation",
    "text": "1.2 Data Assimilation\nNWP models start with an initial value and it is the challenge because initial-value effects next predictions with an increasing error. Thus, updating initial-value prediction is a method to improve prediction accuracy which is called as data assimilation. Infrared radiance from the geostationary satellites and on-site radiosonde observation data assimilation are powerful tools to improve the weather forecast have been widely applied with this purpose in the past a few decades (Geer et al. 2018).\nThere are a number of data assimilation techniques used in weather forecasting. One of the most prominent are the three and four dimensional variational data assimilation methods (3D-var and 4D-var). 3D-var incorporates meteorological data only within a time window around the initialization moment and in this method the analysis increment (an increment is introduced due to the actual observations) does not evolve in time, e.g. it has effect only at the beginning of the simulation. On the other hand 4D-var method uses tangent linear and adjoint models which model the propagation of analysis increment and more computing time is needed (Vladimirov, Dimitrova, and Danchovski 2020)."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#weather-research-and-forecasting-wrf-model",
    "href": "Maksoy_Sakil/WRF.html#weather-research-and-forecasting-wrf-model",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "1.3 Weather Research and Forecasting (WRF) model",
    "text": "1.3 Weather Research and Forecasting (WRF) model\nThe Weather Research and Forecasting (WRF) model is a next-generation open source mesoscale numerical weather prediction system. As it is open source, it has a very flexible structure that allows it to be used by met-offices, universities and atmospheric research centers (Powers et al. 2017). The effort to develop WRF began in the latter 1990’s and was a collaborative partnership of the National Center for Atmospheric Research (NCAR), the National Oceanic and Atmospheric Administration (NOAA), the U.S. Air Force, the Naval Research Laboratory, the University of Oklahoma, and the Federal Aviation Administration (WRF web page).\nThe WRF model is used for operational weather forecasting and research purposes at Turkish State Meteorological Service since last two decades, as it is at many meteorological services around the world. It is the fact that data assimilation of observations into WRF model method is very popular not only in Türkiye but also in all over world since it has a great potential to improve model forecast skill by reducing errors of initial conditions (Yucel and Onen 2014; Yucel et al. 2015; Bao et al. 2015; Cheng et al. 2017)."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#uploading-necessary-packages",
    "href": "Maksoy_Sakil/WRF.html#uploading-necessary-packages",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "2.1 Uploading Necessary Packages",
    "text": "2.1 Uploading Necessary Packages\nWe need several packages for some implementations in R, for instance; opening of the netcdf files of WRF data, handling of WRF outputs, visualization and similar operations (Wickham and Bryan 2023; Wickham et al. 2019; Wickham 2016; Kassambara 2023; Hijmans 2023; Massicotte and South 2023; Pierce 2023; Bengtsson 2023; Pebesma and Bivand 2023; Iannone et al. 2023).\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(raster)\nlibrary(rnaturalearth)\nlibrary(ncdf4)\nlibrary(R.utils)\nlibrary(sf)\nlibrary(gt)\nlibrary(ggpubr)\nlibrary(rnaturalearthdata)"
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#temperature-forecast-of-wrf-in-domain-1",
    "href": "Maksoy_Sakil/WRF.html#temperature-forecast-of-wrf-in-domain-1",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "2.2 Temperature Forecast of WRF in Domain 1",
    "text": "2.2 Temperature Forecast of WRF in Domain 1\nThe code in the below reads a NetCDF file (fname) using the nc_open function from the ncdf4 package. NetCDF is a file format commonly used for storing multidimensional scientific data including NWP model output.\nFirstly, we opened the assimilated WRF output for domain 1. By using sink function the content of WRF output at the attachments (wrfout_d01_2004-08-11_00_00_00.txt) can be created. The data cover time and coverage domain information with meteorological predictions such as potential temperature (T), temperature at 2 meter (T2), wind speed at ten meter (U10 & V10), precipitation (RAINC & RAINNC) and etc. We need to use name of the variables to extract specific data variables from the raw data.\n\nfname &lt;- paste0(setwd(here::here()),\n\"/aswout/wrfout_d01_2004-08-11_00_00_00\")\nnc_data &lt;- nc_open(fname)\n\n{sink(paste0(fname,\".txt\"))\n  print(nc_data)\n  sink()}\n\nIn the WRF data, air temperature at 2 meter (the height of observation at gauges) is available. If 2 meter temperature is not available in netcdf file, the calculation of Air Temperature Prediction from WRF data is described at the given link based on WRF manual. In such a case, perturbation potential temperature, base pressure and perturbation pressure variables has to be extracted to get air temperature prediction.\nfloat T2[west_east,south_north,Time]   \n    FieldType: 104\n    MemoryOrder: XY \n    description: TEMP at 2 M\n    units: K\n    stagger: \n    coordinates: XLONG XLAT\n\n2.2.1 Spatial Resolution (Domain 1)\nSpatial resolution of domain 1 is 12 km as you can see below (DX: 12000, DY:12000).\n78 global attributes:\n    TITLE:  OUTPUT FROM WRF V3.1.1 MODEL\n    START_DATE: 2004-08-11_00:00:00\n    SIMULATION_START_DATE: 2004-08-11_00:00:00\n    WEST-EAST_GRID_DIMENSION: 192\n    SOUTH-NORTH_GRID_DIMENSION: 116\n    BOTTOM-TOP_GRID_DIMENSION: 28\n    DX: 12000\n    DY: 12000\nHere, the code extracts specific variables (longitude, latitude, temperature, and time) from the NetCDF file using the ncvar_getfunction. There are 41 time steps which means we can get predictions along the domain (191x115) for whole period. Additionally, the data includes information for 27 layers from bottom to top of the atmosphere. Fortunately, we do not have to deal with upper layers’ data since we just need to extract temperature predictions at 2 meters.\n\nlong&lt;- ncvar_get(nc_data, \"XLONG\")\nlat&lt;- ncvar_get(nc_data, \"XLAT\", verbose = F)\n\ntemp&lt;- ncvar_get(nc_data, \"T2\") \ndim(temp)\n\n[1] 191 115  41\n\ndim(ncvar_get(nc_data, \"T\"))\n\n[1] 191 115  27  41\n\n\n\n\n2.2.2 Forecast Period & Time Interval (Domain 1)\nWe can also obtained the forecast horizon by getting time steps from the data. After getting time steps, we see that forecast period is between 11 and 16 (00:00 UTC) August, 2004. In this case, the time interval for the forecast period up to +120 hours (or five days) is three hour.\n\nt &lt;- ncvar_get(nc_data, \"Times\"); t\n\n [1] \"2004-08-11_00:00:00\" \"2004-08-11_03:00:00\" \"2004-08-11_06:00:00\"\n [4] \"2004-08-11_09:00:00\" \"2004-08-11_12:00:00\" \"2004-08-11_15:00:00\"\n [7] \"2004-08-11_18:00:00\" \"2004-08-11_21:00:00\" \"2004-08-12_00:00:00\"\n[10] \"2004-08-12_03:00:00\" \"2004-08-12_06:00:00\" \"2004-08-12_09:00:00\"\n[13] \"2004-08-12_12:00:00\" \"2004-08-12_15:00:00\" \"2004-08-12_18:00:00\"\n[16] \"2004-08-12_21:00:00\" \"2004-08-13_00:00:00\" \"2004-08-13_03:00:00\"\n[19] \"2004-08-13_06:00:00\" \"2004-08-13_09:00:00\" \"2004-08-13_12:00:00\"\n[22] \"2004-08-13_15:00:00\" \"2004-08-13_18:00:00\" \"2004-08-13_21:00:00\"\n[25] \"2004-08-14_00:00:00\" \"2004-08-14_03:00:00\" \"2004-08-14_06:00:00\"\n[28] \"2004-08-14_09:00:00\" \"2004-08-14_12:00:00\" \"2004-08-14_15:00:00\"\n[31] \"2004-08-14_18:00:00\" \"2004-08-14_21:00:00\" \"2004-08-15_00:00:00\"\n[34] \"2004-08-15_03:00:00\" \"2004-08-15_06:00:00\" \"2004-08-15_09:00:00\"\n[37] \"2004-08-15_12:00:00\" \"2004-08-15_15:00:00\" \"2004-08-15_18:00:00\"\n[40] \"2004-08-15_21:00:00\" \"2004-08-16_00:00:00\"\n\nymd_hms(t[41]) - ymd_hms(t[1])\n\nTime difference of 5 days\n\n\n\n\n2.2.3 Study Area (Domain 1)\nFigure 1 below shows the coverage of domain 1 where covers Türkiye and its surrounding. The code in the below creates a list of raster objects for each time step from the extracted temperature from raw WRF data set. Kelvin unit has been converted into Celcius by implementing -273.15. Raster objects are used for working with gridded spatial data. Additionally, ggplot2 package is used to create a map visualization. It overlays the temperature data onto a map of countries, setting up appropriate coordinate systems and color scales.\n\nraster_temp&lt;- list()\nfor (i in 1:dim(temp)[3])   {\n    raster_temp[[i]] &lt;- raster(t(temp[, , i] - 273.15), \n       xmn=min(long), xmx=max(long),\n       ymn=min(lat), ymx=max(lat), \n       crs=CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0\"))\n}\n\ntemp_df &lt;-as.data.frame(raster_temp[[length(t)]], xy = TRUE) \nworld &lt;- rnaturalearth::ne_countries(scale='medium',returnclass = 'sf')\n\nggplot(data = world)  + geom_sf(fill = \"white\") +\n  coord_sf(crs = st_crs(4326), xlim = c(10, 55), ylim = c(30,50)) +\n  geom_raster(data = temp_df, aes(x, y, fill = layer), alpha=0.6) +\n  scale_fill_viridis_c(limits = c(0, 35)) +\n  labs(x=\"\",y=\"\", fill= expression(degree*C)) + \n  ggtitle(\"Coverage of Domain 1\") + theme(legend.key.height = unit(1, \"cm\"))\n\n\n\n\n\n\n\nFigure 1: Türkiye with its neighboring countries and coverage of domain 1 for assimilated-WRF predictions."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#temperature-forecast-of-wrf-in-domain-2",
    "href": "Maksoy_Sakil/WRF.html#temperature-forecast-of-wrf-in-domain-2",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "2.3 Temperature Forecast of WRF in Domain 2",
    "text": "2.3 Temperature Forecast of WRF in Domain 2\nThere are 121 time steps for second domain since we can inference by dimension of T2 data.\n\nfname2 &lt;- paste0(setwd(here::here()),\n\"/aswout/wrfout_d02_2004-08-11_00_00_00\")\nnc_data2 &lt;- nc_open(fname2)\n\n{sink(paste0(fname2,\".txt\"))\n  print(nc_data2)\n  sink()}\n\nlong_2&lt;- ncvar_get(nc_data2, \"XLONG\")\nlat_2&lt;- ncvar_get(nc_data2, \"XLAT\", verbose = F)\ntemp_2&lt;- ncvar_get(nc_data2, \"T2\") \n\ndim(temp_2)\n\n[1] 132  63 121\n\n\n\n2.3.1 Spatial Resolution (Domain 2)\nSpatial resolution of domain 2 is 4 km which is a finer resolution than previous one (DX: 4000, DY: 4000).\n78 global attributes:\n    TITLE:  OUTPUT FROM WRF V3.1.1 MODEL\n    START_DATE: 2004-08-11_00:00:00\n    SIMULATION_START_DATE: 2004-08-11_00:00:00\n    WEST-EAST_GRID_DIMENSION: 133\n    SOUTH-NORTH_GRID_DIMENSION: 64\n    BOTTOM-TOP_GRID_DIMENSION: 28\n    DX: 4000\n    DY: 4000\n\n\n2.3.2 Forecast Period & Time Interval (Domain 2)\nForecast period for domain 2 is same with previous one. However, the time interval is one hour and it has a finer temporal resolution.\n\nt2 &lt;- ncvar_get(nc_data2, \"Times\"); t2\n\n  [1] \"2004-08-11_00:00:00\" \"2004-08-11_01:00:00\" \"2004-08-11_02:00:00\"\n  [4] \"2004-08-11_03:00:00\" \"2004-08-11_04:00:00\" \"2004-08-11_05:00:00\"\n  [7] \"2004-08-11_06:00:00\" \"2004-08-11_07:00:00\" \"2004-08-11_08:00:00\"\n [10] \"2004-08-11_09:00:00\" \"2004-08-11_10:00:00\" \"2004-08-11_11:00:00\"\n [13] \"2004-08-11_12:00:00\" \"2004-08-11_13:00:00\" \"2004-08-11_14:00:00\"\n [16] \"2004-08-11_15:00:00\" \"2004-08-11_16:00:00\" \"2004-08-11_17:00:00\"\n [19] \"2004-08-11_18:00:00\" \"2004-08-11_19:00:00\" \"2004-08-11_20:00:00\"\n [22] \"2004-08-11_21:00:00\" \"2004-08-11_22:00:00\" \"2004-08-11_23:00:00\"\n [25] \"2004-08-12_00:00:00\" \"2004-08-12_01:00:00\" \"2004-08-12_02:00:00\"\n [28] \"2004-08-12_03:00:00\" \"2004-08-12_04:00:00\" \"2004-08-12_05:00:00\"\n [31] \"2004-08-12_06:00:00\" \"2004-08-12_07:00:00\" \"2004-08-12_08:00:00\"\n [34] \"2004-08-12_09:00:00\" \"2004-08-12_10:00:00\" \"2004-08-12_11:00:00\"\n [37] \"2004-08-12_12:00:00\" \"2004-08-12_13:00:00\" \"2004-08-12_14:00:00\"\n [40] \"2004-08-12_15:00:00\" \"2004-08-12_16:00:00\" \"2004-08-12_17:00:00\"\n [43] \"2004-08-12_18:00:00\" \"2004-08-12_19:00:00\" \"2004-08-12_20:00:00\"\n [46] \"2004-08-12_21:00:00\" \"2004-08-12_22:00:00\" \"2004-08-12_23:00:00\"\n [49] \"2004-08-13_00:00:00\" \"2004-08-13_01:00:00\" \"2004-08-13_02:00:00\"\n [52] \"2004-08-13_03:00:00\" \"2004-08-13_04:00:00\" \"2004-08-13_05:00:00\"\n [55] \"2004-08-13_06:00:00\" \"2004-08-13_07:00:00\" \"2004-08-13_08:00:00\"\n [58] \"2004-08-13_09:00:00\" \"2004-08-13_10:00:00\" \"2004-08-13_11:00:00\"\n [61] \"2004-08-13_12:00:00\" \"2004-08-13_13:00:00\" \"2004-08-13_14:00:00\"\n [64] \"2004-08-13_15:00:00\" \"2004-08-13_16:00:00\" \"2004-08-13_17:00:00\"\n [67] \"2004-08-13_18:00:00\" \"2004-08-13_19:00:00\" \"2004-08-13_20:00:00\"\n [70] \"2004-08-13_21:00:00\" \"2004-08-13_22:00:00\" \"2004-08-13_23:00:00\"\n [73] \"2004-08-14_00:00:00\" \"2004-08-14_01:00:00\" \"2004-08-14_02:00:00\"\n [76] \"2004-08-14_03:00:00\" \"2004-08-14_04:00:00\" \"2004-08-14_05:00:00\"\n [79] \"2004-08-14_06:00:00\" \"2004-08-14_07:00:00\" \"2004-08-14_08:00:00\"\n [82] \"2004-08-14_09:00:00\" \"2004-08-14_10:00:00\" \"2004-08-14_11:00:00\"\n [85] \"2004-08-14_12:00:00\" \"2004-08-14_13:00:00\" \"2004-08-14_14:00:00\"\n [88] \"2004-08-14_15:00:00\" \"2004-08-14_16:00:00\" \"2004-08-14_17:00:00\"\n [91] \"2004-08-14_18:00:00\" \"2004-08-14_19:00:00\" \"2004-08-14_20:00:00\"\n [94] \"2004-08-14_21:00:00\" \"2004-08-14_22:00:00\" \"2004-08-14_23:00:00\"\n [97] \"2004-08-15_00:00:00\" \"2004-08-15_01:00:00\" \"2004-08-15_02:00:00\"\n[100] \"2004-08-15_03:00:00\" \"2004-08-15_04:00:00\" \"2004-08-15_05:00:00\"\n[103] \"2004-08-15_06:00:00\" \"2004-08-15_07:00:00\" \"2004-08-15_08:00:00\"\n[106] \"2004-08-15_09:00:00\" \"2004-08-15_10:00:00\" \"2004-08-15_11:00:00\"\n[109] \"2004-08-15_12:00:00\" \"2004-08-15_13:00:00\" \"2004-08-15_14:00:00\"\n[112] \"2004-08-15_15:00:00\" \"2004-08-15_16:00:00\" \"2004-08-15_17:00:00\"\n[115] \"2004-08-15_18:00:00\" \"2004-08-15_19:00:00\" \"2004-08-15_20:00:00\"\n[118] \"2004-08-15_21:00:00\" \"2004-08-15_22:00:00\" \"2004-08-15_23:00:00\"\n[121] \"2004-08-16_00:00:00\"\n\nymd_hms(t2[121]) - ymd_hms(t2[1])\n\nTime difference of 5 days\n\n\n\n\n2.3.3 Study Area (Domain 2)\nFigure 2 below shows the comparison of two domains and coverage of domain 2 where covers some part of northwest of Türkiye. It is clearly seen that the intersection of domain 1 and 2 is the entire domain 2. Thus, our study area become only the entire domain 2.\n\nraster_temp_2&lt;- list()\nfor (i in 1:dim(temp_2)[3])   {\n  raster_temp_2[[i]] &lt;- raster(t(temp_2[, , i] - 273.15), \n     xmn=min(long_2), xmx=max(long_2),\n     ymn=min(lat_2), ymx=max(lat_2), \n     crs=CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0\"))\n                              }\n\ntemp_df_2 &lt;-as.data.frame(raster_temp_2[[length(t2)]], xy = TRUE) \nworld &lt;- rnaturalearth::ne_countries(scale='medium',returnclass = 'sf')\n\nggplot(data = world)  + geom_sf(fill = \"white\") + \n  coord_sf(crs = st_crs(4326), xlim = c(19, 47.5), ylim = c(33.5,47)) +\n  geom_raster(data = temp_df, \n            aes(x, y, fill= layer), alpha=0.3, show.legend = FALSE) + \n  geom_raster(data = temp_df_2, \n            aes(x, y, fill= layer), alpha=0.7, show.legend = FALSE) + \n  scale_fill_viridis_c() + labs(x=\"\",y=\"\") + \n  ggtitle(\"Coverage of Domain 2\") \n\n\n\n\n\n\n\nFigure 2: Comparison of domain 1 and 2."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#derivation-of-temperature-prediction-observation",
    "href": "Maksoy_Sakil/WRF.html#derivation-of-temperature-prediction-observation",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "2.4 Derivation of Temperature Prediction & Observation",
    "text": "2.4 Derivation of Temperature Prediction & Observation\n\n2.4.1 Identification of Meteorological Stations\nDomain 2 covers several provinces which are located northwest of Türkiye. Thus, we need to determine meteorological stations for comparing observation versus assimilated and non-assimilated WRF predictions. The code in the chunk reads data from a delimited text file containing information about meteorological stations. Each specific station was selected for each province to evaluate the performance of both assimilated and non-assimilated WRF predictions. Table 1 shows the main gauges across the domain 2.\nThe code uses the dplyr and gt packages for data manipulation and table creation. It assist to perform data wrangling and cleaning on the meteorological station data, containing renaming columns, converting province names, and arranging the data. The tolower function is used to convert the province names to lowercase. The str_to_title() function from the stringr package is applied to convert the province names’ first letter to title case.\n\ndf_gauges &lt;- read.delim(paste0(setwd(here::here()),\"/gauges.txt\"), sep=\"|\")\n\ndf_gauges&lt;- df_gauges[,-c(3,4)]\ncolnames(df_gauges)&lt;- c(\"Station\",\"Province\",\"Latitude\",\"Longitude\",\"Altitude\") \n\ndf_gauges$Province &lt;- tolower(df_gauges$Province) |&gt; str_to_title() \ndf_gauges&lt;- df_gauges |&gt; arrange(Station) \ndf_gauges |&gt;  gt()\n\n\n\nTable 1: Meteorological stations accross the domain 2.\n\n\n\n\n\n\n\n\n\nStation\nProvince\nLatitude\nLongitude\nAltitude\n\n\n\n\n17020\nBartin\n41.62480\n32.35690\n33\n\n\n17022\nZonguldak\n41.44924\n31.77792\n135\n\n\n17026\nSinop\n42.02990\n35.15450\n32\n\n\n17069\nSakarya\n40.76760\n30.39340\n30\n\n\n17070\nBolu\n40.73290\n31.60220\n743\n\n\n17072\nDuzce\n40.84370\n31.14880\n146\n\n\n17074\nKastamonu\n41.37100\n33.77560\n800\n\n\n17080\nCankiri\n40.60820\n33.61020\n755\n\n\n17084\nCorum\n40.54610\n34.93620\n776\n\n\n17085\nAmasya\n40.66680\n35.83530\n409\n\n\n17622\nSamsun\n41.55150\n35.92470\n103\n\n\n\n\n\n\n\n\n\n\nFigure 3 is shown for distribution of meteorological stations across the study area.\n\nextents&lt;- extent(raster_temp_2[[length(t2)]])\n\nggplot(data = world)  + geom_sf(fill = \"white\") + \n  coord_sf(crs = st_crs(4326), xlim = c(extents[1], extents[2]), \n                               ylim = c(extents[3],extents[4])) +\n  geom_raster(data = temp_df_2, \n        aes(x, y, fill = layer), alpha=0.6) +\n  scale_fill_viridis_c(limits = c(10, 25)) + \n  labs(x=\"\",y=\"\", fill= expression(degree*C)) +\n  geom_point(data = df_gauges, aes(x=Longitude, y=Latitude),\n             size=3, colour=\"darkred\") + \n  geom_text(data= df_gauges, mapping = aes(x=Longitude, y=Latitude, \n                                           label=Province), nudge_y = -0.1) + \n  ggtitle(paste(\"Hourly Assimilated WRF Temperature Forecast,\", t2[121])) +\n  theme(legend.key.width=unit(3,\"cm\"), legend.position=\"bottom\")\n\n\n\n\n\n\n\nFigure 3: Distribution of Meteorological Stations Over Domain 2.\n\n\n\n\n\n\n\n2.4.2 Obtain Temperature Observations\nThe code reads temperature observations from an excel file. In the raw data, there is no date column but it has multiple columns which are defined for year, month, day and hour information. Therefore, we need to convert them into the single date column by merging them. Then, these columns can be removed by non-selecting. In the raw data set, some dates can be missing. However, these are not defined as null. Therefore, leaping values can be detected then it assigned as a null by using complete function.\n\ntemp_obs&lt;- read_excel(paste0(setwd(here::here()),\n\"/observation.xlsx\"))\nhead(temp_obs)\n\n# A tibble: 6 × 7\n  Istasyon_No Istasyon_Adi   YIL    AY   GUN  SAAT SICAKLIK\n        &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1       17020 BARTIN        2004     8    10     0     18.7\n2       17020 BARTIN        2004     8    10     1     18.3\n3       17020 BARTIN        2004     8    10     2     18  \n4       17020 BARTIN        2004     8    10     3     17.1\n5       17020 BARTIN        2004     8    10     4     17.6\n6       17020 BARTIN        2004     8    10     5     18.3\n\ntemp_obs&lt;-\n  temp_obs |&gt;\n  mutate(date= as.Date(with(temp_obs, paste(YIL,AY,GUN,sep=\"-\")),\"%Y-%m-%d\")) |&gt;\n  mutate(dates= ymd_hms(paste(date, paste(SAAT, 0, 0, sep = \":\")), tz=\"UTC\")) |&gt;\n  dplyr::select(Istasyon_No, dates, SICAKLIK) |&gt; \n  group_by(Istasyon_No) |&gt; \n  tidyr::complete( dates = seq(ymd_hm(\"2004-08-10 00:00\"), \n                               ymd_hm(\"2004-08-16 23:00\"), by = \"1 hours\"))\n\ncolnames(temp_obs)&lt;- c(\"Station\",\"dates\",\"observation\")\nhead(temp_obs)\n\n# A tibble: 6 × 3\n# Groups:   Station [1]\n  Station dates               observation\n    &lt;dbl&gt; &lt;dttm&gt;                    &lt;dbl&gt;\n1   17020 2004-08-10 00:00:00        18.7\n2   17020 2004-08-10 01:00:00        18.3\n3   17020 2004-08-10 02:00:00        18  \n4   17020 2004-08-10 03:00:00        17.1\n5   17020 2004-08-10 04:00:00        17.6\n6   17020 2004-08-10 05:00:00        18.3\n\n\n\n\n2.4.3 Extraction of Temperature Predictions from WRF\nThis code stacks raster layers (since time is not constant) and extracts temperature values for meteorological station locations. Gauge locations and prediction values with time need to be combined and data frame columns need to be renamed after extraction procedure. The table below contains three-hour temperature predictions for each province/gauge in domain 1.\n\ncentroids &lt;- df_gauges[,c(1,3,4)]\ncoordinates(centroids)= ~ Longitude + Latitude\n\n# domain1\nraster_temp_stack&lt;- stack(raster_temp)\nraster_temp_value&lt;- raster::extract(raster_temp_stack, centroids)\n\nrt_cpv &lt;- cbind(centroids,raster_temp_value)\nrt_cpv_df&lt;- data.frame(rt_cpv)\ncolnames(rt_cpv_df)\n\n [1] \"Station\"   \"layer.1\"   \"layer.2\"   \"layer.3\"   \"layer.4\"   \"layer.5\"  \n [7] \"layer.6\"   \"layer.7\"   \"layer.8\"   \"layer.9\"   \"layer.10\"  \"layer.11\" \n[13] \"layer.12\"  \"layer.13\"  \"layer.14\"  \"layer.15\"  \"layer.16\"  \"layer.17\" \n[19] \"layer.18\"  \"layer.19\"  \"layer.20\"  \"layer.21\"  \"layer.22\"  \"layer.23\" \n[25] \"layer.24\"  \"layer.25\"  \"layer.26\"  \"layer.27\"  \"layer.28\"  \"layer.29\" \n[31] \"layer.30\"  \"layer.31\"  \"layer.32\"  \"layer.33\"  \"layer.34\"  \"layer.35\" \n[37] \"layer.36\"  \"layer.37\"  \"layer.38\"  \"layer.39\"  \"layer.40\"  \"layer.41\" \n[43] \"Longitude\" \"Latitude\"  \"optional\" \n\nrt_cpv_df&lt;- rt_cpv_df[,-ncol(rt_cpv_df)]\nhead(rt_cpv_df)[,1:5]\n\n  Station  layer.1  layer.2  layer.3  layer.4\n1   17020 19.14172 16.38781 18.63476 22.33074\n2   17022 18.32778 17.36398 20.07202 23.92535\n3   17026 20.14706 18.49960 20.52038 19.43371\n4   17069 17.88046 15.75091 16.23025 19.67379\n5   17070 16.78866 13.52252 16.43801 19.22317\n6   17072 17.22476 14.45699 16.89929 19.99230\n\nrt_cpv_df&lt;- \n  rt_cpv_df |&gt; \n  dplyr::select(Station, Longitude, Latitude,  everything() )\ncolnames(rt_cpv_df) &lt;- append(colnames(rt_cpv_df[1:3]),as.character(t))\nhead(rt_cpv_df)[,1:5]\n\n  Station Longitude Latitude 2004-08-11_00:00:00 2004-08-11_03:00:00\n1   17020  32.35690 41.62480            19.14172            16.38781\n2   17022  31.77792 41.44924            18.32778            17.36398\n3   17026  35.15450 42.02990            20.14706            18.49960\n4   17069  30.39340 40.76760            17.88046            15.75091\n5   17070  31.60220 40.73290            16.78866            13.52252\n6   17072  31.14880 40.84370            17.22476            14.45699\n\n\nSame procedures applied in previous chunk has to be followed for assimilated data but for domain 2.\n\n# domain2\nraster_temp_stack_2&lt;- stack(raster_temp_2)\nraster_temp_value_2&lt;- raster::extract(raster_temp_stack_2, centroids)\n\nrt_cpv_2 &lt;- cbind(centroids,raster_temp_value_2)\nrt_cpv_df_2&lt;- data.frame(rt_cpv_2) \nrt_cpv_df_2&lt;- rt_cpv_df_2[,-ncol(rt_cpv_df_2)]\n\nrt_cpv_df_2&lt;- \n  rt_cpv_df_2 |&gt; \n  dplyr::select(Station, Longitude, Latitude,  everything() )\ncolnames(rt_cpv_df_2)&lt;- append(colnames(rt_cpv_df_2[1:3]),as.character(t2))\nhead(rt_cpv_df_2)[,1:5]\n\n  Station Longitude Latitude 2004-08-11_00:00:00 2004-08-11_01:00:00\n1   17020  32.35690 41.62480            16.26486            12.88519\n2   17022  31.77792 41.44924            16.93813            14.32098\n3   17026  35.15450 42.02990            17.94399            18.06762\n4   17069  30.39340 40.76760            20.76284            23.30419\n5   17070  31.60220 40.73290            20.05142            22.39242\n6   17072  31.14880 40.84370            20.31973            22.24160"
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#derivation-of-data-frames",
    "href": "Maksoy_Sakil/WRF.html#derivation-of-data-frames",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "4.1 Derivation of Data Frames",
    "text": "4.1 Derivation of Data Frames\nThere are four data frames, including predictions and observations, to represent each domain and assimilation version. However, these data frames are wider format and it is needed to convert them longer to use them in visualization.\n\n# domain1 assimilated prediction: rt_cpv_df\n# domain2 assimilated prediction: rt_cpv_df_2\n\n# domain1 non_assimilated prediction: rt_cpv_df_nas\n# domain2 non_assimilated prediction: rt_cpv_df_2_nas\n\n# observations: temp_obs\n\ndata_list&lt;- list(rt_cpv_df, rt_cpv_df_2, rt_cpv_df_nas, rt_cpv_df_2_nas)\nnew_df_list&lt;- list()\nvariable&lt;- c(\"predict_do1\", \"predict_do2\",\"predict_do1_nas\",\"predict_do2_nas\")\n  \nfor(i in 1:length(variable)){\n  new_df_list[[i]] &lt;- \n        data_list[[i]] |&gt; \n        distinct(Station, .keep_all = TRUE) |&gt; \n        pivot_longer(\n          cols = starts_with(\"2004\"),\n          names_to = \"dates\",\n          values_to = variable[i],\n          values_drop_na = FALSE\n        ) |&gt; \n        dplyr:: select(Station, dates, variable[i]) # to remove lat long column\n      \n  new_df_list[[i]]$dates&lt;- str_replace(new_df_list[[i]]$dates, \"_\",\" \")\n  new_df_list[[i]]$dates&lt;- ymd_hms(new_df_list[[i]]$dates)\n\n                            }\n# domain1: new_df_list[[1]]; new_df_list[[3]]\n# domain2: new_df_list[[2]]; head(new_df_list[[4]])\n\nfor(i in 1:length(variable)){\nnew_df_list[[i]] &lt;- \n      new_df_list[[i]] |&gt;        \n        left_join(temp_obs, by = c(\"Station\",\"dates\"))\n                            }\n  \nhead(new_df_list[[1]]); head(new_df_list[[3]])\n\n# A tibble: 6 × 4\n  Station dates               predict_do1 observation\n    &lt;dbl&gt; &lt;dttm&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00        19.1        19.1\n2   17020 2004-08-11 03:00:00        16.4        18.7\n3   17020 2004-08-11 06:00:00        18.6        19.9\n4   17020 2004-08-11 09:00:00        22.3        18.8\n5   17020 2004-08-11 12:00:00        24.5        19.2\n6   17020 2004-08-11 15:00:00        22.4        20.1\n\n\n# A tibble: 6 × 4\n  Station dates               predict_do1_nas observation\n    &lt;dbl&gt; &lt;dttm&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00            19.1        19.1\n2   17020 2004-08-11 03:00:00            16.3        18.7\n3   17020 2004-08-11 06:00:00            18.6        19.9\n4   17020 2004-08-11 09:00:00            22.3        18.8\n5   17020 2004-08-11 12:00:00            24.6        19.2\n6   17020 2004-08-11 15:00:00            23.1        20.1\n\nhead(new_df_list[[2]]); head(new_df_list[[4]])\n\n# A tibble: 6 × 4\n  Station dates               predict_do2 observation\n    &lt;dbl&gt; &lt;dttm&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00        16.3        19.1\n2   17020 2004-08-11 01:00:00        12.9        18.8\n3   17020 2004-08-11 02:00:00        12.5        18.8\n4   17020 2004-08-11 03:00:00        12.5        18.7\n5   17020 2004-08-11 04:00:00        12.2        18.8\n6   17020 2004-08-11 05:00:00        12.1        19.6\n\n\n# A tibble: 6 × 4\n  Station dates               predict_do2_nas observation\n    &lt;dbl&gt; &lt;dttm&gt;                        &lt;dbl&gt;       &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00            16.3        19.1\n2   17020 2004-08-11 01:00:00            12.8        18.8\n3   17020 2004-08-11 02:00:00            12.9        18.8\n4   17020 2004-08-11 03:00:00            12.4        18.7\n5   17020 2004-08-11 04:00:00            12.4        18.8\n6   17020 2004-08-11 05:00:00            12.1        19.6"
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#visualization",
    "href": "Maksoy_Sakil/WRF.html#visualization",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "4.2 Visualization",
    "text": "4.2 Visualization\nData frames were manipulated during visualization procedures depending on necessary conditions. For example, non-assimilated (new_df_list[[3]]) and assimilated (new_df_list[[1]]) predictions for domain 1 are combined with observations while drawing first plot below.\n\nhead(\n  new_df_list[[1]] |&gt;        \n    left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) )\n\n# A tibble: 6 × 5\n  Station dates               predict_do1 observation predict_do1_nas\n    &lt;dbl&gt; &lt;dttm&gt;                    &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00        19.1        19.1            19.1\n2   17020 2004-08-11 03:00:00        16.4        18.7            16.3\n3   17020 2004-08-11 06:00:00        18.6        19.9            18.6\n4   17020 2004-08-11 09:00:00        22.3        18.8            22.3\n5   17020 2004-08-11 12:00:00        24.5        19.2            24.6\n6   17020 2004-08-11 15:00:00        22.4        20.1            23.1\n\nhead(\n  new_df_list[[1]] |&gt;        \n    left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n    pivot_longer(\n            cols = -c(1:2),\n            names_to = \"Temperature\",\n            values_to = \"value\",\n            values_drop_na = FALSE) )  \n\n# A tibble: 6 × 4\n  Station dates               Temperature     value\n    &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;           &lt;dbl&gt;\n1   17020 2004-08-11 00:00:00 predict_do1      19.1\n2   17020 2004-08-11 00:00:00 observation      19.1\n3   17020 2004-08-11 00:00:00 predict_do1_nas  19.1\n4   17020 2004-08-11 03:00:00 predict_do1      16.4\n5   17020 2004-08-11 03:00:00 observation      18.7\n6   17020 2004-08-11 03:00:00 predict_do1_nas  16.3\n\n\nFigure 4 is shown for comparison of assimilated and non-assimilated predictions with observations for each gauge (province) by three hour intervals, in domain 1. In this plot, each box represent different provinces (gauges), black line shows observations, red and blue lines are for assimilated and non-assimilated predictions, respectively.\nThe assimilated and non-assimilated predictions are looks like very similar. Thus, it can be said that data assimilation of temperature prediction in domain 1 has not caused major differences. Moreover, predictions are compatible with observations for some gauges such as Bolu, Kastamonu, Çankırı and etc. However, predictions are not compatible with observations for other gauges such as Sinop, Sakarya, Düzce and Samsun even the fluctuations are similar for those gauges.\n\nnew_df_list[[1]] |&gt;        \n  left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n  pivot_longer(\n          cols = -c(1:2),\n          names_to = \"Temperature\",\n          values_to = \"value\",\n          values_drop_na = FALSE) |&gt; \n  mutate(Station = factor(Station, labels = df_gauges$Province )) |&gt;\n  mutate(Temperature =  factor(Temperature, \n         levels= c(\"observation\", \"predict_do1_nas\", \"predict_do1\")) ) |&gt;  \nggplot(aes(x= dates, y=value)) +   \n  geom_line(aes(colour = Temperature), size=0.7) +\n  scale_colour_manual(name= expression(\"Temperature\"~(degree*C)), \n          values = c('observation' = \"black\",\n                     'predict_do1_nas' = \"#23bfce\",\n                     'predict_do1' = \"#fc2852\"),\n          labels = c('observation' = 'Observation',\n                     'predict_do1_nas' = 'Non-Assimilated Prediction DO1',\n                     'predict_do1' = 'Assimilated Prediction DO1')) + \n  theme_bw() + facet_wrap(~Station, scales = \"free_y\") + \n  labs(x=\" \",y=expression(\"Temperature\"~(degree*C))) + \n  theme(axis.text.x = element_text(angle = 0, hjust = 1))  +\n  theme(legend.position = c(.88, .1), \n        strip.background = element_rect(colour=\"black\", fill=\"cornsilk\"))\n\n\n\n\n\n\n\nFigure 4: Comparison of predictions with observations for each province in domain 1.\n\n\n\n\n\nFigure 5 is shown for scatterplot and heatmap of 3-hour assimilated and non-assimilated predictions versus observations in domain 1. Red and blue colors represent assimilated and non-assimilated predictions, respectively, while black line shows fit of observations versus predictions. According to the below plots, there is a clear linear relationship for 3-hour assimilated and non-assimilated predictions with observations in domain 1.\n\nplot1&lt;- \n  new_df_list[[1]] |&gt;        \n    left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n      pivot_longer(\n        cols = -c(1,2,4),\n        names_to = \"Pred.Type\",\n        values_to = \"Prediction\",\n        values_drop_na = FALSE) |&gt;\nggplot(aes(x= observation, y=Prediction, color=Pred.Type)) +   \n  theme_bw() +\n  geom_point(size=2, alpha=0.5) +   \n  geom_smooth(color =\"black\", se=FALSE) +\n  scale_colour_manual(name= \" \", \n          values = c('predict_do1_nas' = \"blue\",\n                     'predict_do1' = \"#fc2852\"),\n          labels = c('predict_do1_nas' = 'Non-Assimilated Prediction DO1',\n                     'predict_do1' = 'Assimilated Prediction DO1')) +\n  labs(x=\"Observation\",y=\"Prediction\") + theme(legend.position = \"top\")\n\nplot2&lt;- \n  new_df_list[[1]] |&gt;\n    left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) |&gt;\n      pivot_longer(\n        cols = -c(1,2,4),\n        names_to = \"Pred.Type\",\n        values_to = \"Prediction\",\n        values_drop_na = FALSE) |&gt;\nggplot(aes(x= observation, y=Prediction, fill=Pred.Type)) + \n  geom_hex(alpha=0.5) + theme_bw() +\n  scale_fill_manual(name= \" \",\n      values = c('predict_do1_nas' = \"blue\",\n                  'predict_do1' = \"red\"),\n      labels = c('predict_do1_nas' = 'Non-Assimilated Prediction DO1',\n                  'predict_do1' = 'Assimilated Prediction DO1')) +\n  labs(x=\"Observation\",y=\"Prediction\") + theme(legend.position = \"top\")\n\nggarrange(plot1,plot2,ncol=2 ,nrow = 1)\n\n\n\n\n\n\n\nFigure 5: Scatterplot and heatmap of observations versus predictions in domain 1.\n\n\n\n\n\nFigure 6 is shown for comparison of hourly assimilated and non-assimilated predictions with observations for each gauge (province) in domain 2. In this plot, each box represent different provinces (gauges), black line shows observations, blue and red lines are for assimilated and non-assimilated predictions, respectively.\nThe assimilated and non-assimilated predictions are looks like very similar for also in domain 2. Hourly assimilated and non-assimilated predictions are compatible with observations for Bartın, Zonguldak, Kastamonu, Çorum and Samsun provinces but not remained provinces. It is the fact that 3-hour predictions are looking better than hourly ones even though domain 2 has finer resolution in both temporally and spatially. Actually, this result also should be expected since increased resolution may cause rise in error.\n\nnew_df_list[[2]] |&gt;        \n  left_join(new_df_list[[4]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n    pivot_longer(\n      cols = -c(1:2),\n      names_to = \"Temperature\",\n      values_to = \"value\",\n      values_drop_na = FALSE) |&gt; \n  mutate(Station = factor(Station, labels = df_gauges$Province )) |&gt;\n  mutate(Temperature =  factor(Temperature, \n         levels= c(\"observation\", \"predict_do2_nas\", \"predict_do2\")) ) |&gt;  \nggplot(aes(x= dates, y=value)) +   \n  geom_line(aes(colour = Temperature), size=0.7) +\n  scale_colour_manual(name= expression(\"Temperature\"~(degree*C)), \n          values = c('observation' = \"black\",\n                     'predict_do2_nas' = \"#fc2852\",\n                     'predict_do2' = \"#23bfce\"),\n          labels = c('observation' = 'Observation',\n                     'predict_do2_nas' = 'Non-Assimilated Prediction DO2',\n                     'predict_do2' = 'Assimilated Prediction DO2')) + \n  theme_bw() + facet_wrap(~Station, scales = \"free_y\") + \n  labs(x=\" \",y=expression(\"Temperature\"~(degree*C))) + \n  theme(axis.text.x = element_text(angle = 0, hjust = 1))  +\n  theme(legend.position = c(.88, .1), \n        strip.background = element_rect(colour=\"black\", fill=\"cornsilk\"))\n\n\n\n\n\n\n\nFigure 6: Comparison of predictions with observations for each province in domain 2.\n\n\n\n\n\nFigure 7 is shown for scatterplot and heatmap of 1-hour assimilated and non-assimilated predictions versus observations in domain 2. Blue and red colors represent assimilated and non-assimilated predictions, respectively, while black line shows fit of observations versus predictions.\nAccording to the below plots, there is not a strong relationship between hourly predictions and observations. We think that the reason of this issue is that predictions are so smooth for Sakarya, Bolu, Düzce and Amasya provinces. This situation causes two cluster on the scatterplot and fluctuated fitting line.\n\nplot3&lt;- \nnew_df_list[[2]] |&gt;        \n  left_join(new_df_list[[4]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n    pivot_longer(\n      cols = -c(1,2,4),\n      names_to = \"Pred.Type\",\n      values_to = \"Prediction\",\n      values_drop_na = FALSE) |&gt;\nggplot(aes(x= observation, y=Prediction, color=Pred.Type)) +   \n  theme_bw() +\n  geom_point(size=2, alpha=0.5) +   \n  geom_smooth(color =\"black\", se=FALSE) +\n  scale_colour_manual(name= \" \", \n    values = c('predict_do2_nas' = \"#fc2852\",\n               'predict_do2' = \"blue\"),\n    labels = c('predict_do2_nas' = 'Non-Assimilated Prediction DO2',\n               'predict_do2' = 'Assimilated Prediction DO2')) +\n  labs(x=\"Observation\",y=\"Prediction\") + theme(legend.position = \"top\")\n\nplot4&lt;- \nnew_df_list[[2]] |&gt;\n  left_join(new_df_list[[4]], by = c(\"Station\",\"dates\",\"observation\")) |&gt;\n    pivot_longer(\n       cols = -c(1,2,4),\n       names_to = \"Pred.Type\",\n       values_to = \"Prediction\",\n       values_drop_na = FALSE) |&gt;\nggplot(aes(x= observation, y=Prediction, fill=Pred.Type)) + \n  geom_hex(alpha=0.5) +     theme_bw() +\n  scale_fill_manual(name= \" \",\n           values = c('predict_do2_nas' = \"#fc2852\",\n                      'predict_do2' = \"blue\"),\n           labels = c('predict_do2_nas' = 'Non-Assimilated Prediction DO2',\n                      'predict_do2' = 'Assimilated Prediction DO2')) +\n  labs(x=\"Observation\",y=\"Prediction\") + theme(legend.position = \"top\")\n\nggarrange(plot3,plot4,ncol=2 ,nrow = 1)\n\n\n\n\n\n\n\nFigure 7: Scatterplot and heatmap of observations versus predictions in domain 2.\n\n\n\n\n\nFigure 8 is shown for comparison of three-hour assimilated and non-assimilated predictions with observations for each gauge (province) in both two domains. In this plot, each box represent different provinces (gauges), black line shows observations, pink and red lines are for assimilated and non-assimilated predictions in Domain 1, respectively. Additionally, blue and lighter blue lines represent assimilated and non-assimilated predictions in Domain 2, respectively.\nThis figure is providing us to compare all predictions and observations in same plot for each province/gauge. The predictions in both two domains are compatible except smoothed ones which are mentioned above.\n\nnew_df_list[[1]] |&gt;        \n  left_join(new_df_list[[3]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n  left_join(new_df_list[[2]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n  left_join(new_df_list[[4]], by = c(\"Station\",\"dates\",\"observation\")) |&gt; \n    pivot_longer(\n      cols = -c(1,2),\n      names_to = \"Temperature\",\n      values_to = \"value\",\n      values_drop_na = FALSE) |&gt; \n  mutate(Station = factor(Station, labels = df_gauges$Province )) |&gt;\n  mutate(Temperature =  factor(Temperature, \n         levels= c(\"observation\", \"predict_do1\", \"predict_do1_nas\",\n                   \"predict_do2_nas\", \"predict_do2\")) ) |&gt; \nggplot(aes(x= dates, y=value)) +   \n  geom_line(aes(colour = Temperature), size=0.7) +\n  scale_colour_manual(name= expression(\"Temperature\"~(degree*C)), \n          values = c('observation' = \"black\",\n                     'predict_do1_nas' = \"#fc2852\",\n                     'predict_do1' = \"#fc95aa\",\n                     'predict_do2_nas' = \"#23bfce\",\n                     'predict_do2' = \"#157882\"),\n          labels = c('observation' = 'Observation',\n                     'predict_do1_nas' = 'Non-Assimilated Prediction DO1',\n                     'predict_do1' = 'Assimilated Prediction DO1',\n                     'predict_do2_nas' = 'Non-Assimilated Prediction DO2',\n                     'predict_do2' = 'Assimilated Prediction DO2')) +\n  theme_bw() + \n  facet_wrap(~Station, scales = \"free_y\") + \n  labs(x=\" \",y=expression(\"Temperature\"~(degree*C))) + \n  theme(axis.text.x = element_text(angle = 0, hjust = 1))  +\n  theme(legend.position = c(.88, .1), \n        strip.background = element_rect(colour=\"black\", fill=\"cornsilk\"))\n\n\n\n\n\n\n\nFigure 8: Comparison of predictions with observations for each province in both two domains."
  },
  {
    "objectID": "Maksoy_Sakil/WRF.html#error-analysis",
    "href": "Maksoy_Sakil/WRF.html#error-analysis",
    "title": "Evaluation of 3D-var Data Assimilation of WRF Temperature Prediction Over Northwestern Türkiye: A Case Study for the Period of 11 and 16 August, 2004",
    "section": "4.3 Error Analysis",
    "text": "4.3 Error Analysis\nThis part calculates various error metrics, including bias, mean squared error (MSE), root mean squared error (RMSE), normalized RMSE (NRMSE), and correlation coefficients for the predictions. Table 2 shows the error statistics for both domains with respect to complete assimilated and non-assimilated predictions.\nAs anticipated, prediction errors in domain 2 surpass those in domain 1, resulting in a less favorable correlation coefficient. Surprisingly, when comparing assimilated prediction errors with their non-assimilated counterparts within each domain, there is no significant reduction in errors. We think that this situation is also caused smoothed predictions as mentioned previously. Thus, errors for only Kastamunu province which has more appropriate predictions in both domains is given below. Additionally, to examine the opposite of this situation Düzce case is also given below.\n\n#BIAS\nbias &lt;- function(x,y) {mean((x-y), na.rm = TRUE)}\n#MSE\nmse&lt;- function(x,y) {mean((x-y)^2, na.rm = TRUE)}\n#RMSE\nrmse&lt;- function(x,y) {sqrt( mean( (x-y)^2, na.rm = TRUE) )}\n#NRMSE\nnrmse&lt;- function(x,y){sqrt( mean((x-y)^2, na.rm = TRUE) ) / (max(x)-min(y))}\n\n\n# domain1: new_df_list[[1]]; new_df_list[[3]]\n# domain2: new_df_list[[2]]; head(new_df_list[[4]])\n\nerror_table&lt;- data.frame(\n                   Statistics = c(\"BIAS\",\"MSE\",\"RMSE\",\"NRMSE\",\"COR.COEF\"), \n                   Assim_DO1 = 1:5, \n                   Assim_DO2 = 1:5, \n                   Non_Assim_DO1 = 1:5,   \n                   Non_Assim_DO2 = 1:5)\n\n\nfor (i in 1:4) {\n  error_table[1,i+1] &lt;- bias(as.matrix(new_df_list[[i]][,4]), \n                           as.matrix(new_df_list[[i]][,3]))\n  error_table[2,i+1] &lt;- mse(as.matrix(new_df_list[[i]][,4]), \n                           as.matrix(new_df_list[[i]][,3]))\n  error_table[3,i+1] &lt;- rmse(as.matrix(new_df_list[[i]][,4]), \n                           as.matrix(new_df_list[[i]][,3]))\n  error_table[4,i+1] &lt;- nrmse(as.matrix(new_df_list[[i]][,4]), \n                           as.matrix(new_df_list[[i]][,3]))\n  error_table[5,i+1] &lt;- cor(as.matrix(new_df_list[[i]][,4]), \n                           as.matrix(new_df_list[[i]][,3]), use='pairwise.complete.obs')\n               }\n\nerror_table[,2:5]&lt;- round(error_table[,2:5],4)\nerror_table |&gt;  gt()\n\n\n\nTable 2: Error statistics of entire predictions in each domain.\n\n\n\n\n\n\n\n\n\nStatistics\nAssim_DO1\nAssim_DO2\nNon_Assim_DO1\nNon_Assim_DO2\n\n\n\n\nBIAS\n2.1705\n2.0086\n2.1191\n1.9911\n\n\nMSE\n15.5098\n30.5784\n15.3419\n30.4394\n\n\nRMSE\n3.9383\n5.5298\n3.9169\n5.5172\n\n\nNRMSE\n0.1536\n0.1927\n0.1524\n0.1924\n\n\nCOR.COEF\n0.7477\n0.3793\n0.7466\n0.3812\n\n\n\n\n\n\n\n\n\n\nTable 3 shows the error statistics with respect to assimilated and non-assimilated predictions for Kastamonu province in both domains. When we investigate the results for only Kastamunu gauge/province assimilated predictions have less errors slightly and better correlation than non-assimilated ones in both domains.\n\nerror_table&lt;- data.frame(\n                   Statistics = c(\"BIAS\",\"MSE\",\"RMSE\",\"NRMSE\",\"COR.COEF\"), \n                   Assim_DO1 = 1:5, \n                   Assim_DO2 = 1:5, \n                   Non_Assim_DO1 = 1:5,   \n                   Non_Assim_DO2 = 1:5)\n\nfor (i in 1:4) {\n  kastamonu&lt;- \n      new_df_list[[i]] |&gt; \n        mutate(Station = factor(Station, labels = df_gauges$Province )) |&gt; \n        filter(Station == \"Kastamonu\" )\n    \n  error_table[1,i+1] &lt;- bias(as.matrix(kastamonu[,4]), \n                           as.matrix(kastamonu[,3]))\n  error_table[2,i+1] &lt;- mse(as.matrix(kastamonu[,4]), \n                           as.matrix(kastamonu[,3]))\n  error_table[3,i+1] &lt;- rmse(as.matrix(kastamonu[,4]), \n                           as.matrix(kastamonu[,3]))\n  error_table[4,i+1] &lt;- nrmse(as.matrix(kastamonu[,4]), \n                           as.matrix(kastamonu[,3]))\n  error_table[5,i+1] &lt;- cor(as.matrix(kastamonu[,4]), \n                           as.matrix(kastamonu[,3]), use='pairwise.complete.obs')\n               }\n\nerror_table[,2:5]&lt;- round(error_table[,2:5],4)\nerror_table |&gt;  gt()\n\n\n\nTable 3: Error statistics of predictions for Kastomonu province in each domain.\n\n\n\n\n\n\n\n\n\nStatistics\nAssim_DO1\nAssim_DO2\nNon_Assim_DO1\nNon_Assim_DO2\n\n\n\n\nBIAS\n-0.7900\n3.5949\n-0.8210\n3.5408\n\n\nMSE\n9.2272\n20.2453\n9.2425\n19.9165\n\n\nRMSE\n3.0376\n4.4995\n3.0401\n4.4628\n\n\nNRMSE\n0.1469\n0.1991\n0.1469\n0.1976\n\n\nCOR.COEF\n0.8009\n0.8327\n0.8026\n0.8304\n\n\n\n\n\n\n\n\n\n\nTable 4 shows the error statistics with respect to assimilated and non-assimilated predictions for Düzce province in both two domains. In this example, errors are bigger in assimilated versions and correlation coefficients smaller in domain 2.\n\n\n\n\nTable 4: Error statistics of predictions for Duzce province in each domain.\n\n\n\n\n\n\n\n\n\nStatistics\nAssim_DO1\nAssim_DO2\nNon_Assim_DO1\nNon_Assim_DO2\n\n\n\n\nBIAS\n4.3474\n-1.9797\n4.3616\n-1.9523\n\n\nMSE\n24.4499\n20.1177\n24.7878\n19.6618\n\n\nRMSE\n4.9447\n4.4853\n4.9787\n4.4342\n\n\nNRMSE\n0.2174\n0.3537\n0.2174\n0.3497\n\n\nCOR.COEF\n0.8513\n0.3535\n0.8473\n0.3886"
  }
]